<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Toolbox: Experimental evidence</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script src="site_libs/core-js-2.5.3/shim.min.js"></script>
<script src="site_libs/react-17.0.0/react.min.js"></script>
<script src="site_libs/react-17.0.0/react-dom.min.js"></script>
<script src="site_libs/reactwidget-1.0.0/react-tools.js"></script>
<script src="site_libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="site_libs/reactable-binding-0.3.0/reactable.js"></script>

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/main/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>



<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>










<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Toolbox of interventions</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="table_concept.html">Conceptual toolbox</a>
</li>
<li>
  <a href="table_evidence.html">Evidence toolbox</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Toolbox: Experimental evidence</h1>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span>
workflowr <span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span
class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2022-12-14
</p>
<p>
<strong>Checks:</strong> <span
class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 7
<span class="glyphicon glyphicon-exclamation-sign text-danger"
aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>toolbox/</code> <span
class="glyphicon glyphicon-question-sign" aria-hidden="true"
title="This is the local directory in which the code in this file was executed.">
</span>
</p>
<p>
This reproducible <a href="https://rmarkdown.rstudio.com">R Markdown</a>
analysis was created with <a
  href="https://github.com/workflowr/workflowr">workflowr</a> (version
1.7.0). The <em>Checks</em> tab describes the reproducibility checks
that were applied when the results were created. The <em>Past
versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date
</a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate"
class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git
repository, you know the exact version of the code that produced these
results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the
global environment can affect the analysis in your R Markdown file in
unknown ways. For reproduciblity it’s best to always run the code in an
empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20221201code">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Seed:</strong>
<code>set.seed(20221201)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20221201code"
class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20221201)</code> was run prior to running
the code in the R Markdown file. Setting a seed ensures that any results
that rely on randomness, e.g. subsampling or permutations, are
reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Session information:</strong>
recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded"
class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package
versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be
confident that you successfully produced the results during this
run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr
project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomaskozyrevatoolboxtreea0ed13c271874f628bcbe59457e9cf278e84ff5btargetblanka0ed13ca">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Repository version:</strong>
<a href="https://github.com/askozyreva/toolbox/tree/a0ed13c271874f628bcbe59457e9cf278e84ff5b" target="_blank">a0ed13c</a>
</a>
</p>
</div>
<div
id="strongRepositoryversionstrongahrefhttpsgithubcomaskozyrevatoolboxtreea0ed13c271874f628bcbe59457e9cf278e84ff5btargetblanka0ed13ca"
class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development
and connecting the code version to the results is critical for
reproducibility.
</p>
<p>
The results in this page were generated with repository version
<a href="https://github.com/askozyreva/toolbox/tree/a0ed13c271874f628bcbe59457e9cf278e84ff5b" target="_blank">a0ed13c</a>.
See the <em>Past versions</em> tab to see a history of the changes made
to the R Markdown and HTML files.
</p>
<p>
Note that you need to be careful to ensure that all relevant files for
the analysis have been committed to Git prior to generating the results
(you can use <code>wflow_publish</code> or
<code>wflow_git_commit</code>). workflowr only checks the R Markdown
file, but you know if there are other scripts or data files that it
depends on. Below is the status of the Git repository when the results
were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rproj.user/

Unstaged changes:
    Deleted:    data/README.md
    Modified:   data/toolbox_concept.xlsx
    Modified:   data/toolbox_evidence.xlsx
    Modified:   output/infos_raw.json
    Modified:   output/infos_refined.json

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not
included in this status report because it is ok for generated content to
have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the repository in which changes were
made to the R Markdown (<code>analysis/table_evidence.Rmd</code>) and
HTML (<code>public/table_evidence.html</code>) files. If you’ve
configured a remote Git repository (see <code>?wflow_git_remote</code>),
click on the hyperlinks in the table below to view the files as they
were in that past version.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/askozyreva/toolbox/803ec1e914fd9756ae7a02d7372a05d8f2c20a52/public/table_evidence.html" target="_blank">803ec1e</a>
</td>
<td>
Kozyreva
</td>
<td>
2022-12-14
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/askozyreva/toolbox/blob/67aaf8d6a8d8c4b60ebd7c94a51fe59b2e80c904/analysis/table_evidence.Rmd" target="_blank">67aaf8d</a>
</td>
<td>
Kozyreva
</td>
<td>
2022-12-14
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/askozyreva/toolbox/blob/caa5a423f90c9020a43bd382f2bf29647a281aa9/analysis/table_evidence.Rmd" target="_blank">caa5a42</a>
</td>
<td>
Kozyreva
</td>
<td>
2022-12-01
</td>
<td>
initial commit
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/askozyreva/toolbox/caa5a423f90c9020a43bd382f2bf29647a281aa9/public/table_evidence.html" target="_blank">caa5a42</a>
</td>
<td>
Kozyreva
</td>
<td>
2022-12-01
</td>
<td>
initial commit
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<p>This part of the online supplement is a digital toolbox in the form
of a dynamic table that offers an overview of published evidence . For
publication details, links to articles, and detailed results,
<strong><em>click on the arrow or the “Expand” button</em></strong> to
the left of the intervention type. To search through the whole table,
use the search function below. Use the smaller search fields under a
column’s header to search within that column. You can sort a column by
clicking on its header, or sort multiple columns by holding the shift
key while sorting.</p>
<button onclick="Reactable.toggleAllRowsExpanded(&#39;evidence-table&#39;)">Click here to expand/collapse all rows</button>
<div id="evidence-table" class="reactable html-widget" style="width:1800px;height:auto;"></div>
<script type="application/json" data-for="evidence-table">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"Intervention":["Accuracy prompts","Accuracy prompts","Accuracy prompts","Accuracy prompts","Accuracy prompts","Accuracy prompts","Debunking","Debunking","Debunking","Debunking","Friction","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Lateral reading in education","Lateral reading in education","Lateral reading in education","Lateral reading in education","Lateral reading in education","Lateral reading online","Lateral reading online","Lateral reading online","Media literacy tips","Media literacy tips","Media literacy tips","Media literacy tips","Rebuttals of science denialism","Rebuttals of science denialism","Self-reflection tools","Social norms","Social norms","Social norms","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels"],"References":["Pennycook et al., Psych Sci (2020).","Pennycook et al., Nature (2021).","Epstein et al., Harv Misinfo Review (2021).","Roozenbeek et al., Psych Sci (2021).","Pennycook & Rand, Nat Comms (2022).","Arechar et al., PsyArXiv (2022).","Ecker et al., Cogn Res: Princ Implic (2020).","Paynter et al., PLOS ONE (2019).","Tay et al., Br J Psychol (2022).","Swire et al., J Exp Psychol: Learn Mem Cogn (2017).","Fazio, Harv Misinfo Review (2020).","Roozenbeek et al., Palgrave Commun (2019).","Basol et al., BD & S (2021).","Roozenbeek et al., Harv Misinfo Review (2020).","Maertens et al., J Exp Psychol: Appl (2021).","van der Linden et al., Global Challenges (2017).","Cook et al., PLOS ONE (2017).","Roozenbeek et al., Sci Adv (2022).","Wineburg et al., J Educ Psychol (2022).","Wineburg et al., Teach Coll Rec (2019).","McGrew et al., Br J Educ Psychol (2019).","McGrew, Comput Educ (2020).","Brodsky et al., Cogn Res: Princ Implic  (2021).","Panizza et al., Sci Rep (2022).","Donovan et al., Mem Cogn (2020).","Resnick et al., arXiv (2021).","Guess et al., PNAS (2020).","Ali et al., arXiv (2021).","Badrinathan, Am Political Sci Rev (2021).","Epstein et al (2021). Harv Misinfo Review.","Schmid et al., J Cogn (2020).","Schmid et al., NHB (2019).","Lorenz-Spreen et al., Sci Rep (2021).","Ecker et al., QJEP (2022).","Cookson et al., PLOS ONE (2021).","Andi et al., Digit Journal (2021).","Clayton et al., Political Behav (2020).","Ecker et al., (2010). Mem Cogn.","Pennycook et al., Manag Sci (2020).","Kim et al., Manag Inf Syst (2019).","Amazeen et al., Journal Mass Commun Q (2018).","Grady et al., Cogn Res: Princ Implic (2021)."],"Experimental setting":["Online","Online; Field","Online","Online","Online","Online","Online","Field","Online","Laboratory","Online","Online","Online","Online","Online;Laboratory","Online","Online","Online; Field","Field","Laboratory","Field","Field","Field","Online","Online; Laboratory","Online","Online; Field","Field","Field","Online","Laboratory","Online","Online","Laboratory","Laboratory","Laboratory","Online","Laboratory","Online","Online","Online","Online"],"Design":["RCT","RCT","RCT","RCT (replication test)","Internal meta-analysis","RCT","Mixed-design RCT","RCT","RCT","Between-subjects pre–post design","RCT","Within-subjects pre–post design","Within-subjects pre–post design; RCT","Mixed-design RCT","Longitudinal RCT","Pre–post, within-subjects, mixed-design RCT","Mixed-design RCT","RCT","RCT (matched-control design)","Expert–novice study","RCT; Within–subjects pre-post design","Within-subjects pre–post design","RCT (matched-control design)","RCT","Mixed-design RCT","Mixed-design RCT","Longitudinal RCT","Longitudinal RCT","RCT (randomized block design)","RCT","Within-subjects pre–post design; Between-subjects pre–post design","Within-subjects pre–post design; Between-subjects pre–post design","RCT","Between-subjects pre–post design","Longitudinal RCT","RCT","Between-subjects pre–post design","Between-subjects design; RCT","RCT","Within–between design with repeated measures","RCT","Longitudinal between-subjects design"],"Treatment":["Accuracy prompt: accuracy rating question.","Accuracy prompt: accuracy rating question.","Accuracy prompt in 3 formats: evaluation (accuracy rating question), long evaluation (8 questions), and importance (answering how important is it to share only accurate content).","Accuracy prompt: accuracy rating question.","Accuracy prompts: evaluation, importance, norms, PSA video, reason, and tips.","Accuracy prompt: accuracy rating question and media literacy tips.","Narrative and non-narrative corrections focusing on factual evidence.","Training materials combining basic refutational approach with 6 additional elements thought to boost the effectiveness of a correction: source credibility, self-affirmation, social norming, warning, graphical representations, and salience.","Prebunking and debunking in the form of intervention articles.","Misinformation corrections and factual affirmations in the form of brief retraction, detailed refutation, brief affirmation, and detailed affirmation.","Friction: prompt asking to explain why a headline is true or false before sharing.","Game in which participants learn to apply 6 common misinformation techniques: impersonation, emotional language, group polarisation, floating conspiracies, discrediting opponents, and trolling.","Study 1: Inoculation in the form of manipulation techniques learned in the game (using moral–emotional language, using fake experts, and conspiratorial reasoning); Study 2: Real-world infographics.","Inoculation in the form of manipulation techniques learned in the game: trolling, exploiting emotional language, artificially amplifying the reach, conspiracy theories, and polarisation.","Inoculation in the form of manipulation techniques learned in the game: using moral–emotional language, using fake experts, and conspiratorial reasoning.","Inoculation: forewarning and preemptive refutation.","Inoculation: explanation messages.","5 short (≈ 1.5 min.) inoculation videos, each inoculating against a specific manipulation technique (emotional manipulation, incoherence, false dichotomies, scapegoating, ad hominem attacks).","Classroom-based lateral reading as part of Civic Online Reasoning curriculum.","Lateral reading through think-aloud protocols.","Classroom-based lateral reading to evaluating the credibility of online information.","Classroom-based lessons on evaluating online information.","Classroom-based lesson on the Digital Polarization Initiative's 4 fact-checking moves.","Social media pop-up presenting a list of civic online reasoning techniques (e.g., lateral reading, click restraint) as tips to verify the information in the post; pop-up combined with monetary incentives; monetary incentives alone.","Studies 1, 2: Encouragement to research online with plain access.","Requirement to search for corroborating evidence.","Tips on how to spot false news.","Tips presented in a video and personalised feedback.","Media literacy in-person training, flyer with tips.","Media literacy tips (Facebook’s “Tips to Spot False News”).","Study 1: Outnumbering and rebuttal; Study 2: Outnumbering, rebuttal, and forewarning; Study 3: Outnumbering from multiple sources, rebuttal, and forewarning.","Topic rebuttal, technique rebuttal.","Self-reflection by filling out a personality questionnaire.","Descriptive(fictional) norms contesting belief-congruent claims; refutations; norms and refutations.","Normative feedback in the form of a text.","A warning in the form of descriptive and injunctive social norms.","General warning about misleading articles; specific warnings about individual articles questioned by fact-checkers (“Disputed” or “Rated false” tags under false headlines).","Specific warning(detailed information about the continued influence effect); general warning (reminder that facts are not always properly checked before information is disseminated).","Warning labels: \"Disputed by 3rd Party Fact-Checkers\" and \"False\" stamped over a headline.","Three types of source ratings: expert rating, user article rating, and user source rating.","Correction with or without a visual rating scale.","Warning labels indicating that a headline is false presented before, during, or after the headline."],"Outcome variable":["Sharing discernment","Sharing discernment","Sharing discernment","Sharing discernment","Sharing discernment","Sharing discernment","Reliance on misinformation in inference questions, misinformation beliefs","Support for evidence-based autism treatments, use intention","Reliance on misinformation in inference questions, willingness to purchase fair-trade products","Direct belief ratings, reliance on misinformation in inference questions","Likelihood to share false headlines","Reliability ratings of tweets","Manipulativeness ratings, confidence in manipulativeness rating, willingness to share","Susceptibility to political misinformation (measured via reliability ratings of social media posts), confidence in reliability ratings, likelihood to share posts with others","Reliability ratings of news headlines","Estimate of the scientific agreement on human-caused climate change (0%–100%)","Perceived consensus, acceptance of anthropogenic global warming","Studies 1–6: manipulation technique recognition, attitudinal certainty (confidence), trustworthiness discernment, sharing intentions; \nStudy 7: manipulation technique recognition","Assessment score for judgments of credibility of online sources on a 3-point rubric (beginning, emerging, mastery)","Evaluation of credibility of online sources","Evaluation of credibility of online sources","Civic online reasoning skills including ad identification, lateral reading, analyzing evidence, claim research","Scores for evaluations of the trustworthiness of online content, self-reported use of lateral reading","Accuracy score, correct guessing (binary), technique adoption (search behavior, self-report)","Accuracy of reproduced information, self-reported search rates","Correlation of lay ratings with journalist ratings","Accuracy discernment between false and mainstream headlines","Identification of misinformation (agreement with news statement)","Binary identification of misinformation","Sharing discernment","Attitudes towards vaccination, intention to vaccinate","Attitudes towards behaviors favored by science and intentions to perform these behaviors","Identification of personality-targeted ads","Endorsement of belief-congruent claims (in beliefs, predictive estimates, inferential reasoning)","Personal belief in anti-vaccine conspiracy theories, intention to vaccinate,perceptions of other UK parents’ beliefs and intentions","Willingness to share a false news article","Accuracy ratings of news headlines, willingness to “like” or share a given headline on Facebook","Reliance on misinformation in inference questions","Accuracy ratings, sharing intentions","Rating of article's believability and likelihood to engage with it","Truthfulness ratings of a statement","Accuracy judgment (on a binary scale)"],"Sample size":[856,7955,2391,1583,26863,33480,2279,47,735,202,501,14163,3548,681,315,2167,1106,29116,499,45,67,68,230,3520,561,1301,11924,750,1309,9070,887,1773,828,441,202,1003,2994,217,6839,889,1020,418],"Sample Country":["United States","United States","United States","United States","United States","Argentina, Australia, Brazil, China, Egypt, India, Italy, Mexico, Nigeria, Philippines, Russia, Saudi Arabia, South Africa, Spain, United Kingdom, United States","United States","Australia","United States","Australia","United States","International (unspecified)","Study 1: International English-speaking community; majority from Europe (59.3%) and North America (22.7%). Study 2: Germany, France, United Kingdom","International (at least 50% United States)","International","United States","United States","United States","United States","United States","United States","United States","United States","United Kingdom","NA","United States","United States, India","Pakistan","India","United States","Germany","Germany, United States","United Kingdom","Australia","United Kingdom","United States","United States","Australia","United States","United States","United States","United States"],"Sample demographics":["Quota-matched","Convenience; Quota-matched; Twitter users","Quota-matched","Quota-matched","Convenience; Quota-matched; Sample matched","Quota-matched","Convenience","Early-intervention professionals working with preschool children with autism","Convenience","Convenience","Convenience","Convenience","Convenience; Quota-matched","Convenience","Convenience","Convenience","Quota-matched","Studies 1–6: Quota-matched; Study 7: Geographic, age, and online behavior quota","Students enrolled in a required school government course in an urban district","Stanford University undergraduates, PhD historians, professional fact-checkers","Convenience","Convenience","Convenience","Convenience","Convenience","Convenience; Professionals","Quota-matched; Convenience; Probability sample","Cluster: low- and middle-income areas","Random, drawn from the city of Gaya in the state of Bihar","Quota-matched: age, gender, race, geographic region","Convenience","Convenience; Quota-matched","Convenience","Convenience","Convenience: Parents of young children","Convenience","Convenience","Convenience","Convenience","Convenience","Nationally representative","Convenience"],"Recruitment":["Lucid","MTurk; Lucid; Twitter","Lucid","Respondi","MTurk; Lucid; YouGov","Lucid","Prolific","Professional organizations","MTurk","University campus; Local community","MTurk","Online; Press release","Online; Prolific","Prolific","Prolific","Study 2: MTurk","Qualtrics","Studies 1–6: Prolific \n\nStudy 7: YouTube","All schools in one urban district","University campus; Professional organizations","University campus","High school","University campus","Prolific","MTurk; Northwestern University’s subject pool","MTurk; Professional organizations","YouGov; MTurk; Internet Research Bureau; Morsel (polling firm)","Random grid sampling","Random walk sampling","Lucid","University campus","University Campus; Panel Survey Company (Germany); MTurk","Prolific","University campus","Prolific","MTurk","MTurk","University campus","MTurk","Qualtrics","YouGov","MTurk"],"Longevity":["Not measured","Not measured, but study 7 had a test window of 24 hours","Not measured","Not measured","Not measured","Not measured","Yes, 2-day delay; intervention still effective","Yes, 6 weeks; effects still present in pre-post comparison, but no longer an advantage of the optimized debunking vs. the treatment-as-usual debunking","Not measured","Intervention still effective after one month, but belief regression was evident.","Not measured","Not measured","1-week follow-up with new items. Main effect for manipulativeness and confidence were still present for the Go Viral game. Effect for sharing no longer significant. Prebunking infographics no longer significant after 1 week.","Not measured","Study 1 and 3: Follow-ups at several time intervals (from 1 week to up tor 13 weeks): intervention effect was still significant. Study 2: inoculation effect decays and is no longer significant after 2 months with no \"boosters\" in between.","Not measured","Not measured","For study 7, median time between seeing the YouTube ad and answering the survey question was 18.4 hours.","Yes. The intervention was conducted mid semester and posttest was administered at the end of the semester.","Not measured","Yes. Posttest was administered 5 weeks after completion of the intervention.","Yes. Posttest was administered two weeks after the final treatment lesson.","Not measured","Not measured","Not measured","Not measured","Lasts up to 3 weeks in the U.S. (but decays by about half), does not last in India.","Yes, similar effect size (but less precisely estimated) 4-6 weeks later","Outcomes were collected only ~2 weeks later","Not measured","Not measured","Not measured","Not measured","Not measured","6-week delay, no effect remained","Not measured","Not measured","Not measured","Not measured","Not measured","Not measured","Yes, 2 weeks, no effect remained"],"Main findings":["Accuracy prompt intervention significantly increased participants’ level of discernment between sharing true headlines about COVID-19 and sharing false headlines about COVID-19, mainly by increasing sharing intentions for true headlines.","Priming accuracy improved sharing discernment between false and true headlines in online experiments. In the field experiment, an accuracy message increased the average quality of the news sources shared.","Different accuracy prompts interventions increased sharing discernment by 3–6 percentage points.","The first stage of the replication test was unsuccessful. After collecting a second round of data, the authors found a small but significant interaction between treatment condition and truth discernment.","Accuracy prompts increased the quality of news that people shared (sharing discernment) relative to control, primarily by reducing sharing intentions for false headlines by 10% relative to control.","Prompt condition increased sharing discernment relative to the baseline sharing condition. There was significant variation across countries in the magnitude of this effect.","Corrections reduced misinformation inferences or beliefs regardless of format, both immediately and after a 2-day delay.","The benefits of optimized debunking were greater immediately after training, but this effect was not sustained at follow-up.","Debunking worked as well as or better than prebunking, with little impact of misinformation type, thus reducing references to misinformation and increasing positive sentiment expressed in social media posts. There was no good evidence for an effect of debunking on a willingness-to-pay measure.","Adults over the age of 65 were worse than younger adults at sustaining their postcorrection belief that myths were inaccurate. A greater level of explanatory detail promoted more sustained belief change. Fact affirmations promoted more sustained belief change than did myth retractions over 1 week (but not over 3 weeks).","Explaining why a headline was true or false reduced participants’ intention to share false headlines, but had no effect on their intention to share true headlines. The effect of providing an explanation was larger when participants were seeing the headline for the first time. The intervention was less effective for headlines that had been seen previously in the experiment.","The study provides preliminary evidence that active inoculation through the Bad News game significantly reduced the perceived reliability of tweets that embedded several common online misinformation strategies.","The game reduced the perceived reliability of misinformation, increased people's confidence in their assessment of the reliability of misinformation, and reduced intentions to share misinformation with others.","Participants who played the game found misinformation significantly less reliable after playing, were significantly more confident in their assessment, and were significantly less likely to report sharing misinformation, thereby supporting the effectiveness of Harmony Square as a tool to inoculate people against online manipulation.","Inoculation-based media and information literacy interventions such as the Bad News Game were found to confer protection against the influence of misinformation over time. With regular assessment, the positive effects could be maintained for at least 3 months. Without regular “boosting,” the effects dissipated within 2 months.","Communicating the scientific consensus on human-caused climate change significantly increased public perception of the expert consensus. The introduction of misinformation contesting the existence of a scientific consensus neutralized the positive effect of highlighting normative expert agreement. Preemptively warning people about politically motivated attempts to spread misinformation helped promote and protect (“inoculate”) public attitudes about the scientific consensus.","Studies 1 and 2, employing different styles of misinformation, both found that inoculation neutralized the negative influence of misinformation on perceived consensus about anthropogenic global warming.","The videos were effective at conferring resilience against manipulation techniques commonly used in misinformation (improving technique discernment, confidence in identifying manipulative content, trustworthiness discernment, and sharing discernment). Findings were not moderated by outcome measure response order, nor did any covariates consistently interact with the main effect (including political ideology, bullshit receptivity, and education). On YouTube, the videos were effective at improving manipulation technique recognition.","Less than 6 hours of classroom instruction based on lateral reading strategy significantly improved students’\njudgment about the credibility of online sources.","Compared to the other groups, fact-checkers arrived at more warranted conclusions in a fraction of the time using lateral reading strategy. Undergraduates and professors stayed on an unfamiliar website to decide whether it was credible, whereas fact-checkers opened new tabs and verified the website’s credibility by checking other, trusted sites.","Students in the treatment group were over twice as likely to score higher at posttest than at pretest, while students in the control condition were equally likely to score higher at posttest than at pretest.","Students' scores improved significantly from pretest to posttest on 3 of the 4 tasks—investigating the source of a website, critiquing evidence, and locating reliable sources—during an open internet search.","At posttest, students in the treatment condition were more likely than students in the control classes to engage in lateral reading and to accurately assess the trustworthiness of online sources.","Monetary incentives were overall effective in increasing accuracy. The pop-up worked when the source of information was unknown. Pop-up and incentives, when used together, produced a cumulative effect on accuracy.","The opportunity to search reduced inaccurate reproductions, particularly for topics about which participants were unlikely to possess sufficient background knowledge to validate independently.","Requirement to seek out or consider external evidence in 2 treatment conditions improved misinformation judgments. Judgments in treatment conditions were also more internally consistent between raters, showed less partisan divide, and were better correlated with expert journalist judgments. When averaging ratings from several raters, the correlation with a journalist was higher in the individual research condition.","The intervention improved discernment between mainstream and false news headlines among both a nationally representative sample in the United States (by 26.5%) and a highly educated online sample in India (by 17.5%). This increase in discernment remained measurable several weeks later in the United States, but not in India. However, no effects were found among a representative sample of respondents in a largely rural area of northern India, where rates of social media use are far lower.","The study did not find a significant effect of video-based general educational messages about misinformation. However, when such messages were augmented with personalized feedback based on individuals' past engagement with fake news, there was an improvement of 0.14 standard deviations in identifying fake news.","The intervention had no effect on average. However, it improved misinformation identification skills for one set of respondents (non-BJP respondents) and had a negative effect on BJP partisans.","The media literacy tips intervention increased sharing discernment by 3 percentage points.","The study found that damage from science denialists could be mitigated through rebuttals from science advocates. Forewarnings were also an effective weight-of-evidence strategy to mitigate science denialism influence.The strategy of outnumbering science deniers had no success across 3 studies.","Not responding to science deniers had a negative effect on attitudes towards behaviors favored by science (e.g., vaccination) and intentions to perform these behaviors. Providing the facts about the topic or uncovering the rhetorical techniques typical for denialism had positive effects. There was no evidence that complex combinations of topic and technique rebuttals are more effective than single strategies, nor that rebutting science denialism in public discussions backfires. This was the even the case for vulnerable groups (e.g., U.S. conservatives).","A short, simple intervention prompting participants to reflect on an attribute of their own personality—by completing a short personality questionnaire—boosted their ability to accurately identify ads that were targeted at them by up to 26 percentage points.","Experiment 1 found that using a single-point estimate to communicate a norm affected belief but had less impact than a refutation. Experiment 2 used a verbally presented distribution of 4 values to communicate a norm, which was largely ineffective. Experiment 3 used a graphically presented social norm with 25 values, which was found to be as effective at reducing claim belief as a refutation, with the combination of both interventions being most impactful.","Compared to an assessment-only control, UK parents of young children who were exposed to the normative feedback intervention showed reduced belief in anti-vaccine conspiracy theories at immediate follow-up. Moreover, mediation analysis showed that the intervention reduced perceived norms of anti-vaccine conspiracy beliefs, which in turn reduced personal beliefs.","Participants exposed to a social norm-based message with both descriptive and injunctive elements were 5 percentage points less likely to say that they were willing to share a false news article with their social network.","False headlines were perceived as less accurate when participants received a general warning about misleading information on social media or when specific headlines were accompanied by a “Disputed” or “Rated false” tag. Though the magnitudes of these effects were relatively modest, they generally did not vary by whether headlines were congenial to participants’ political views. Adding a “Rated false” tag to an article headline lowered its perceived accuracy more than adding a “Disputed” tag (Facebook’s original approach) relative to a control condition.","A specific warning succeeded in reducing the continued reliance on outdated information but did not eliminate it. A more general warning was even less effective. A combination of specific warning and a plausible alternative explanation for the retracted information further reduced the continued influence effect but did not eliminate it altogether.","Attaching warning labels to false news headlines had small to moderate effects on accuracy ratings and sharing intentions (no partisanship effect). An unintended consequence of warning labels, the implied truth effect, emerged:Untagged headlines (even if false) were seen as more accurate and were given more consideration for sharing on social media.","Ratings affected believability, but only when sources were rated low: Articles from low-rated sources—when the ratings were from experts or users evaluating articles—were less believable, but articles from high-rated sources were not more believable. Believability and confirmation bias both affected user actions.","The effectiveness of rating scales was especially pronounced when they were used to correct non-political misinformation. In contrast, varying the type of correction format had no effect when political misinformation was involved.","Presenting the warning before a false headline was effective initially, though it was not significantly better in most areas than the label under the headline. Two weeks later, however, across conditions, people once again believed items they once knew were false, especially when those items supported their political views."]},"columns":[{"accessor":".details","name":"Details","type":"NULL","minWidth":75,"sortable":false,"resizable":false,"filterable":false,"searchable":false,"width":75,"align":"center","cell":[{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]}],"details":[{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Accuracy prompts"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Pennycook, G., McPhetres, J. Zhang, Y., Lu, J. G. & Rand, D. G. (2020). Fighting COVID-19 misinformation on social media: Experimental evidence for a scalable accuracy-nudge intervention. Psychological Science, 31(7), 770–780."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/0956797620939054"},"children":["https://doi.org/10.1177/0956797620939054"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/7d3xh/"},"children":["https://osf.io/7d3xh/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Across two studies with more than 1,700 U.S. adults recruited online, we present evidence that people share false claims about COVID-19 partly because they simply fail to think sufficiently about whether or not the content is accurate when deciding what to share. In Study 1, participants were far worse at discerning between true and false content when deciding what they would share on social media relative to when they were asked directly about accuracy. Furthermore, greater cognitive reflection and science knowledge were associated with stronger discernment. In Study 2, we found that a simple accuracy reminder at the beginning of the study (i.e., judging the accuracy of a non-COVID-19-related headline) nearly tripled the level of truth discernment in participants’ subsequent sharing intentions. Our results, which mirror those found previously for political fake news, suggest that nudging people to think about accuracy is a simple way to improve choices about what to share on social media.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Presented true and false COVID-related headlines and asked participants whether they would be willing to share the headline on social media. Participants were randomly assigned to either a control condition (news-sharing task) or a treatment condition (rating the accuracy of 1 of 4 headlines—all politically neutral and unrelated to COVID-19—before beginning the news-sharing task).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 2: False (source: snopes.com, factcheck,org) and true (source: Harvard GHI) news headlines relating to COVID-19, the nudge in the form of politically neutral non-Covid headlines",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"6-item cognitive reflection test, general science knowledge quiz, Medical Maximizer-Minimizer scale, political ideology on both social and fiscal issues, and Democrat versus Republican Party alignment",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The paper includes 2 studies, but only study 2 was testing the accuracy prompt intervention. Study 1 (not included here) tested for a dissociation between accuracy judgments and sharing intentions when participants evaluated a set of true and false news headlines about COVID-19.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["2"],"Description":["Study 2 experimentally tested whether subtly making the concept of accuracy salient increased the quality of COVID-19 information that people were willing to share online."],"N":[856],"Effect size":["Cohen’s d: 0.14 (95%-CI: 0.05-0.23)"],"Comments":["In the treatment condition, sharing intentions for true headlines were significantly higher than for false headlines."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"3ed5371ffe40298f04d5f997bec6dd94","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Accuracy prompts"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Pennycook, G., Epstein, Z., Mosleh, M., Arechar, A. A., Eckles, D., & Rand, D. G. (2021). Shifting attention to accuracy can reduce misinformation online. Nature, 592(7855), 590–595."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1038/s41586-021-03344-2"},"children":["https://doi.org/10.1038/s41586-021-03344-2"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/p6u8k/"},"children":["https://osf.io/p6u8k/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"In recent years, there has been a great deal of concern about the proliferation of false and misleading news on social media. Academics and practitioners alike have asked why people share such misinformation, and sought solutions to reduce the sharing of misinformation. Here, we attempt to address both of these questions. First, we find that the veracity of headlines has little effect on sharing intentions, despite having a large effect on judgments of accuracy. This dissociation suggests that sharing does not necessarily indicate belief. Nonetheless, most participants say it is important to share only accurate news. To shed light on this apparent contradiction, we carried out four survey experiments and a field experiment on Twitter; the results show that subtly shifting attention to accuracy increases the quality of news that people subsequently share. Together with additional computational analyses, these findings indicate that people often share misinformation because their attention is focused on factors other than accuracy—and therefore they fail to implement a strongly held preference for accurate sharing. Our results challenge the popular claim that people value partisanship over accuracy, and provide evidence for scalable attention-based interventions that social media platforms could easily implement to counter misinformation online.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Presented true and false COVID-related headlines and asked participants whether they would be willing to share the headline on social media. In Studies 3 and 4, participants were randomly assigned to a control condition (news-sharing task) or a treatment condition (rating the accuracy of a single headline—unrelated to politics—before beginning the news-sharing task). Study 4 also included two other conditions: active control (rating the funniness of a headline—unrelated to politics—before beginning the news-sharing task) and importance (participants were asked whether they think it's important to only share accurate content before they began the news-sharing task). In Study 7, a digital field experiment was carried out on Twitter where \"bots\" were used to follow people who had retweeted dubious content. Users who followed back the bots were sent a message asking about accuracy (for different users at different times, maintaining experimental control). Tweet history (news quality ratings) was monitored for people who had received the message in the previous 24 hours and those who had not received it yet.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Pro-democratic or pro-Republican false (source: snopes.com) and true (source: mainstream news outlets) news headlines, lede sentence, and image, the nudge in the form of politically neutral headlines.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"CRT, political-knowledge questionnaire, PANAS, political partisanship, Importance of sharing only accurate news on social media, the positive and negative affective schedule",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The paper includes 7 studies, of which 5 were testing accuracy prompt interventions. Study 1 and 2 (not included here) tested for a dissociation between accuracy judgments and sharing intentions when participants evaluated a set of true and false news headlines.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["3","4","5","6","7"],"Description":["Study 3 experimentally tested whether subtly shifting attention to accuracy increases the veracity of the news people are willing to share.","Study 4 experimentally tested whether subtly shifting attention to accuracy increases the veracity of the news people are willing to share.","Study 5 experimentally tested whether subtly shifting attention to accuracy increases the veracity of the news people are willing to share. Study 5 also included an importance treatment condition that tested another approach for making accuracy salient by having participants begin the study by indicating the importance they place on sharing only accurate content (instead of rating the accuracy of a neutral headline).","Study 6 experimentally tested a full-attention treatment that directly forces participants to consider the accuracy of each headline before deciding whether to share it. The goal was to determine how much of the sharing of false headlines in the full-attention treatment was attributable to inattention, confusion, and purposeful sharing.","Study 7 was a field experiment on Twitter and it experimentally tested whether an accuracy prompt would improve the quality of news sources that people share."],"N":[727,780,671,398,5379],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["Treatment condition significantly increased sharing discernment (interaction between headline veracity and treatment): b = 0.053, 95% CI [0.032, 0.074], F(1, 17,413) = 24.21, P < 0.0001.","Treatment condition significantly increased sharing discernment (interaction between headline veracity and treatment): b = 0.065, 95% CI [0.036, 0.094], F(1, 18,673) = 19.53, P < 0.0001.","Both treatments significantly increased sharing discernment relative to the controls (interaction between veracity and condition: treatment, b = 0.054, 95% confidence interval [0.023, 0.085], F = 11.98, P = 0.0005; importance treatment, b = 0.038, 95% confidence interval [0.014, 0.061], F = 9.76, P = 0.0018).","Of the sharing intentions for false headlines, the inattention-based account explains 51.2% (95% CI [38.4%, 62.0%]) of sharing, the confusion-based account explains 33.1% (95% CI [25.1%, 42.4%]) of sharing, and the preference-based account explains 15.8% (95% CI [11.1%, 21.5%]) of sharing.","Relative to baseline, the accuracy message increased the average quality of the news sources shared (b = 0.007, t(5375) = 2.91, 95% null acceptance region of t [−0.44, 2.59], PFRI = 0.009) and the total quality of shared sources summed over all posts (b = 0.014, t(5375) = 3.12, 95% null acceptance region of t [−0.08, 2.90], PFRI = 0.011)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"1b17c2685822f7c4e637b8c12b900daf","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Accuracy prompts"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Epstein, Z, Berinsky, A. J., Cole, R., Gully, A., Pennycook, G., & Rand, D. G. (2021). Developing an accuracy-prompt toolkit to reduce COVID-19 misinformation online. Harvard Kennedy School Misinformation Review, 2(3)."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.37016/mr-2020-71"},"children":["https://doi.org/10.37016/mr-2020-71"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/hu4k2/"},"children":["https://osf.io/hu4k2/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Recent research suggests that shifting users’ attention to accuracy increases the quality of news they subsequently share online. Here we help develop this initial observation into a suite of deployable interventions for practitioners. We ask (i) how prior results generalize to other approaches for prompting users to consider accuracy, and (ii) for whom these prompts are more versus less effective. In a large survey experiment examining participants’ intentions to share true and false headlines about COVID-19, we identify a variety of different accuracy prompts that su­ccessfully increase sharing discernment across a wide range of demographic subgroups while maintaining user autonomy.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"To assess the impact of 8 experimental treatments on sharing intentions for true and false headlines, each treatment was administered prior to a news-sharing task. Participants then saw true and false COVID-related headlines and were asked about their sharing intentions and accuracy judgments. Of the 8 treatments, 3 were accuracy prompts. In the evaluation treatment, as in Pennycook et al. (2021) and Pennycook, McPhetres, et al. (2020), participants evaluated the accuracy of a non-COVID-related headline—thereby priming the concept of accuracy when the participants continued on to the sharing task. In the long evaluation treatment, participants evaluated the accuracy of 8 non-COVID-related headlines (half true, half false). After each headline, they learned whether their answer was correct or incorrect and whether the headline had beena real news headline or afake news headline. In the importance treatment, as in Pennycook et al. (2021), participants were asked “How important is it to you that you share only news articles on social media (such as Facebook and Twitter) if they are accurate?”",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"False and true news cards (e.g., the combination of a headline, an image, and a source) relating to COVID-19",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Level of concern about COVID-19, the extent to which they had been following COVID-19–related news, CRT, importance of sharing only accurate news",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Several interventions were comparatively tested in this study. However, not all of them were accuracy prompts. Only the accuracy prompts interventions are included here: Evaluation, Long Evaluation, Importance.  Other tested  interventions: media literacy tips, generic norms, partisan norms, tips + norms, importance + norms. Tips and Norms are included in respective sections on Media Literacy Tips and Social Norms.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["Evaluation","Long Evaluation","Importance"],"Description":["Asking participants to judge the accuracy of a non-COVID-19 related headline","Asking participants to judge the accuracy of a series of 4 non-COVID-19–related headlines (and providing corrective feedback on their responses)","Asking participants how important it was to them to share only accurate news"],"N":[935,410,1046],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["Intervention increased sharing discernment by roughly 50% (3 percentage points)","Intervention increased sharing discernment by roughly 100% (6 percentage points)","Intervention increased sharing discernment by roughly 50% (3 percentage points)"]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"8c3c0c73781856e417085987d42f8cce","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Accuracy prompts"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Roozenbeek, J.,  Freeman, A. L. J., & van der Linden, S. (2021). How accurate are accuracy-nudge interventions? A preregistered direct replication of Pennycook et al. (2020). Psychological Science, 32(7), 1169–1178."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/09567976211024535"},"children":["https://doi.org/10.1177/09567976211024535"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/rkfq5/"},"children":["https://osf.io/rkfq5/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"As part of the Systematizing Confidence in Open Research and Evidence (SCORE) program, the present study consisted of a two-stage replication test of a central finding by Pennycook et al. (2020), namely that asking people to think about the accuracy of a single headline improves “truth discernment” of intentions to share news headlines about COVID-19. The first stage of the replication test (n = 701) was unsuccessful (p = .67). After collecting a second round of data (additional n = 882, pooled N = 1,583), we found a small but significant interaction between treatment condition and truth discernment (uncorrected p = .017; treatment: d = 0.14, control: d = 0.10). As in the target study, perceived headline accuracy correlated with treatment impact, so that treatment-group participants were less willing to share headlines that were perceived as less accurate. We discuss potential explanations for these findings and an unreported change in the hypothesis (but not the analysis plan) from the preregistration in the original study.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Presented true and false COVID-related headlines and asked participants whether they would be willing to share the headline on social media. Participants were randomly assigned to a treatment or a control group. Participants in both groups were shown 15 real and 15 false headlines related to COVID-19 and asked whether they would be likely to share it on social media (response options ranged from extremely unlikely to extremely likely). In the treatment group, participants first saw a headline unrelated to COVID-19 and were asked about its accuracy (yes/no response options).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"False (source: snopes.com, factcheck,org) and true (source: Harvard GHI) news headlines relating to COVID-19, format of Facebook posts, including an image, a headline, and a lede sentence, the nudge in the form of politically neutral non-Covid headlines",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"type of social media accounts, 6-item CRT, general science knowledge quiz, Medical Maximizer-Minimizer scale, political ideology on both social and fiscal issues, and Democrat versus Republican Party alignment",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study 1 attempted to replicate the finding that subtly making the concept of accuracy salient increased the quality of COVID-19 information that people were willing to share online.","Study 2 recruited additional 882 participants to attempt replication of the finding that subtly making the concept of accuracy salient increased the quality of COVID-19 information that people were willing to share online."],"N":[701,1583],"Effect size":["(no standard effect size available/yet extracted)","Cohen's d: 0.14 (95%-CI: 0.12-0.17)"],"Comments":["No significant interaction between headline veracity and treatment, β = 0.0046, 95% confidence interval (CI) = [−0.016, 0.026], F(3, 21030) = 1.53, p = .67","Significant interaction effect between headline veracity and treatment, β = 0.015, 95% CI = [0.0027, 0.027], F(3, 47490) = 4.52, treatment-group effect size: d = −0.14, 95% CI = [−0.17, −0.12]. The effect size for the control group was directionally similar (d = −0.10, 95% CI = [−0.13, −0.078]), but sharing discernment was still 1.4 times higher in the treatment group than in the control group, an attenuation of about 50% compared with the effect sizes reported in the target study."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"903c5cf01ac1df3f9272124db8725883","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Accuracy prompts"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Pennycook, G., & Rand, D.G (2022). Accuracy prompts are a replicable and generalizable approach for reducing the spread of misinformation. Nature Communications, 13(1), Article 2333."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1038/s41467-022-30073-5"},"children":["https://doi.org/10.1038/s41467-022-30073-5"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/4mv9z/"},"children":["https://osf.io/4mv9z/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Interventions that shift users attention toward the concept of accuracy represent a promising approach for reducing misinformation sharing online. We assess the replicability and generalizability of this accuracy prompt effect by meta-analyzing 20 experiments (with a total N = 26,863) completed by our group between 2017 and 2020. This internal meta-analysis includes all relevant studies regardless of outcome and uses identical analyses across all studies. Overall, accuracy prompts increased the quality of news that people share (sharing discernment) relative to control, primarily by reducing sharing intentions for false headlines by 10% relative to control in these studies. The magnitude of the effect did not significantly differ by content of headlines (politics compared with COVID-19 related news) and did not significantly decay over successive trials. The effect was not robustly moderated by gender, race, political ideology, education, or value explicitly placed on accuracy, but was significantly larger for older, more reflective, and more attentive participants. This internal meta-analysis demonstrates the replicability and generalizability of the accuracy prompt effect on sharing discernment.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In this internal meta-analysis of 20 experiments of accuracy prompt experiments conducted between 2017 and 2020, internal lab records were searched to identify raw data for studies satisfying the inclusion and exclusion criteria. Key dimensions of variation across included studies: the subject pool from which the participants were recruited, the topic of the headlines about which the participants made sharing decisions, the specific set of headlines shown, and the particular set of accuracy prompts employed. The meta-analysis examined how the effect of the accuracy prompts varied across these dimensions; it also examined how various individual-level variables moderated the effect of the accuracy prompts.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"False (source: snopes.com, factcheck.org) and true (mainstream news sources) news headlines in the format of a Facebook post",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Political orientation, preference for the Democratic versus Republican party, voting for Donald Trump in the 2016 U.S. Presidential Election, CRT, importance that participants self-reported placing on only sharing accurate news on social media",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Internal meta-analysis of 20 studies conducted between 2017 and 2020 aimed to gauge an effect of the various accuracy prompts on sharing discernement across all experiments."],"N":[26863],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Accuracy prompts significantly increased sharing discernment (interaction between headline veracity and treatment dummies: b = 0.038, z = 7.102, p < 0.001), which translates into a 71.7% increase over the meta-analytic estimate of baseline sharing discernment in the control condition (headline veracity dummy: b = 0.053, z = 6.636, p < 0.001). This increase in discernment was driven by accuracy prompts significantly decreasing sharing intentions for false news (treatment dummy: b = −0.034, z = 7.851, p < 0.001; Fig. 2), which translates into a 10% decrease relative to the meta-analytic estimate of baseline sharing intentions for false news in the control condition (intercept: b = 0.341, z = 15.695, p < 0.001)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"923e36398cc304a4b814262abb09e210","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Accuracy prompts"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Arechar, A. A., Allen, J. N. L., Berinsky, A. J., Cole, R., Epstein, Z., Garimella, K., Gully, A., Lu, J. G., Moss, R. M., Stagnaro, M. N., Zhang, Y., Pennycook, G., & Rand, D. G. (2022). Understanding and combatting COVID-19 misinformation across 16 countries on six continents. PsyArXiv. (Preprint, not peer-reviewed)"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.31234/osf.io/a9frz"},"children":["https://doi.org/10.31234/osf.io/a9frz"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://psyarxiv.com/a9frz/"},"children":["https://psyarxiv.com/a9frz/"]},"(USA)",{"name":"a","attribs":{"href":"https://osf.io/mtnfr/"},"children":["https://osf.io/mtnfr/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The spread of misinformation online is a global problem that requires global solutions. To that end, we conducted an experiment in 16 countries across 6 continents (N = 33,480) to investigate predictors of susceptibility to misinformation and interventions to combat misinformation. In every country, participants with a more analytic cognitive style and stronger accuracy-related motivations were better at discerning truth from falsehood; valuing democracy was also associated with greater truth discernment whereas political conservatism was negatively associated with truth discernment in most countries. Subtly prompting people to think about accuracy was broadly effective at improving the veracity of news that people were willing to share, as were minimal digital literacy tips. Finally, crowdsourced accuracy evaluation was able to differentiate true from false headlines with high accuracy in all countries. The consistent patterns we observe suggest that the psychological factors underlying the misinformation challenge are similar across the globe, and that similar solutions may be broadly effective.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Presented 20 COVID-related headlines (10 true, 10 false) and asked participants whether they would be willing to share the headline on social media. Participants were randomly assigned to 1 of 4 experimental conditions: accuracy, sharing, prompt, or tips. Sharing, prompt, and tips: Participants were asked about their likelihood of sharing such content on social media. Accuracy: Participants rated the level of accuracy.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"10 true and 10 false news headlines, randomly sampled from a larger set of 45 headlines (of which 30 were false).",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"self-reported preference for analytic thinking, CRT",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Participants were randomly assigned to one of the four conditions: Accuracy, Sharing, Prompt, and Tips. Here we include only results for the Prompt condition. But sharing discernment was also higher in the Tips condition compared to the baseline Sharing condition (meta-analytic estimate, b=0.076, z=4.30, p<0.001), and the magnitude of this effect did not significantly vary across countries (χ2=14.54, p=0.485).",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1 across 16 countries"],"Description":["Study conducted in 16 countries across 6 continents experimentally tested whether subtly shifting attention to accuracy increases the veracity of the news people are willing to share."],"N":[33480],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Prompt condition increased sharing discernment relative to the baseline Sharing condition (meta-analytic estimate, b=0.171, z=4.61, p<0.001). There was significant variation across countries in the magnitude of this effect (χ2=58.57, p<0.001)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"f0d7354f2a9e5b0d338a6f6a404ecbdc","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Ecker, U. K. H., Butler, L. H., & Hamby, A. (2020). You don't have to tell a story! A registered report testing the effectiveness of narrative versus non-narrative misinformation corrections. Cognitive Research: Principles and Implications, 5(1), Article 64."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1186/s41235-020-00266-x"},"children":["https://doi.org/10.1186/s41235-020-00266-x"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/gtm9z/"},"children":["https://osf.io/gtm9z/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation often has an ongoing effect on people’s memory and inferential reasoning even after clear corrections are provided; this is known as the continued influence effect. In pursuit of more effective corrections, one factor that has not yet been investigated systematically is the narrative versus non-narrative format of the correction. Some scholars have suggested that a narrative format facilitates comprehension and retention of complex information and may serve to overcome resistance to worldview-dissonant corrections. It is, therefore, a possibility that misinformation corrections are more effective if they are presented in a narrative format versus a non-narrative format. The present study tests this possibility. We designed corrections that are either narrative or non-narrative, while minimizing differences in informativeness. We compared narrative and non-narrative corrections in three preregistered experiments (total N = 2279). Experiment 1 targeted misinformation contained in fictional event reports; Experiment 2 used false claims commonly encountered in the real world; Experiment 3 used real-world false claims that are controversial, in order to test the notion that a narrative format may facilitate corrective updating primarily when it serves to reduce resistance to correction. In all experiments, we also manipulated test delay (immediate vs. 2 days), as any potential benefit of the narrative format may only arise in the short term (if the story format aids primarily with initial comprehension and updating of the relevant mental model) or after a delay (if the story format aids primarily with later correction retrieval). In all three experiments, it was found that narrative corrections are no more effective than non-narrative corrections. Therefore, while stories and anecdotes can be powerful, there is no fundamental benefit of using a narrative format when debunking misinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In 3 experiments, narrative (N) and non-narrative (NN, fact-based) corrections were contrasted. Experiment 1: RCT (2 control conditions and the 2 treatment conditions, N and NN, mixed within-between design, with the within-subjects factor of condition, and the between-subjects factor of test delay). Experiments 2 and 3: 2 × 2 mixed within–between design, with the within-subjects factor of correction type (NN; N) and the between-subjects factor of test delay (immediate; delayed).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 1: fictitious event reports,\nStudy 2: false claims encountered in the real world,\nStudy 3: controversial real-world false claims selected to be congruent with a conservative worldview",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Memory test for the purposes of adequate encoding of test items",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2","3"],"Description":["Experiment 1 investigated whether corrections of event-related misinformation are more effective if presented in a narrative format. Experiment 1 targeted misinformation contained in fictional event reports in four conditions. There were two control conditions: One featured no misinformation (noMI condition), another featured a piece of misinformation that was not corrected (noC condition). The two experimental conditions corrected the initially-provided misinformation using either a non-narrative (NN) or narrative (N) correction. The test phase followed the study phase either immediately or after a 2-day delay.","Experiment 2 tested whether corrections targeting real-world misconceptions are more effective if they are provided in a narrative versus non-narrative format. Experiment 2 used false claims commonly encountered in the real world, including both true facts and common misconceptions (myths). Claims were followed by explanations that affirmed the facts and corrected the myths. Corrections were either in a non-narrative (NN) or narrative (N) form, and the test was again either immediate or delayed.","Experiment 3 tested whether narrative corrections would be more effective than non-narrative corrections when debunking worldview-consistent misconceptions. Experiment 3 used real-world false claims that are controversial, including both facts and myths, which were followed by affirmations and corrections. Corrections were again either non-narrative (NN) or narrative (N), and the test was immediate or delayed."],"N":[770,776,733],"Effect size":["Partial-Eta squared (η2p): 0.237 (immediate, non-narrative)","Partial-Eta squared (η2p): 0.604 (immediate, non-narrative)","Partial-Eta squared (η2p): 0.386 (immediate, non-narrative)"],"Comments":["Corrections reduced misinformation reliance in inferences regardless of format, both immediately and after a 2-day delay. η2p = .237 (immediate, non-narrative) / .245 (immediate, narrative) / .134 (delayed, non-narrative) / .127 (delayed, narrative) [note these are from contrasts that take full ANOVA model into account; effect sizes from isolated contrasts are larger, .211 - .415].","Corrections reduced misinformation beliefs and misinformation reliance in inferences regardless of format, both immediately and after a 2-day delay.  η2p = .604 (immediate, non-narrative) / .578 (immediate, narrative) / .506 (delayed, non-narrative) / .480 (delayed, narrative) [not reported in paper; calculated from one-sample t-test of belief change against zero; note zero may not be most appropriate baseline as it does not take demand characteristics into account].","Corrections slightly reduced misinformation beliefs and misinformation reliance in inferences regardless of format, both immediately and after a 2-day delay. η2p = .386 (immediate, non-narrative) / .334 (immediate, narrative) / .196 (delayed, non-narrative) / .185 (delayed, narrative) [not reported in paper; calculated from one-sample t-test of belief change against zero; note zero may not be most appropriate baseline as it does not take demand characteristics into account]."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"dd0fb158aae99737d783f24be90e8a92","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Paynter, J., Luskin-Saxby, S., Keen, D., Fordyce, K., Frost, G., Imms, C., Miller, S., Trembath, D., Tucker, M., & Ecker, U. K. H. (2019). Evaluation of a template for countering misinformation—Real-world autism treatment myth debunking. PLOS ONE, 14(1), Article e0210746."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0210746"},"children":["https://doi.org/10.1371/journal.pone.0210746"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0210746"},"children":["https://doi.org/10.1371/journal.pone.0210746"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation poses significant challenges to evidence-based practice. In the public health domain specifically, treatment misinformation can lead to opportunity costs or direct harm. Alas, attempts to debunk misinformation have proven sub-optimal, and have even been shown to “backfire”, including increasing misperceptions. Thus, optimized debunking strategies have been developed to more effectively combat misinformation. The aim of this study was to test these strategies in a real-world setting, targeting misinformation about autism interventions. In the context of professional development training, we randomly assigned participants to an “optimized-debunking” or a “treatment-as-usual” training condition and compared support for non-empirically-supported treatments before, after, and six weeks following completion of online training. Results demonstrated greater benefits of optimized debunking immediately after training; thus, the implemented strategies can serve as a general and flexible debunking template. However, the effect was not sustained at follow-up, highlighting the need for further research into strategies for sustained change.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Compared existing training materials aiming to reduce support for (and ultimately use of) non-empirically supported autism treatments with an optimized debunking treatment in early-childhood intervention staff immediately and after a 6-week delay.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Extensive training materials refuting fad autism treatments in the form of text, charts, images and aimed to build trust, convey social norms, affirm participants' identity, and communicate the evidence clearly.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Attitudes towards evidence-based practice; deference to scientific authority; perceived social validity of intervention",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Study experimentally compared existing training materials aiming to reduce support for (and ultimately use of) non-empirically-supported autism treatments with an optimized debunking treatment in early-childhood intervention staff immediately and after a 6-week delay."],"N":[856],"Effect size":["Partial-Eta squared (η2p): 0.66 (immediate)"],"Comments":["η2p = .66 (immediate) [effect size of interaction, i.e. comparison to treatment-as-usual  intervention: η2p = .20] / .45 (delayed) [note the latter is not reported in the paper as there was no longer a significant difference to the treatment-as-usual condition]."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"859f052e36cfbaac8b55a3f2f4f08bd4","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Tay, L. Q., Hurlstone, M. J., Kurz, T., & Ecker, U. K. H. (2022). A comparison of prebunking and debunking interventions for implied versus explicit misinformation. British Journal of Psychology, 113(3), 591–607."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1111/bjop.12551"},"children":["https://doi.org/10.1111/bjop.12551"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/dktnu/"},"children":["https://osf.io/dktnu/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Psychological research has offered valuable insights into how to combat misinformation. The studies conducted to date, however, have three limitations. First, pre-emptive (“prebunking”) and retroactive (“debunking”) interventions have mostly been examined in parallel, and thus it is unclear which of these two predominant approaches is more effective. Second, there has been a focus on misinformation that is explicitly false, but implied misinformation that uses literally true information to mislead is common in the real world. Finally, studies have relied mainly on questionnaire measures of reasoning, neglecting behavioural impacts of misinformation and interventions. To offer incremental progress towards addressing these three issues, we conducted an experiment (N = 735) involving misinformation on fair trade. We contrasted the effectiveness of prebunking versus debunking and the impacts of implied versus explicit misinformation, and incorporated novel measures assessing consumer behaviours (i.e., willingness-to-pay; information seeking; online misinformation promotion) in addition to standard questionnaire measures. In general, both prebunking and debunking reduced misinformation reliance. We also found that individuals tended to rely more on explicit than implied misinformation both with and without interventions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Tested effectiveness of (identical) prebunking and debunking interventions with implied and explicit misinformation.The experiment adopted a 2 (misinformation type: implied, explicit) × 3 (intervention type: no intervention, prebunking, debunking) plus control between-subjects design.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Fictional articles for control, implied misinformation, and explicit misinformation on a real-world topic (fair trade).",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Social media behavior, tweet-sentiment",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","1"],"Description":["Study experimentally tested effects of prebunking and debunking interventions on reliance on misinformation, willingness-to-purchase, and tweet sentiment","Study experimentally tested effects of prebunking and debunking interventions on reliance on misinformation, willingness-to-purchase, and tweet sentiment"],"N":[856,856],"Effect size":["Cohen’s d: 0.89","Cohen’s d: 0.46"],"Comments":["Both debunking and prebunking significantly reduced participants’ number of references to misinformation, with t(625) = −9.60, p = <.001, d = .89, and t(625) = −8.08, p = <.001, d = .73, respectively. The difference between debunking and prebunking conditions was not significant, t(625) = −1.61, p = .108, d = .28.","Debunking resulted in more positive sentiments compared to the no-intervention condition, t(625) = 4.65, p = <.001, d = .46, as well as compared to the prebunking condition, t(625) = 2.44, p = .030, d = .24. There was also a significant difference between prebunking and no-intervention conditions, with prebunking resulting in more positive sentiments, t(625) = 2.23, p = .030, d = .22."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"d4621ff773b84f248515ee3b12b4534b","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Swire, B., Ecker, U. K. H., & Lewandowsky, S. (2017). The role of familiarity in correcting inaccurate information. Journal of Experimental Psychology: Learning, Memory, and Cognition, 43(12), 1948–1961."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1037/xlm0000422"},"children":["https://doi.org/10.1037/xlm0000422"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"People frequently continue to use inaccurate information in their reasoning even after a credible retraction has been presented. This phenomenon is often referred to as the continued influence effect of misinformation. The repetition of the original misconception within a retraction could contribute to this phenomenon, as it could inadvertently make the “myth” more familiar—and familiar information is more likely to be accepted as true. From a dual-process perspective, familiarity-based acceptance of myths is most likely to occur in the absence of strategic memory processes. Thus, we examined factors known to affect whether strategic memory processes can be utilized: age, detail, and time. Participants rated their belief in various statements of unclear veracity, and facts were subsequently affirmed and myths were retracted. Participants then rerated their belief either immediately or after a delay. We compared groups of young and older participants, and we manipulated the amount of detail presented in the affirmative or corrective explanations, as well as the retention interval between encoding and a retrieval attempt. We found that (a) older adults over the age of 65 were worse at sustaining their postcorrection belief that myths were inaccurate, (b) a greater level of explanatory detail promoted more sustained belief change, and (c) fact affirmations promoted more sustained belief change in comparison with myth retractions over the course of 1 week (but not over 3 weeks), This supports the notion that familiarity is indeed a driver of continued influence effects.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants (in one study, aged 18–30; in a second study, aged 50+) rated statements both before and after misinformation corrections and factual affirmations. Corrections and affirmations were presented briefly (with no explanation) or in some detail (with an explanation of why the claims were true or false). Participants re-rated their belief immediately afterward, 30 minutes later, one week later, or three weeks later. Experiments used a 2 × 2 × 3 within–between design, with within-subjects factors type of item (myth, fact) and type of explanation (the veracity of each statement was explained either briefly or in some detail), and the between-subjects factor retention interval (immediate, 30 minutes, 1 week).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"40 misinformation and factual statements about different topics",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Direct belief ratings and inference questions",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study 1 experimentally tested effects of corrections on people’s belief in true and false statements. The experiment used a 2 × 2 × 3 within-between design, with within-subjects factors type of item (myth vs. fact) and type of explanation (the veracity of each statement was explained either briefly or in some detail), and the between-subjects factor retention interval (immediate, 30-min, or 1-week).","Experiment 2 was a conceptual replication of Experiment 1 but tested older adults. An additional 3-week retention interval condition was also added."],"N":[93,109],"Effect size":["Partial-Eta squared (η2p): 0.15 (Brief/detailed explanation)","Partial-Eta squared (η2p): 0.12 (Brief/detailed explanation)"],"Comments":["Belief rating η2p = .15 (Brief/detailed explanation); .10 (retention interval); Inference question η2p = .16 (Brief/detailed explanation); .12 (retention interval)","Belief rating η2p = .12 (Brief/detailed explanation); .25 (retention interval); Inference question η2p = .12 (Brief/detailed explanation); .16 (retention interval)"]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"892a842f5d796825b823ad03d64d9ff1","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Friction"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Fazio, L. K. (2020). Pausing to consider why a headline is true or false can help reduce the sharing of false news.Harvard Kennedy School (HKS) Misinformation Review."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.37016/mr-2020-009"},"children":["https://doi.org/10.37016/mr-2020-009"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/mu7n8/"},"children":["https://osf.io/mu7n8/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"In an online experiment, participants who paused to explain why a headline was true or false indicated that they were less likely to share false information compared to control participants. Their intention to share accurate news stories was unchanged. These results indicate that adding “friction” (i.e., pausing to think) before sharing can improve the quality of information shared on social media.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants indicated how likely they would be to share true and false news headlines. Half of the participants were asked to explain how they knew that the headline was true or false before providing each rating. The experiment had a 2 (repetition: repeated, new) × 2 (task: control, explain) mixed design. Repetition was manipulated within-subjects; the participants’ task during the share phase was manipulated between-subjects.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"24 real political headlines from Pennycook, Cannon, and Rand (2018), available at  at https://osf.io/txf46/. Stimuli included an equal number of true/false and pro-democrat/pro-republican headlines. Across participants, researchers counterbalanced which set of 12 was repeated (presented during the exposure and share phase) and new (presented only during the share phase).",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Study experimentally tested whether asking participants to explain why a headline was true or false would affect their intentions to share true and false headlines."],"N":[501],"Effect size":["Cohen’s d: 0.27"],"Comments":["In the control condition, over half of the participants (57%) indicated that they would be “likely”, “somewhat likely” or “extremely likely” to share at least one false headline. However, in the explanation condition, only 39% indicated that they would be at least “likely” to share one or more false headlines. A similar decrease occurred in the number of people who indicated that they would be “extremely likely” to share at least one false headline (24% in control condition; 17% in explanation condition)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"0a9682238e2ac01292d0ffcc1676747d","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Roozenbeek, J., & van der Linden, S. (2019). Fake news game confers psychological resistance against online misinformation. Palgrave Communications, 5(1),Article 65."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1057/s41599-019-0279-9"},"children":["https://doi.org/10.1057/s41599-019-0279-9"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://figshare.com/articles/dataset/Bad_News_Dataset/8269763"},"children":["https://figshare.com/articles/dataset/Bad_News_Dataset/8269763"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The spread of online misinformation poses serious challenges to societies worldwide. In a novel attempt to address this issue, we designed a psychological intervention in the form of an online browser game. In the game, players take on the role of a fake news producer and learn to master six documented techniques commonly used in the production of misinformation: polarisation, invoking emotions, spreading conspiracy theories, trolling people online, deflecting blame, and impersonating fake accounts. The game draws on an inoculation metaphor, where preemptively exposing, warning, and familiarising people with the strategies used in the production of fake news helps confer cognitive immunity when exposed to real misinformation. We conducted a large-scale evaluation of the game with N = 15,000 participants in a pre-post gameplay design. We provide initial evidence that people’s ability to spot and resist misinformation improves after gameplay, irrespective of education, age, political ideology, and cognitive style.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"One study (n = 14,163), within-subjects pre–post design. Participants were Bad News game players who took part in an in-game survey at the start and end of the game. Participants rated 6 items (2 \"real news,\" 4 \"fake news\") on a 1–7 reliability scale.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"2 true (non-manipulative and factual) and 4 misleading simulated Twitter posts",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"CRT (ball & bat question), age, gender, education, political ideology",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The treatment questions reflected a random sample of the strategies included in the game: impersonation, conspiracy, and discrediting.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This study experimentally tested whether learning common misinformation techniques through an inoculation game would impact people's recognition of these techniques as reflected in the judgements of headlines reliability."],"N":[14163],"Effect size":["Cohen’s d: 0.33"],"Comments":["The process of active inoculation through playing the Bad News game significantly reduced the perceived reliability of tweets that embedded several common online misinformation strategies. The observed effect sizes range from small to moderate, and are in line with the average effect-size in the context of research on resistance to persuasion."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"cd77d2b2120d76b299e364e90f45362c","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Basol, M., Roozenbeek, J., Berriche, M., Uenal, F., McClanahan, W. P., & van der Linden, S. (2021). Towards psychological herd immunity: Cross-cultural evidence for two prebunking interventions against COVID-19 misinformation. Big Data & Society, 8(1), Article 20539517211013868."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/20539517211013868"},"children":["https://doi.org/10.1177/20539517211013868"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/mbqwj/"},"children":["https://osf.io/mbqwj/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation about the novel coronavirus (COVID-19) is a pressing societal challenge. Across two studies, one preregistered (n1 = 1771 and n2 = 1777), we assess the efficacy of two ‘prebunking’ interventions aimed at improving people’s ability to spot manipulation techniques commonly used in COVID-19 misinformation across three different languages (English, French and German). We find that Go Viral!, a novel five-minute browser game, (a) increases the perceived manipulativeness of misinformation about COVID-19, (b) improves people’s attitudinal certainty (confidence) in their ability to spot misinformation and (c) reduces self-reported willingness to share misinformation with others. The first two effects remain significant for at least one week after gameplay. We also find that reading real-world infographics from UNESCO improves people’s ability and confidence in spotting COVID-19 misinformation (albeit with descriptively smaller effect sizes than the game). Limitations and implications for fake news interventions are discussed.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Two studies for the Go Viral game:Study 1 implemented voluntary in-game pre–post test and Study 2 was an RCT with 3 conditions (Go Viral, UNESCO infographics, control) across 3 languages (English, German, French).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 1: 3 true and 3 misleading social media posts, Study 2: 9 true and 9 misleading social media posts (source: real - BBC, Reuters, AP, misleading - fact-checking websites)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Study 1: age, gender, education, political ideology, geographic origin. Study 2: age, gender, education, political ideology, COVID vaccination intentions, counterarguing, (motivational) threat measures.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2.  Go Viral! treatment condition","2. Infographics treatment condition"],"Description":["Study 1 experimentally tested whether playing the Go Viral! game will impact people's recognition of manipulativeness of misinformation about COVID-19.","This part of study 2 experimentally tested whether playing the Go Viral! game will impact people's recognition of manipulativeness of misinformation about COVID-19, as well as their confidence in their judgements and willingness to share misinformation.","This part of study 2 experimentally tested whether exposing people to prebunking Infographics will impact people's recognition of manipulativeness of misinformation about COVID-19, as well as their confidence in their judgements and willingness to share misinformation"],"N":[1771,1777,1777],"Effect size":["Cohen’s d: 0.52","Cohen’s d: 0.56","Cohen’s d: 0.17"],"Comments":["Participants, who played Go Viral!, irrespective of their demographic background (aside from political ideology), found misinformation about COVID-19 significantly more manipulative after playing than before, whereas their assessment of real news did not change in a meaningful sense. The effect sizes are in line with previous studies that have used similar designs.","Go Viral game compared to control condition: d=0.56 (manipulativeness), d=0.44 (confidence), d=0.15 (sharing). The game 1) reduces the perceived reliability of misinformation, 2) increases people's confidence in their assessment of the reliability of misinformation, and 3) reduces intentions to share misinformation with others.","Prebunking infographics compared to control condition: d=0.17 (manipulativeness), d=0.15 (confidence), sharing n.s. This indicates that reading through the UNESCO infographics significantly increases the perceived manipulativeness of COVID-19 misinformation, as well as confidence in their assessment of misinformation manipulativeness. However, for willingness to share there is no significant difference between the Infographics condition and the control group nor the Go Viral! condition. These results are similar (and significant) in all three countries."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"f0ca699f8340625aaa216ddc09ef4e77","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Roozenbeek, J., & van der Linden, S. (2020). Breaking Harmony Square: A game that “inoculates” against political misinformation. Harvard Kennedy School (HKS) Misinformation Review."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.37016/mr-2020-47"},"children":["https://doi.org/10.37016/mr-2020-47"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/r89h3/"},"children":["https://osf.io/r89h3/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"We present Harmony Square, a short, free-to-play online game in which players learn how political misinformation is produced and spread. We find that the game confers psychological resistance against manipulation techniques commonly used in political misinformation: players from around the world find social media content making use of these techniques significantly less reliable after playing, are more confident in their ability to spot such content, and less likely to report sharing it with others in their network.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"To test whether playing the Harmony Square game (1) reduces the perceived reliability of misinformation, (2) increases people's confidence in their assessment of the reliability of misinformation, and (3) reduces intentions to share misinformation with others, the study used a 2 (treatment, control) × 2 (pre, post) mixed-design RCT.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"16 simulated misleading social media posts containing manipulation techniques learned in the game (8 real fake news found on social media and fake news sites and 8 fictional fake news items created for the study). Topic: a mix of politically partisan and politically neutral content, political items included an equal number of right-leaning and left-leaning items.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Age, gender, education, political ideology, social media use, political interest, how often people check the news, reliability judgments of social media posts containing misinformation",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The purpose of the Harmony Square game was not to learn how to distinguish high-quality and low-quality content, but rather to teach people how to spot common types of misinformation on social media. For that reason, the studies tests only included manipulative (fake) items, both real and fictional.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study experimentally tested whether playing the Harmony Square game 1) reduces the perceived reliability of misinformation, 2) increases people's confidence in their assessment of the reliability of misinformation, and 3) reduces intentions to share misinformation with others."],"N":[681],"Effect size":["Cohen’s d: 0.51"],"Comments":["Reliability of real fake news (d = 0.51), experimenter-designed fake news (d = 0.54), confidence (d = 0.30, d = 0.30), and less sharing (d = 0.28, d = 0.27)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"70892350386900569bf7159b74572578","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Maertens, R., Roozenbeek, J., Basol, M., & van der Linden, S. (2021). Long-term effectiveness of inoculation against misinformation: Three longitudinal experiments. Journal of Experimental Psychology: Applied, 27(1), 1–16."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1037/xap0000315"},"children":["https://doi.org/10.1037/xap0000315"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/2dtkb/"},"children":["https://osf.io/2dtkb/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"This study investigates the long-term effectiveness of active psychological inoculation as a means to build resistance against misinformation. Using 3 longitudinal experiments (2 preregistered), we tested the effectiveness of Bad News, a real-world intervention in which participants develop resistance against misinformation through exposure to weakened doses of misinformation techniques. In 3 experiments (NExp1 = 151, NExp2 = 194, NExp3 = 170), participants played either Bad News (inoculation group) or Tetris (gamified control group) and rated the reliability of news headlines that either used a misinformation technique or not. We found that participants rate fake news as significantly less reliable after the intervention. In Experiment 1, we assessed participants at regular intervals to explore the longevity of this effect and found that the inoculation effect remains stable for at least 3 months. In Experiment 2, we sought to replicate these findings without regular testing and found significant decay over a 2-month time period so that the long-term inoculation effect was no longer significant. In Experiment 3, we replicated the inoculation effect and investigated whether long-term effects could be due to item-response memorization or the fake-to-real ratio of items presented, but found that this is not the case. We discuss implications for inoculation theory and psychological research on misinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Three longitudinal experiments (two preregistered) investigated the long-term effectiveness of gamified inoculation (operationalized as the Bad News Game). Participants played either Bad News (inoculation group) or Tetris (gamified control group) and rated the reliability of news headlines that either used a misinformation technique or did not. Online pre–post and between-groups experiment with the Bad News Game embedded into a Qualtrics survey, run through Prolific. Study 1: Follow-up after 1 week, 5 weeks, and 13 weeks. Study 2: Follow-up after 9 weeks. Study 3: Follow-up after 1 week.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 1, 2: 18 misleading posts and 3 non-misleading Twitter-like posts, Study 3: 6 misleading and 1 non-misleading (pretest) Twitter-like posts, 6 misleading and 6 non-misleading (posttest) Twitter-like posts (source: expert team, based on DEPICT framework)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Age, gender, political ideology (from 1–7, very left-wing to very right-wing), country of residence, first language, social media usage (from 1–5, never to daily), and a single-item cognitive reflection test (the bat and the ball)",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2","3"],"Description":["In Experiment 1, researchers aimed to test the hypothesis that inoculation effects are subject to decay in repeated measures over time. T1 = pretest; T2 = posttest (0 weeks); T3 = posttest (1 week); T4 = posttest (5 weeks); T5 = posttest (13 weeks).","In Experiment 2, researchers eliminated the confound of repeated measurement by removing all follow-ups between the direct posttest and the posttest 2 months later.","In Experiment was identical to Experiment 1 (up to T3, the first follow-up) but changed both the item set and fake-to-real ratio for the follow-up measure. It also omitted the control group."],"N":[118,110,87],"Effect size":["Cohen’s d: 1","Cohen’s d: 0.69","Cohen’s d: 0.72 (95%-CI: 0.48-0.95)"],"Comments":["The inoculation effects were significant over all 4 testing points: from the immediate post-test (d = -1.0) to 13 weeks delayed test). Researchers hypothesized that the repeated tests might have confounded the result as they could function as booster sessions or simply testing effects.","The inoculation effect decays over the course of 9 weeks, rendering the effect no longer significant. The analyses also show that the decay is only partial. Treatment effect at T2 (immediate post-test): d = 0.69. Between T2 and T3 (in 2 months): non-significant inoculation effect retention of 36%, d = −0.35.","The inoculation retention over a 1-week period was similar between the two experimental setups (Exp 1 and 3), thereby finding no evidence for item ratio or item set specific retention effects."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"b8e40ab4f16b8c73d5ec3ab012bab2fe","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["van der Linden, S., Leiserowitz, A., Rosenthal, S., & Maibach, E. (2017). Inoculating the public against misinformation about climate change. Global Challenges, 1(2), Article 1600008."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1002/gch2.201600008"},"children":["https://doi.org/10.1002/gch2.201600008"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Effectively addressing climate change requires significant changes in individual and collective human behavior and decision-making. Yet, in light of the increasing politicization of (climate) science, and the attempts of vested-interest groups to undermine the scientific consensus on climate change through organized “disinformation campaigns,” identifying ways to effectively engage with the public about the issue across the political spectrum has proven difficult. A growing body of research suggests that one promising way to counteract the politicization of science is to convey the high level of normative agreement (“consensus”) among experts about the reality of human-caused climate change. Yet, much prior research examining public opinion dynamics in the context of climate change has done so under conditions with limited external validity. Moreover, no research to date has examined how to protect the public from the spread of influential misinformation about climate change. The current research bridges this divide by exploring how people evaluate and process consensus cues in a polarized information environment. Furthermore, evidence is provided that it is possible to pre-emptively protect (“inoculate”) public attitudes about climate change against real-world misinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In Study 1 (N = 1,000), a persuasive myth about climate change was identified. In Study 2 (N = 2,167) Americans were randomly assigned to 1 of 6 experimental conditions—a pure control group, a facts-only condition, a misinformation-only condition, a false balance condition, a forewarning-only condition, and a full inoculation condition (forewarning + preemptive refutation)—before they were exposed to misinformation about climate change.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"97% scientific consensus (facts) and Oregon Global Warming Petition (misinformation)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Climate attitudes",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"This paper features 2 studies. We only include intervention Study 2 here. The purpose of Study 1 was to identify the most influential and representative “countermessages” used by climate change opponents.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["2"],"Description":["Study 2 experimentally tested  whether it is possible to “inoculate” people against climate misinformation."],"N":[2167],"Effect size":["Cohen’s d: 0.75"],"Comments":["The consensus-treatment (CT) alone elicited a large increase in perceived scientific agreement (d = 1.23 relative to control). In contrast, the (misinformation) countermessage (CM) had a substantial negative influence (d = 0.48) when presented on its own. When participants viewed the messages sequentially (CT | CM), the informational value of the consensus-treatment was negated completely (d = 0.04). As hypothesized, the general (In1 | CM) and detailed (In2 | CM) inoculation interventions were each successful in preserving much of the positive effect of the consensus message in the presence of counterinformation (d= 0.33 and 0.75 or one-third and two-thirds of the initial consensus-treatment effect, respectively)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"ccdeb02163336767c835ee4493f8c10a","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Cook J., Lewandowsky, S., & Ecker, U. K. H. (2017) Neutralizing misinformation through inoculation: Exposing misleading argumentation techniques reduces their influence. PLOS ONE, 12(5), Article e0175799."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0175799"},"children":["https://doi.org/10.1371/journal.pone.0175799"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://datadryad.org/stash/dataset/doi:10.5061/dryad.f17j3"},"children":["https://datadryad.org/stash/dataset/doi:10.5061/dryad.f17j3"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation can undermine a well-functioning democracy. For example, public misconceptions about climate change can lead to lowered acceptance of the reality of climate change and lowered support for mitigation policies. This study experimentally explored the impact of misinformation about climate change and tested several pre-emptive interventions designed to reduce the influence of misinformation. We found that false-balance media coverage (giving contrarian views equal voice with climate scientists) lowered perceived consensus overall, although the effect was greater among free-market supporters. Likewise, misinformation that confuses people about the level of scientific agreement regarding anthropogenic global warming (AGW) had a polarizing effect, with free-market supporters reducing their acceptance of AGW and those with low free-market support increasing their acceptance of AGW. However, we found that inoculating messages that (1) explain the flawed argumentation technique used in the misinformation or that (2) highlight the scientific consensus on climate change were effective in neutralizing those adverse effects of misinformation. We recommend that climate communication messages should take into account ways in which scientific content can be distorted, and include pre-emptive inoculation messages.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Study 1 tested the effect of inoculation against misinformation that takes the form of \"false balance\" media coverage of climate change. Participants were randomly assigned to 1 of 5 groups: a control group or one of 4 groups that saw misinformation. For the 4 misinformation groups, consensus information and inoculation information were fully crossed so that prior to the misinformation, participants read consensus information, inoculation information, a message combining both consensus and inoculation information, or no message. Study 2 tested the impact of misinformation that explicitly seeks to manufacture doubt about the scientific consensus on climate change. It had a 2 × 2 between-subjects design, fully crossing a misinformation intervention and an inoculation intervention such that participants were divided into a control group (no intervention text), an inoculation group (inoculation with no misinformation), a misinformation group (misinformation with no inoculation), and an inoculation/misinformation group (inoculation preceding misinformation).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"1 mock news article featuring scientists presenting research supporting AGW and rejecting AGW with alternative explanations",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"AGW acceptance, free-market support, trust in climate scientists, trust in contrarian scientists, attribution of long-term climate trends to human activity, perceived consensus, and mitigative climate policy support.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Experiment 1 tested the effect of inoculation against misinformation that takes the form of ‘false balance’ media coverage regarding climate change.","Experiment 2 tested the impact of misinformation that explicitly seeks to manufacture doubt about the scientific consensus on climate change (fake experts). Experiment 2 also tested whether inoculating participants prior to reading misinformation was effective in neutralizing the influence of the misinformation."],"N":[751,400],"Effect size":["Eta squared: 0.01","Eta squared: 0.01"],"Comments":["Experiment 1 found that pre-emptively explaining the potentially misleading effect of false-balance media coverage was effective in neutralizing the negative influence of that type of misleading media coverage. Relative to perceived consensus in the control group (70%), Misinformation only: 63,5%, Consensus + Misinformation: 86%; Inoculation + Misinformation = 70%, Inoculation + Consensus + Misinformation = 84%","Experiment 2 demonstrated that misinformation—in the form of fake experts casting doubt on a scientific consensus—had a polarizing effect on climate attitudes, such that people with low free-market support increased climate acceptance, while people with high free-market support decreased climate acceptance. However, an inoculating message that explains the misinforming technique without mentioning any specifics fully neutralized the polarizing effect of misinformation. Relative to perceived consensus in the control group (54,5%), Misinformation only: 44,5%, Inoculation only: 50,4%; Inoculation + Misinformation:51,6%"]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"6a773cb0665db5ac2c381960f626eea6","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Roozenbeek, J., van der Linden, S., Goldberg, B., Rathje, S., & Lewandowsky, S. (2022). Psychological inoculation improves resilience against misinformation on social media. Science Advances, 8(34), Article eabo6254."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1126/sciadv.abo6254"},"children":["https://doi.org/10.1126/sciadv.abo6254"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/3769y/"},"children":["https://osf.io/3769y/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Online misinformation continues to have adverse consequences for society. Inoculation theory has been put forward as a way to reduce susceptibility to misinformation by informing people about how they might be misinformed, but its scalability has been elusive both at a theoretical level and a practical level. We developed five short videos that inoculate people against manipulation techniques commonly used in misinformation: emotionally manipulative language, incoherence, false dichotomies, scapegoating, and ad hominem attacks. In seven preregistered studies, i.e., six randomized controlled studies (n = 6464) and an ecologically valid field study on YouTube (n = 22,632), we find that these videos improve manipulation technique recognition, boost confidence in spotting these techniques, increase people’s ability to discern trustworthy from untrustworthy content, and improve the quality of their sharing decisions. These effects are robust across the political spectrum and a wide variety of covariates. We show that psychological inoculation campaigns on social media are effective at improving misinformation resilience at scale.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Studies 1–6: Participants were randomly assigned to either a treatment (inoculation video) or control (unrelated video) condition, after which outcome variables were assessed + covariates. \n\nStudy 7: YouTube users were shown an inoculation video as a YouTube ad. Viewers were shown a single-item multiple-choice survey question assessing manipulation technique recognition within the YouTube environment. A control group answered a survey question but did not see an inoculation video ad.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Studies 1-6: A series of 10 social media posts (Twitter or Facebook, anonymized) randomly either manipulative (i.e., making use of a manipulation technique learned about in the inoculation video) or neutral (i.e., not making use of a manipulation technique. Items were taken from real-world examples of manipulative content but slightly adapted to fit within the social media post format. We created separate item sets for each video. \n\nStudy 7: 6 manipulative headlines taken from Studies 1-6 (3 headlines per inoculation video, with 2 videos being run as YouTube ads).",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Age, gender, education, political ideology, news consumption, social media use, populism, analytical thinking (CRT), numeracy skills, \"bullshit\" receptivity, conspiracy belief, misinformation susceptibility (MIST), 10-item personality inventory (TIPI), actively open-minded thinking (AOT)",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"To our knowledge, this was the first large-scale field study of inoculation interventions on a social media platform (in our case YouTube).",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2","3","4","5","6","7"],"Description":["Study 1 tested whether a +- 1.5 minute inculation video about emotional manipulation conferred resistance against the use of this manipulation technique in social media content.","Study 2 tested whether a +- 1.5 minute inculation video about incoherence (mutually exclusive arguments) conferred resistance against the use of this manipulation technique in social media content.","Study 1 tested whether a +- 1.5 minute inculation video about false dichotomies conferred resistance against the use of this manipulation technique in social media content.","Study 2 tested whether a +- 1.5 minute inculation video about scapegoating conferred resistance against the use of this manipulation technique in social media content.","Study 5 tested whether a +- 1.5 minute inculation video about ad hominem attacks conferred resistance against the use of this manipulation technique in social media content.","Study 6 sought to replicate Study 1, i.e., we tested whether a +- 1.5 minute inculation video about emotional manipulation conferred resistance against the use of this manipulation technique in social media content. We also tested whether the order of presentation of the outcome measures (technique recognition, trustworthiness and sharing) interacted with the main effect.","Study 6 was conducted on YouTube. We sought to assess whether watching an inoculation video as a YouTube ad subsequently improved people's ability to identify manipulation techniques. We ran 2 of the videos (emotional language, Studies 1 and 6; and false dichotomies, study 3) as YouTube ads and asked each a single survey question, each containing a manipulative headline and asking participants to identify which manipulation technique is used. We administered a total of 6 items, 3 per video. The control group did not watch an inoculation video as a YouTube ad but did answer a survey question."],"N":[1072,1086,1095,1080,1083,1072,22632],"Effect size":["Cohen's d: 0.49","Cohen's d: 0.62","Cohen's d: 0.68","Cohen's d: 0.28","Cohen's d: 0.45","Cohen's d: 0.67","Cohen's h: 0.09"],"Comments":["Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.49), confidence in identifying manipulative content, (p < 0.001, d = 0.50), trustworthiness discernment (p < 0.001, d = 0.25), and sharing discernment (p < 0.001, d = 0.21).","Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.62), no significant effect on confidence in identifying manipulative content, (p = 0.471, d = 0.04), a significant effect on trustworthiness discernment (p = 0.002, d = 0.19), and no effect on sharing discernment (p = 0.109, d = 0.10).","Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.68), confidence in identifying manipulative content, (p < 0.001, d = 0.48), trustworthiness discernment (p < 0.001, d = 0.32), and sharing discernment (p < 0.001, d = 0.22).","Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.28), confidence in identifying manipulative content, (p < 0.001, d = 0.35), no significant effect on trustworthiness discernment (p = 0.100, d = 0.10), and no effect on sharing discernment (p = 0.067, d = 0.11).","Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.45), confidence in identifying manipulative content, (p < 0.001, d = 0.24), trustworthiness discernment (p = 0.002, d = 0.19), and sharing discernment (p < 0.001, d = 0.19).","Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.67), trustworthiness discernment (p < 0.001, d = 0.44), and sharing discernment (p < 0.001, d = 0.34). The order of presentation under each item did not interact significantly with the main effect (p > 0.351).","Results showed a significant effect of watching an inoculation video on participants' ability to correctly identify a manipulation technique (Cohen's h = 0.09, p < 0.001)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"1ffd06cf2e6edeedee8b6c951be425ed","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading in education"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Wineburg, S., Breakstone, J., McGrew, S., Smith, M. D., & Ortega, T. (2022). Lateral reading on the open Internet: A district-wide field study in high school government classes. Journal of Educational Psychology, 114(5), 893–909."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1037/edu0000740"},"children":["https://doi.org/10.1037/edu0000740"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.1037/edu0000740.supp"},"children":["https://doi.org/10.1037/edu0000740.supp"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"In a study conducted across an urban school district, we tested a classroom-based intervention in which students were taught online evaluation strategies drawn from research with professional fact checkers. Students practiced the heuristic of lateral reading: leaving an unfamiliar website to search the open Web before investing attention in the site at hand. Professional development was provided to high school teachers who then implemented six 50-minute lessons in a district-mandated government course. Using a matched control design, students in treatment classrooms (n = 271) were compared to peers (n = 228) in regular classrooms. A multilevel linear mixed model showed that students in experimental classrooms grew significantly in their ability to judge the credibility of digital content. These findings inform efforts to prepare young people to make wise decisions about the information that darts across their screens",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In a field experiment conducted across all high schools in an urban district, researchers tested a classroom-based intervention in which students were taught lateral reading. Professional development was provided to teachers who then implemented six 50-minute lessons in a district-mandated government course. Using a matched-control design, students in treatment classrooms (n = 271) were compared to peers (n = 228) in regular classrooms. Schools (3 control, 3 treatment) were matched based on race/ethnicity and the percentage of students enrolled in the free/reduced lunch program. Matched schools were assigned to opposite conditions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Assessments including seven constructed response items designed to assess participants' judgements of credibility of online sources. The pretest and posttest used parallel forms.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Covariates included in the multilevel model: home language, ethnicity, race, gender, hours spend online per day, and frequency of checking the trustworthiness of online information.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The present study investigated whether high school students would improve as evaluators of online content on the open Web after completing six 50-minute lessons based on Civic Online Reasoning curriculum taught by their teachers."],"N":[499],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Students in the treatment condition (n = 271) were more likely to show improvement from pretest to posttest than control students (n = 228), Robust beta coefficient for condition x time(pre-post) = 1.66, SE = .44, t = 3.77, 95% CI (.44, 3.77)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"abd882194945f661b4f49083edd94564","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading in education"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Wineburg, S., & McGrew, S. (2019). Lateral reading and the nature of expertise: Reading less and learning more when evaluating digital information. Teachers College Record, 121(11), 1–40.\n"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://eric.ed.gov/?id=EJ1262001"},"children":["https://eric.ed.gov/?id=EJ1262001"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The Internet has democratized access to information but in so doing has opened the floodgates to misinformation, fake news, and rank propaganda masquerading as dispassionate analysis. Despite mounting attention to the problem of online misinformation and growing agreement that digital literacy efforts are important, prior research offers few concrete ideas about what skilled evaluations look like. Purpose/Objective/Research Question/Focus of Study: Our purpose in this study was to seek out those who are skilled in online evaluations in order to understand how their strategies and approaches to evaluating digital content might inform educational efforts. We sampled 45 experienced users of the Internet: 10 Ph.D. historians, 10 professional fact checkers, and 25 Stanford University undergraduates. Analysis focused on the strategies participants used to evaluate online information and arrive at judgments of credibility. Research Design: In this expert/novice study, participants thought aloud as they evaluated live websites and searched for information on social and political issues such as bullying, minimum wage, and teacher tenure. We analyze and present findings from three of the tasks participants completed. Findings/Results: Historians and students often fell victim to easily manipulated features of websites, such as official-looking logos and domain names. They read vertically, staying within a website to evaluate its reliability. In contrast, fact checkers read laterally, leaving a site after a quick scan and opening up new browser tabs in order to judge the credibility of the original site. Compared to the other groups, fact checkers arrived at more warranted conclusions in a fraction of the time. Conclusions/Recommendations: We draw on insights gleaned from the fact checkers' practices to examine current curricular approaches to teaching web credibility as well as to suggest alternatives",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"An expert/novice study in which Stanford University undergraduates (n = 25), PhD historians (n = 10), and professional fact-checkers (n = 10) thought aloud as they evaluated websites and searched for information on social and political issues.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Live Internet sources addressing social and political issues",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This exploratory expert-novice study aimed to better understand the nature of expertise in the evaluation of online information."],"N":[45],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["For the first task (evaluating articles about bullying on two websites), Fact checkers had a perfect mean score of 2 (SD = 0); historians, 0.7 (SD = 0.95); and students, .16 (SD = 0.37). For the second task (an article at the minimum wage.com), fact-checkers’ conclusions averaged 3.3 (SD = .82) out of 4, versus historians’ average of 1.3 (SD = 1.4) and students’ .52 (SD = 1.16). For the third task (offline article on the court case Vergara v. California, task: researching the funding source), the fact checkers’ conclusions merited a 3.6 (SD = 0.70), versus historians’ 2.4 (SD = 1.3) and students’ 2.3 (SD = 1.5)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"ab61238eda2009ebe9f1516d26b0601c","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading in education"]},{"name":"div","attribs":{"className":"detail-description"},"children":["McGrew, S., Smith, M., Breakstone, J., Ortega, T., Wineburg, S. (2019). Improving university students’ web savvy: An intervention study. British Journal of Educational Psychology, 89(3), 485–500."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1111/bjep.12279"},"children":["https://doi.org/10.1111/bjep.12279"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Young people increasingly turn to the Internet for information about social and political issues. However, they struggle to evaluate the trustworthiness of the information they encounter online. This pilot study investigated whether a focused curricular intervention could improve university students’ ability to make sound judgements of credibility. Participants (n = 67) were students in four sections of a ‘critical thinking and writing’ course at a university on the West Coast of the United States. Course sections were randomly assigned to treatment (n = 29) and control conditions (n = 38). We conducted a pre-and-posttest, treatment/control experiment using a 2 × 2 × 2 design (treatment condition × order × time) with repeated measures on the last factor. Students in the treatment group received two 75-min lessons on evaluating the credibility of online content. An assessment of online reasoning was administered to students 6 weeks prior to the intervention and again 5 weeks after. Students in the treatment group were significantly more likely than students in the control group to have shown gains from pretest to posttest. Results suggest that teaching students a small number of flexible heuristics that can be applied across digital contexts can improve their evaluation of online sources.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A pre-and-posttest, treatment/control experiment using a 2 × 2 × 2 design (treatment condition × order × time) with repeated measures on the last factor. University students in the treatment group received two 75-minute lessons on evaluating the credibility of online content. An assessment of online reasoning was administered to students six weeks prior to the intervention and again five weeks after.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Pretest and posttest included 4 constructed response items asking to evaluate the credibility of online sources with a live Internet connection",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"evaluating articles, evaluating evidence",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The magnitude of the effect was estimated in terms of probability that students in the treatment group would score higher at posttest than students in the control group.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This pilot study investigated whether a focused curricular intervention could improve university students’ ability to make sound judgements of credibility."],"N":[67],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Overall, students in the treatment group were over twice as likely (2.15 times) to score higher at posttest than at pretest, while students in the control condition were equally likely (1.00 times) to score higher at posttest than at pretest."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"ede47ba143ad44d7c60637bd11f3aa07","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading in education"]},{"name":"div","attribs":{"className":"detail-description"},"children":["McGrew, S. (2020). Learning to evaluate: An intervention in civic online reasoning. Computers & Education, 145, Article 103711."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1016/j.compedu.2019.103711"},"children":["https://doi.org/10.1016/j.compedu.2019.103711"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Students turn to the Internet for information but often struggle to evaluate the trustworthiness of what they find. Teachers should help students develop effective evaluation strategies in order to ensure that students have access to reliable information on which to base decisions. This study reports on the results of an attempt to teach students to reason about online information. Students were taught strategies for evaluating digital content that were based on the practices of professional fact checkers. Eight lessons were devoted to teaching students strategies to effectively evaluate digital content. Pre- and posttests, each composed of four brief, constructed-response items, were administered to 68 11th-grade students who participated in the study. Students' scores improved significantly from pre-to posttest on three of the four tasks: students demonstrated an improved ability to investigate the source of a website, critique evidence, and locate reliable sources during an open Internet search. These results are promising and suggest that explicit instruction on fact-checking strategies may help students develop more effective online evaluation strategies.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A pre-and-posttest classroom intervention with 68 high school students that included 8 lessons on how to evaluate online information about political and social issues, taught approximately once per week over 2 months. All students received the same treatment and took the same outcome measures at pretest and posttest. The pretest and posttest forms included 4 constructed-response tasks. Pre-and-post forms were designed to be parallel.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Pretest and posttest included 4 brief, constructed-response items that were selected to assess a range civic online reasoning skills, including lateral reading.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"students’ ability to evaluate social and political information online",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Statistical significance was used to estimate the change in scores from pretest to posttest on each of the four constructed response tasks.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This study investigated whether a focused curricular intervention could improve high school students’ online reasoning skills."],"N":[68],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["1. Ad Identification. Change from pre to post was not significant (Z = 1.44; p = .15). 2. Lateral Reading. Pre-to-post change was significant (Z =  3.59; p < .001). 3. Analyzing Evidence. Pre-to-post change was significant (Z =  6.23; p < .001). 4. Claim Research. Pre-to-post change was significant (Z = 3.77; p < .001)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"1accda226176b00a3aeb344efbebda34","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading in education"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Brodsky, J. E., Brooks, P. J., Scimeca, D., Todorova, R., Galati, P., Batson, M., Grosso, R., Matthews, M., Miller, V., & Caulfield, M. (2021). Improving college students’ fact checking strategies through lateral reading instruction in a general education civics course. Cognitive Research: Principles and Implications, 6(1), Article 23."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1186/s41235-021-00291-4"},"children":["https://doi.org/10.1186/s41235-021-00291-4"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/9rbkd/"},"children":["https://osf.io/9rbkd/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"College students lack fact-checking skills, which may lead them to accept information at face value. We report findings from an institution participating in the Digital Polarization Initiative (DPI), a national effort to teach students lateral reading strategies used by expert fact-checkers to verify online information. Lateral reading requires users to leave the information (website) to find out whether someone has already fact-checked the claim, identify the original source, or learn more about the individuals or organizations making the claim. Instructor-matched sections of a general education civics course implemented the DPI curriculum (N = 136 students) or provided business-as-usual civics instruction (N = 94 students). At posttest, students in DPI sections were more likely to use lateral reading to fact-check and correctly evaluate the trustworthiness of information than controls. Aligning with the DPI’s emphasis on using Wikipedia to investigate sources, students in DPI sections reported greater use of Wikipedia at posttest than controls, but did not differ significantly in their trust of Wikipedia. In DPI sections, students who failed to read laterally at posttest reported higher trust of Wikipedia at pretest than students who read at least one problem laterally. Responsiveness to the curriculum was also linked to numbers of online assignments attempted, but unrelated to pretest media literacy knowledge, use of lateral reading, or self-reported use of lateral reading. Further research is needed to determine whether improvements in lateral reading are maintained over time and to explore other factors that might distinguish students whose skills improved after instruction from non-responders.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A field experiment that tested the efficacy of teaching university students the Digital Polarization Initiative's 4 fact-checking moves with a pre-and-post, treatment/control curricular intervention. Students in the treatment condition (n = 136) were taught the 4 moves in 3 sessions of their introductory civics class and were provided opportunities to practice these moves with internet sources in 3 more classes 2 weeks later. Students in the control classes (n = 94) received standard instruction.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Two sets of lateral reading problems (A and B). Problems were adapted from the Stanford History Education Group’s Civic Online Reasoning curriculum and from Mike Caulfield's Four Moves blog. Students completed one of the lateral reading problem sets (A or B) as a pretest and the other problem set as a posttest. Set order was counterbalanced across instructors at pretest and posttest.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Other pre/post outcome measures (Likert-type rating scales):\n- Self-report on use of Wikipedia\n- Trust of Wikipedia\n\nA general media literacy questionnaire adapted from Ashley et al. (2013) that included 18 Likert-type “agree/disagree” items.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This field experiment tested the efficacy of teaching university students Digital Polarization Initiative's (DPI) four fact-checking strategies."],"N":[230],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["At posttest, students in DPI sections had an average score of M = 2.22 (SD = 0.92) across the four problems and received a score of 4 on an average of 1.07 problems (SD = 1.07). In contrast, students in control sections had an average score of M = 1.15 (SD = 0.30) and received a score of 4 on an average of 0.03 problems (SD = 0.23). For the self-reported lateral reading at posttest, there was a significant main effect of condition, F(1, 228) = 4.13, p = .043, ηp2 = 0.02, with students in the DPI sections reporting higher use of lateral reading (M = 3.45, SD = 0.84) than students in the control sections (M = 3.25, SD = 0.88). The interaction of time and condition was not significant, F(1, 228) = 1.06, p = .304, ηp2 = 0.01."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"42d21e976bbe26c7e17a0f21ca300aa8","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading online"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Panizza, F., Ronzani, P., Martini, C., Mattavelli, S., Morisseau, T., & Motterlini, M. (2022). Lateral reading and monetary incentives to spot disinformation about science. Scientific Reports, 12(1), Article 5678."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1038/s41598-022-09168-y"},"children":["https://doi.org/10.1038/s41598-022-09168-y"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/5zx9g/"},"children":["https://osf.io/5zx9g/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Disinformation about science can impose enormous economic and public health burdens. A recently proposed strategy to help online users recognise false content is to follow the techniques of professional fact checkers, such as looking for information on other websites (lateral reading) and looking beyond the first results suggested by search engines (click restraint). In two preregistered online experiments (N = 5387), we simulated a social media environment and tested two interventions, one in the form of a pop-up meant to advise participants to follow such techniques, the other based on monetary incentives. We measured participants’ ability to identify whether information was scientifically valid or invalid. Analysis of participants’ search style reveals that both monetary incentives and pop-up increased the use of fact-checking strategies. Monetary incentives were overall effective in increasing accuracy, whereas the pop-up worked when the source of information was unknown. Pop-up and incentives, when used together, produced a cumulative effect on accuracy. We suggest that monetary incentives enhance content relevance, and could be combined with fact-checking techniques to counteract disinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Study 1: Participants were randomly assigned to 1 of 3 experimental conditions: control, incentive, and pop-up. Study 2 had a between-subjects design with 2 factors, pop-up (present, absent) and monetary incentive (present, absent).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 1: Each participant observed one interactive, science-themed Facebook post from a set of 9 different Facebook posts varying in various properties, such as the scientific topic, the source reputation, and its level of factual reporting.\nStudy 2: 1 out of 6 interactive, science-themed Facebook posts from sources unknown to participants.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Search behaviour (tracked and self-report), content plausibility, trust in scientists, science literacy, conspiracy ideation",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"In Experiment 1, results were aggregated with posts from known sources (lateral reading is a heuristic to determine trustworthiness of unfamiliar sources).  Lateral reading is as effective as the strongest intervention (i.e. paying participants to be accurate), but that very few participants actually follow this strategy. The pop-up increases lateral reading and click restraint: +11% [95% CI: +7%, +17%]. Participants who use lateral reading and click restraint increase accuracy by +0.4 (on a scale from 1 to 6), \nbut: even when the pop-up is present, only 16% of participants (1/6 of the sample) reports using lateral reading, and only 11% reports adopting click restraint.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Experiment 1 tested separately the efficacy of pop-up (incl. lateral reading) and monetary incentives, and compared their effects to a control condition with no interventions.","Experiment 2 replicated the format of the first one, with two main modifications: 1) a pre-screening survey to identify lesser-known sources of information and only used those sources as the basis for the Facebook posts the participants were asked to evaluate; 2) added an experimental condition that included both incentive and pop-up interventions, to test the interaction between the two."],"N":[517,3003],"Effect size":["Common Language Effect Size: 0.54 (95%-CI: 0.49-0.58)","Common Language Effect Size: 0.52 (95%-CI: 0.5-0.54)"],"Comments":["In the first study (whole sample N = 2384), results showed a significant effect of incentive (β=0.293 [0.092, 0.494], z=3.225, p=0.003) and a lack of significance for the pop-up (β=−0.009 [−0.207,0.188], z=−0.103, p=.918). Technique adoption: both incentive and pop-up increased technique adoption ( incentive: β=1.042 [0.527, 1.556], z=4.728, p<0.001; pop-up: β=1.556 [1.065, 2.046], z=7.405, p<0.001), but that the increase was markedly higher with the presence of the pop-up than with monetary incentives (β=0.514 [0.157, 0.871], z=3.362, p<0.001). Participants were more likely to use lateral reading when the source was unknown (stimuli included known sources, such as BBC).","In the second study, results revealed a significant effect of pop-up on accuracy scores (β=0.137 [−0.018,0.292], z=2.115, p=0.034; Mixed-effects regression with errors clustered by post: p=0.052), but not on correct guessing (β=0.076 [−0.112,0.265], z=0.966, p=0.334). The combination of the two interventions significantly increased both accuracy indices compared to control (accuracy score: β=0.487 [0.268, 0.705], z=5.315, p<0.001; correct guessing: β=0.389 [0.123, 0.654], z=3.496, p<0.001), and that the contribution of incentive was greater than the contribution of pop-up (accuracy score: β=0.213 [−0.007,0.432], z=2.307, p=0.028; correct guessing: β=0.2362 [−0.032,0.504], z=2.103, p=0.047). According to the ordinal logistic regression model, the combination of the two interventions led to a 10.4% [5.4%,14.2%] increase in correct guessing, and a 6.9% [2.8%,12.4%] increase in  correct responses compared to control."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"2c416023650744a64477ab307fcd96d9","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading online"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Donovan, A. M., & Rapp, D. N. (2020). Look it up: Online search reduces the problematic effects of exposures to inaccuracies. Memory & Cognition, 48(7), 1128–1145."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.3758/s13421-020-01047-z"},"children":["https://doi.org/10.3758/s13421-020-01047-z"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"People often reproduce information they read, which is beneficial when that information is accurate. Unfortunately, people are also often exposed to inaccurate information, with subsequent reproductions allowing for problematic decisions and behaviors. One empirically validated consequence of exposures to inaccuracies is that after reading falsehoods, participants are more likely to make errors answering related questions than if they previously read accurate statements, particularly for unfamiliar information. Interventions designed to attenuate these reproductions are often ineffective, at least as studied in tasks that restrict participants to generating answers based on text content and relevant prior knowledge. In the real world, however, people have access to outside resources to evaluate information. In three experiments, we tested whether affording the option to search for relevant online information following exposure to inaccurate statements would reduce reproductions of those inaccuracies on a post-reading task. Participants given the opportunity to search for information were less likely to reproduce inaccurate information and more likely to produce correct responses, in comparison to the performance of participants who were not allowed to search. We also tested whether warnings about potentially inaccurate information would encourage searches and inform responses. While warnings increased searching, additional reductions in inaccurate reproductions were not observed. Given the contingencies of many lab tasks, reproductions of inaccurate information might be overestimated. Resources available in the real world can offer useful supports for reducing the influence of and uncertainty associated with inaccurate exposures, consistent with contemporary accounts of memory and comprehension.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants read stories containing accurate, inaccurate, and neutral (i.e., unspecified) statements. Afterwards they completed a questionnaire with critical items relating to the statements. Studies 1 and 2 used a 3 (statement type: inaccurate, accurate, neutral) × 2 (item difficulty: easy, hard) × 2 (condition: search, no search) design.  Study 3 used a 3 (statement type: inaccurate, accurate, neutral) × 2 (item difficulty: easy, hard) × 2 (condition: warning, no warning) design.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"4 short fictional narratives of common, real-world events and topics adapted from Marsh (2004). Each of the stories contained 8 general knowledge statements for a total of 32 critical statements (accurate, inaccurate, or neutral; 8–12 statements of each type).",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Screen recordings (experiment 2), search behaviour self-report",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Note that Study 3 tested effectiveness of warnings (about the possible inclusion of inaccuracies in the stories as a means of motivating participants to consult online resources) and not of search and hence is not included here.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study 1 experimentally tested whether opportunities for online search would reduce inaccurate reproductions of information.","Study 2 experimentally tested whether opportunities for online search  would reduce inaccurate reproductions of information. The study was conducted in the lab."],"N":[231,96],"Effect size":["Partial-Eta squared (η2p): use of inaccurate statements: 0.05; use of accurate statements: 0.13","Partial-Eta squared (η2p): use of inaccurate statements: 0.14; use of accurate statements: 0.46"],"Comments":["Participants in the search condition (M = 5.17%, SD = 10.97) were less likely to reproduce inaccurate information than were participants in the no-search condition (M = 7.40%, SD = 15.10) [F (1, 214) = 11.01, MS = 0.83, p = .001, ηp2 = .05]. Participants in the search condition produced more correct responses (M = 72.37%, SD = 30.74) than did participants in the no-search condition (M = 59.30%, SD = 35.74), [F (1, 214) = 31.82, MS = 22.25, p < .001, ηp2 = .13].","Participants in the search condition (M = 4.30%, SD = 9.61) used inaccurate information to answer questions less often than did participants in the no-search condition (M = 7.52%, SD = 14.73), [F (1,95) = 14.94, MS = 0.72, p < .001, ηp2 =.14]."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"3eec397c942973c41db1ad876836a0c3","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading online"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Resnick, P., Alfayez, A., Im, J., & Gilbert, E. (2021). Informed crowds can effectively identify misinformation. arXiv.  (Preprint, not peer-reviewed)"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.48550/arXiv.2108.07898"},"children":["https://doi.org/10.48550/arXiv.2108.07898"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Can crowd workers be trusted to judge whether news-like articles circulating on the Internet are wildly misleading, or does partisanship and inexperience get in the way? We assembled pools of both liberal and conservative crowd raters and tested three ways of asking them to make judgments about 374 articles. In a no research condition, they were just asked to view the article and then render a judgment. In an individual research condition, they were also asked to search for corroborating evidence and provide a link to the best evidence they found. In a collective research condition, they were not asked to search, but instead to look at links collected from workers in the individual research condition. The individual research condition reduced the partisanship of judgments. Moreover, the judgments of a panel of sixteen or more crowd workers were better than that of a panel of three expert journalists, as measured by alignment with a held out journalist's ratings. Without research, the crowd judgments were better than those of a single journalist, but not as good as the average of two journalists.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"3 conditions (no research control, individual research, collective research) × 3 respondent's ideology (liberal, conservative, moderate). None of the raters received explicit training. Both treatment conditions required raters to seek out or consider external evidence.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"368 articles (headline and lede), of which 207 that were flagged by Facebook algorithms as requiring fact-checking and 165 from the study by Godel et al, 2021, which consisted of most popular articles in several political and non-political categories.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"misinformation judgements, subjective opinio on enforcement action, prediction about other raters’ subjective opinions",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study investigated whether requirement to search for corroborating evidence either in a collective or individual research conditions would improve misinformation detection in lay people ratings (journalists' ratings were taken as a benchmark)."],"N":[1301],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Lay raters in the two research conditions correlate with a journalist better than do raters in the no research condition. The individual research condition has greater power than the collective research condition for large groups of lay raters. In the individual research condition, 15 lay raters were equivalent to 3 journalists; even 54 raters were not sufficient to achieve the same power as three journalists in the collective search condition and in the no research condition."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"96a7a2a076654c99fe04c98c8bedfaa5","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Media literacy tips"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Guess, A. M., Lerner, M., Lyons, B., Montgomery, J. M., Nyhan, B., Reifler, J., & Sircar, N. (2020). A digital media literacy intervention increases discernment between mainstream and false news in the United States and India. Proceedings of the National Academy of Sciences, 117(27), 15536–15545."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1073/pnas.1920498117"},"children":["https://doi.org/10.1073/pnas.1920498117"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.7910/DVN/Q5QINN"},"children":["https://doi.org/10.7910/DVN/Q5QINN"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Widespread belief in misinformation circulating online is a critical challenge for modern societies. While research to date has focused on psychological and political antecedents to this phenomenon, few studies have explored the role of digital media literacy shortfalls. Using data from preregistered survey experiments conducted around recent elections in the United States and India, we assess the effectiveness of an intervention modeled closely on the world’s largest media literacy campaign, which provided “tips” on how to spot false news to people in 14 countries. Our results indicate that exposure to this intervention reduced the perceived accuracy of both mainstream and false news headlines, but effects on the latter were significantly larger. As a result, the intervention improved discernment between mainstream and false news headlines among both a nationally representative sample in the United States (by 26.5%) and a highly educated online sample in India (by 17.5%). This increase in discernment remained measurable several weeks later in the United States (but not in India). However, we find no effects among a representative sample of respondents in a largely rural area of northern India, where rates of social media use are far lower.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Longitudinal RCTs, self-reported survey measures, Web tracking data",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Mock social media posts based on real stories",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Online untrustworthy news consumption",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Note that effect sizes are substantially larger for respondents who were successfully treated with the media literacy intervention. The reported effect sizes are for the Intention to treat.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1: U.S. online","2: India online","3: India face-to-face"],"Description":["Study 1 experimentally tested whether exposure to the media literacy intervention would cause a decrease in the perceived accuracy of false news articles.","Study 2 experimentally tested whether exposure to the media literacy intervention would cause a decrease in the perceived accuracy of false news articles.","Study 3 experimentally tested whether face-to-face exposure to the media literacy intervention would cause a decrease in the perceived accuracy of false news articles."],"N":[4907,3273,3744],"Effect size":["Cohen’s d: 0.2","Cohen’s d: 0.11","Cohen’s d: n.s."],"Comments":["The media literacy treatment significantly reduced beliefs in false news articles. Wave 1 of the US study: a decrease of nearly 0.2 points on a 4-point scale (intent to treat [ITT]: β=−0.196, SE=0.020; P<0.005). Wave 2 of the US study: While the effect is still present weeks later, its magnitude attenuates by more than half relative to wave 1 (ITT, β=−0.080 [SE=0.019], P<0.005; ATT, β=−0.121 [SE=0.028], P<0.005).","The media literacy treatment significantly reduced beliefs in false news articles (ITT: β=−0.126, SE = 0.026; P<0.005) in the first wave of a two-wave survey. The study found no statistically reliable evidence that the treatment affected headline accuracy ratings among wave 2 respondents in either India sample.","The study found no evidence that the media literacy treatment systematically affected beliefs in false news stories or discrimination between false and mainstream news among India face-to-face respondents."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"81aa9af61157e349dd367f03fc282fa6","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Media literacy tips"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Ali, A., & Qazi, I. A. (2021). Countering misinformation on social media through educational interventions: Evidence from a randomized experiment in Pakistan.  (Preprint, not peer-reviewed)"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.48550/arXiv.2107.02775"},"children":["https://doi.org/10.48550/arXiv.2107.02775"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Fake news is a growing problem in developing countries with potentially far-reaching consequences. We conduct a randomized experiment in urban Pakistan to evaluate the effectiveness of two educational interventions to counter misinformation among low-digital literacy populations. We do not find a significant effect of video-based general educational messages about misinformation. However, when such messages are augmented with personalized feedback based on individuals' past engagement with fake news, we find an improvement of 0.14 standard deviations in identifying fake news. We also find negative but insignificant effects on identifying true news, driven by female respondents. Our results suggest that educational interventions can enable information discernment but their effectiveness critically depends on how well their features and delivery are customized for the population of interest.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In Treatment 1, participants were shown an informational video in the national language that educated them about common features of misinformation. In Treatment 2, participants were shown the video and then given personalized feedback about their own responses to fake news stories shown at baseline. The feedback highlighted the features of each fake news item that indicated the news was fake. Endline surveys were conducted after 1 week and then 4 to 6 weeks after the treatment was delivered. To assess the longevity and external validity of the treatments, researchers conducted a follow-up phone survey 15 months after the interventions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Video or video and personalized feedback based on ability to identify a fake news story",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Age, gender, digital literacy",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study aimed to evaluate the effectiveness of two educational interventions to counter misinformation among low-digital literacy populations. Treatment one = educational video. Treatment 2 = video + personalized feedback."],"N":[750],"Effect size":["Cohen’s d: 0.14"],"Comments":["Treatment 1 does not have any significant impact. Participants who received treatment 2 were 0.14 standard deviations more likely to correctly identify fake news relative to the control group. There is no significant effect on correctly identifying true news. As a result, the overall effect of treatment 2 on all news is 0.076 standard deviations, which points to improved discernment of news."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"8b244f352d23669ca594a6ba2d47c33f","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Media literacy tips"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Badrinathan, S. (2021). Educative interventions to combat misinformation: Evidence from a field experiment in India. American Political Science Review, 115(4), 1325–1341."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1017/S0003055421000459"},"children":["https://doi.org/10.1017/S0003055421000459"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.7910/DVN/ITKNX5"},"children":["https://doi.org/10.7910/DVN/ITKNX5"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation makes democratic governance harder, especially in developing countries. Despite its real-world import, little is known about how to combat misinformation outside of the United States, particularly in places with low education, accelerating Internet access, and encrypted information sharing. This study uses a field experiment in India to test the efficacy of a pedagogical intervention on respondents’ ability to identify misinformation during the 2019 elections (N = 1,224). Treated respondents received hour-long in-person media literacy training in which enumerators discussed inoculation strategies, corrections, and the importance of verifying misinformation, all in a coherent learning module. Receiving this hour-long media literacy intervention did not significantly increase respondents’ ability to identify misinformation on average. However, treated respondents who support the ruling party became significantly less able to identify pro-attitudinal stories. These findings point to the resilience of misinformation in India and the presence of motivated reasoning in a traditionally nonideological party system.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants were randomized into 1 of 3 groups: 2 treatment and 1 placebo control. Participants in both treatment conditions received an hour-long in-person media literacy training session covering inoculation strategies, corrections, and the importance of verifying misinformation. They also received a demonstration of how to fact-check news stories and a flyer with tips on how to spot misinformation. One treatment group received corrections to 4false pro-BJP (Bharatiya Janata Party, a right-wing political party) stories; the other received corrections to 4 false anti-BJP stories. Apart from differences in the stories that were fact-checked, the tips on the flyer were the same for both treatment groups. Each treatment condition had an equal proportion of BJP and non-BJP partisans. Control group participants were shown a placebo demonstration about plastic pollution and received a flyer containing tips to reduce plastic usage. Trained enumerators administered the intervention in a household visit. Approximately 2 weeks after the intervention, participants were revisited to conduct an endline survey and measure the outcomes of interest.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"A series of 14 news stories, which varied in content, salience, and critically, partisan slant. Half of the stories were pro-BJP and the other half \nanti-BJP.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"BJP support, political knowledge, digital literacy, self-reported WhatsApp use, WhatsApp trust",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This study experimentally tested whether an hour-long media literacy intervention would increase ability to identify misinformation among respondents in Bihar, India."],"N":[1224],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["The study found no evidence that the intervention increased respondents' ability to identify misinformation, on average. While there was no average treatment effect, the interaction effect of the treatment on BJP partisans produces a negative effect on the ability to identify misinformation. For pro-BJP stories, the treatment effect for non-BJP supporters was 0.277, indicating that those who did not support the BJP and received the treatment identified an additional 0.277 stories. However, the treatment effect for BJP supporters was -0.135, indicating that those who supported the BJP and received the treatment identified 0.135 fewer stories."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"a7396ccc706bc84322d594e7a2926cee","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Media literacy tips"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Epstein, Z., Berinsky, A. J., Cole, R., Gully, A., Pennycook, G., & Rand, D. G. (2021). Developing an accuracy-prompt toolkit to reduce COVID-19 misinformation online. Harvard Kennedy School (HKS) Misinformation Review."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.37016/mr-2020-71"},"children":["https://doi.org/10.37016/mr-2020-71"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.7910/DVN/18SHLJ"},"children":["https://doi.org/10.7910/DVN/18SHLJ"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Recent research suggests that shifting users’ attention to accuracy increases the quality of news they subsequently share online. Here we help develop this initial observation into a suite of deployable interventions for practitioners. We ask (i) how prior results generalize to other approaches for prompting users to consider accuracy, and (ii) for whom these prompts are more versus less effective. In a large survey experiment examining participants’ intentions to share true and false headlines about COVID-19, we identify a variety of different accuracy prompts that su­ccessfully increase sharing discernment across a wide range of demographic subgroups while maintaining user autonomy.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"The study assessed the impact of 8 experimental treatments on sharing intentions of true versus false headlines. Each treatment was administered prior to the beginning of the news-sharing task. Then participants were shown true and false COVID-related headlines and asked about their sharing intentions and accuracy judgments. Of the 8 treatments, 1 was a media literacy tips intervention. In this treatment, participants were provided with 4 simple digital literacy tips, taken from an intervention developed by Facebook (Guess et al., 2020).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Mock social media posts based on real stories",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Level of concern about COVID-19, the extent to which they had been following COVID-19–related news, CRT, importance of sharing only accurate news",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["Tips"],"Description":["Participants were provided with four simple digital literacy tips, taken from an intervention developed by Facebook"],"N":[906],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Intervention increased sharing discernment by roughly 50% (3 percentage points)"]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"843bbaeabcf9b28414dcbb533dc43676","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Rebuttals of science denialism"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Schmid, P., Schwarzer, M., & Betsch, C. (2020). Weight-of-evidence strategies to mitigate the influence of messages of science denialism in public discussions. Journal of Cognition, 3(1), Article 36."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"http://doi.org/10.5334/joc.125"},"children":["http://doi.org/10.5334/joc.125"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.17605/OSF.IO/SEFQU"},"children":["https://doi.org/10.17605/OSF.IO/SEFQU"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"In mass media, the positions of science deniers and scientific-consensus advocates are repeatedly presented in a balanced manner. This false balance increases the spread of misinformation under the guise of objectivity. Weight-of-evidence strategies are an alternative, in which journalists lend weight to each position that is equivalent to the amount of evidence that supports the position. In public discussions, journalists can invite more advocates of scientific consensuses than science deniers (outnumbering) or they can employ warnings about the false-balance effect prior to the discussions (forewarning). In three pre-registered laboratory experiments, we tested the efficacy of outnumbering and forewarning as weight-of-evidence strategies to mitigate science deniers’ influence on individuals’ attitudes towards vaccination and their intention to vaccinate. We explored whether advocates’ responses to science deniers (rebuttal) and audiences’ issue involvement moderate the efficacy of these strategies. A total of N = 887 individuals indicated their attitudes towards vaccination and their intention to vaccinate before and after watching a television (TV) discussion. The presence and absence of forewarning, outnumbering and rebuttal were manipulated between subjects; participants also indicated their individual issue involvement. We obtained no evidence that outnumbering mitigates damage from denialism, even when advocates served as multiple sources. However, forewarning about the false-balance effect mitigated deniers’ negative effects. Moreover, the protective effect was independent of rebuttal and issue involvement. Thus, forewarnings can serve as an effective, economic and theory-driven strategy to counter science denialism in public discussions, at least for highly educated individuals such as university students.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Experiment 1: A 2 (pre–post measure: within) × 2 (rebuttal absent vs. present: between) × 2 (outnumbering absent vs. present: between) mixed design. Rebuttal was either absent or present.\nExperiment 2: A 2 (pre–post measure: within) × 2 (rebuttal absent vs. present: between) × 2 (outnumbering absent vs. present: between) × 2 (forewarning absent vs. present: between) mixed design. Rebuttal was either absent or present.\nExperiment 3: A 2 (pre-post measure: within) × 2 (rebuttal absent vs. present: between) × 2 (outnumbering absent vs. present: between) × 2 (forewarning absent vs. present: between) mixed design. Rebuttal was either absent or present.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Mock TV discussion as part of a fictitious scenario. Discussion statements taken from a science denier using misleading techniques:  impossible expectation, conspiracy theories. In the advocate for science condition topic and technique rebuttal were used",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Other relevant variables: issue involvement, speakers’ perceived credibility (a full list of variables is provided in the supplement of the paper).\n\n                                \n                        \n                \n",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The study focused on several conditions for rebutting science denialism in public discussions (i.e., outnumbering, forewarning). All conditions included rebuttals - and we focus only on effectiveness of rebuttal conditions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2","3"],"Description":["Study 1: Outnumbering and rebuttal, results for rebuttal vs no rebuttal.","Study 2: Outnumbering, forewarning and rebuttal, results for rebuttal vs no rebuttal.","Study 3: outnumbering by delivering multiple rebuttal sources, forewarning and rebuttal, results for rebuttal vs no rebuttal."],"N":[101,390,390],"Effect size":["Partial-Eta squared (η2p): Attitude η²p = .094; Intention η²p = .036","Partial-Eta squared (η2p): Attitude η²p = .116; Intention η²p = .124 ; confidence in vaccination η²p = .154","Partial-Eta squared (η2p): Attitude η²p = .059; Intention η²p = .078 ; confidence in vaccination η²p = .108"],"Comments":["Watching the public debate significantly damaged individuals’ attitudes towards vaccination and their intention to get vaccinated, but the rebuttal successfully mitigated this damage. The mitigating effect was only marginally significant on intention to get vaccinated.","Replicating the results from Experiment 1, watching the public debate significantly damaged individuals’ attitudes towards vaccination, including intention to get vaccinated and confidence in vaccination. However, the rebuttal mitigated this damage on all outcome measures, again indicating successful manipulation.",null]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"3888a5515a5694b830780feed0902588","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Rebuttals of science denialism"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Schmid, P., & Betsch, C. (2019). Effective strategies for rebutting science denialism in public discussions. Nature Human Behaviour, 3(9), 931–939."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1038/s41562-019-0632-4"},"children":["https://doi.org/10.1038/s41562-019-0632-4"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/xx2kt/"},"children":["https://osf.io/xx2kt/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Science deniers question scientific milestones and spread misinformation, contradicting decades of scientific endeavour. Advocates for science need effective rebuttal strategies and are concerned about backfire effects in public debates. We conducted six experiments to assess how to mitigate the influence of a denier on the audience. An internal meta-analysis across all the experiments revealed that not responding to science deniers has a negative effect on attitudes towards behaviours favoured by science (for example, vaccination) and intentions to perform these behaviours. Providing the facts about the topic or uncovering the rhetorical techniques typical for denialism had positive effects. We found no evidence that complex combinations of topic and technique rebuttals are more effective than single strategies, nor that rebutting science denialism in public discussions backfires, not even in vulnerable groups (for example, US conservatives). As science deniers use the same rhetoric across domains, uncovering their rhetorical techniques is an effective and economic addition to the advocates’ toolbox.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Experiments with a 2 (pre–post measure: within) × 2 (technique rebuttal absent vs. present: between) × 2 (topic rebuttal absent vs. present: between) mixed design. Participants were randomly assigned to 1 of 4 rebuttal conditions (advocate absent, topic only, technique only, combination of topic and technique).\n\n                                \n                        \n                \n",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"2 vignettes from an audiotaped or written radio discussion. The discussion included statements from a science denier (Study 1,2,3,4, 6: Vaccination Study 5: Climate Change) using misleading techniques: impossible expectation, conspiracy theories. In the advocate for science condition topic rebuttal, technique rebuttal or the combination of both were used. Depending on condition, an advocate for science was either absent, used topic rebuttal, technique rebuttal or a combination of both. \n\n                                \n                        \n                \n",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Other relevant moderators: political ideology (Experiment 4, Experiment 6), confidence in vaccination (Experiment 2,3,4 & 6)\nOther relevant mediators: perceived persuasiveness of the denier and advocate\n(Experiment 1), the perceived argument strength of the denier\nand advocate (Experiments 2 and 5) and participants’ persuasion\nknowledge (Experiment 3). \nRelevant control variables: Knowledge about vaccination, climate change (a full list of variables is provided in the supplement of the paper)\n                                \n                        \n                \n",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["Across all 6 studies","Across all 6 studies","Across all 6 studies"],"Description":["Internal meta-analysis: Rebuttal vs no rebuttal","Internal meta-analysis: Technique rebuttal vs no technique rebuttal","Internal meta-analysis: Topic rebuttal vs no topic rebuttal"],"N":[1773,1773,1773],"Effect size":["Hedges' g: 0.49 (95%-CI: 0.37-0.6)","Hedges' g: 0.31 (95%-CI: 0.22-0.41)","Hedges' g: 0.21 (95%-CI: 0.04-0.38)"],"Comments":["Attitude: g=0.49, 95% CI: 0.37, 0.60; intention: g=0.57, 95% CI: 0.46, 0.68","Attitude: g = 0.31, 95% CI: 0.22, 0.41; intention: g = 0.31, 95% CI: 0.20, 0.42","Attitude: g = 0.21, 95% CI: 0.04, 0.38; intention: g = 0.33, 95% CI: 0.24, 0.43."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"308cd3d75510939c68bf03692be9035a","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Self-reflection tools"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Lorenz-Spreen, P., Geers, M., Pachur, T., Hertwig, R., Lewandowsky, S., & Herzog, S. M. (2021). Boosting people’s ability to detect microtargeted advertising. Scientific Reports, 11(1), Article 15541."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1038/s41598-021-94796-z"},"children":["https://doi.org/10.1038/s41598-021-94796-z"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/ne4r9/"},"children":["https://osf.io/ne4r9/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Online platforms’ data give advertisers the ability to “microtarget” recipients’ personal vulnerabilities by tailoring different messages for the same thing, such as a product or political candidate. One possible response is to raise awareness for and resilience against such manipulative strategies through psychological inoculation. Two online experiments (total N=828) demonstrated that a short, simple intervention prompting participants to reflect on an attribute of their own personality—by completing a short personality questionnaire—boosted their ability to accurately identify ads that were targeted at them by up to 26 percentage points. Accuracy increased even without personalized feedback, but merely providing a description of the targeted personality dimension did not improve accuracy. We argue that such a “boosting approach,” which here aims to improve people’s competence to detect manipulative strategies themselves, should be part of a policy mix aiming to increase platforms’ transparency and user autonomy.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Presented hypothetical online advertisements and asked participants to detect the ones that were tailored to them. There were 2 experimental conditions: In the boosting condition, participants completed an 8-item extraversion questionnaire and received personalized feedback on their extraversion score relative to a large sample of online participants. The control condition followed the same procedure, but participants completed an unrelated affinity-to-technology questionnaire.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"10 stimuli, 5 targeting extraverts and 5 targeting introverts (source: Matz et al. 2017; https://doi.org/10.1073/pnas.1710966114)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Age and education",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Experiment 1 tested whether receiving a self-reflection boost in the form of personality assessment would impact participants' ability to identify personalized ads.","Experiment 2 aimed to disentangle the components underlying the effects found in Experiment 1 by omitting individual parts of the intervention step-by-step and observing the resulting effects"],"N":[284,544],"Effect size":["Common Language Effect Size (CL): 0.78 (95%-CI: 0.7-0.84)","Common Language Effect Size (CL): 0.62 (95%-CI: 0.52-0.71)"],"Comments":["Relative to the control condition, participants in the boosting condition correctly identified, on average, 26 percentage points more ads targeted at them (95% Bayesian credible interval, CI 18–35)—raising the mean accuracy from 64% (95% CI 53–73) to 90% (95% CI 85–94).","Reflecting on one’s relevant personality dimension—without receiving any relevant feedback—is necessary, but also sufficient to boost people’s ability to identify ads that have been targeted at them. The boosting condition that included the extraversion questionnaire improved participants’ performance by, on average, 10 percentage points (95% CI 2–20) compared to the boosting condition with only the extraversion description, raising mean accuracy from 72% (95% CI 62–80) to 83% (95% CI 76–88)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"0c37854a3921ce76736ac94989f0e685","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Social norms"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Ecker, U. K. H., Sanderson, J. A., McIlhiney, P., Rowsell, J. J., Quekett, H. L., Brown, G. D. A., & Lewandowsky, S. (2022). Combining refutations and social norms increases belief change. Quarterly Journal of Experimental Psychology."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/17470218221111750"},"children":["https://doi.org/10.1177/17470218221111750"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/ekxzy/"},"children":["https://osf.io/ekxzy/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformed beliefs are difficult to change. Refutations that target false claims typically reduce false beliefs, but tend to be only partially effective. In this study, a social norming approach was explored to test whether provision of peer norms could provide an alternative or complementary approach to refutation. Three experiments investigated whether a descriptive norm—by itself or in combination with a refutation—could reduce the endorsement of worldview-congruent claims. Experiment 1 found that using a single-point estimate to communicate a norm affected belief but had less impact than a refutation. Experiment 2 used a verbally presented distribution of four values to communicate a norm, which was largely ineffective. Experiment 3 used a graphically presented social norm with 25 values, which was found to be as effective at reducing claim belief as a refutation, with the combination of both interventions being most impactful. These results provide a proof of concept that normative information can aid in the debunking of false or equivocal claims, and suggests that theories of misinformation processing should take social factors into account.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants were shown predictive claims (X will have effect Y), followed by refutations of the claims, fictional peer norms at odds with the claims (i.e., norms that indicated low endorsement of/skepticism about the claims), or both. Experiment 1 used a 2 × 2 between-participants design with factors confidentiality (private, public) and post-refutation norm (absent, present). Experiment 2 comprised 2 parallel sub-experiments, 2A and 2B, each with 3 within-subjects conditions. Experiment 2A included refutation, narrow-norm, and wide-norm conditions. Experiment 2B combined norms and refutations, and thus compared a refutation condition (which was identical to Experiment 2A) with refutation-plus-narrow-norm and refutation-plus-wide-norm conditions. Thus, each participant received 3 claims that were then challenged by a refutation and/or norm. Experiment 3 was identical to Experiment 3 but the norms were presented in graphical format as distributions of individual ratings; these could be narrow or wide, representing a sharp or weak consensus.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Fictional articles about real-world topics. All experiments used claims that were worldview-congruent for most participants, as determined by a pilot rating.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Claim belief; predictive estimates relating to claims; misinformation reliance in inferential reasoning; ancillary measures: Need for authenticity; social assertiveness",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2","3"],"Description":["Experiment 1 tested whether social norms can reduce belief in a contested claim, and whether such an effect is dependent on the public nature of belief expressions. Experiment 1 also used a point norm (x out of 100 peers endorsed the claim).","Experiment 2 again examined whether social-norm information can reduce the endorsement of questionable worldview-congruent claims, either in isolation or in combination with a refutation. Moreover, Experiment 2 used a distribution rather than a point norm.","Experiment 3 presented both claim-endorsement and predicted-estimates norms. Like in the Exp. 2, both norms used a distribution-based approach but with a larger number of data points; to facilitate this, a graphical presentation format was employed."],"N":[143,144,154],"Effect size":["Cohen's d (private condition, pre-refutation norm): 0.31","Cohen's d (narrow norm condition): 0.18","Cohen's d (narrow norm condition): 0.77"],"Comments":["mean belief-change scores (Belief Rating 2 – Belief Rating 1) differed significantly from zero in both the private condition, M = −0.29 (SD = 0.93), t(71) = −2.67, d = 0.31, p = .009, and the public condition, M = 0.37 (SD = 0.83), t(70) = −3.71, d = 0.44, p < .001. This demonstrated a small belief-reducing effect of the initial, pre-refutation norm.","Change scores were significantly different from zero in the refutation condition of Experiment 2A, M = −1.53, SD = 1.76, d = 0.87, and all conditions of Experiment 2B (refutation: M = −1.75, SD = 1.98, d = 0.89; refutation-plus-narrow-norm: M = −1.61, SD = 2.14, d = 0.75; refutation-plus-wide-norm: M = −1.15, SD = 1.77, d = 0.65), all t(72) ⩽ 5.54, all p < .001. There was no significant belief change in the norm-only conditions of Experiment 2A (narrow: M = −0.21, SD = 1.15, d = 0.18; wide: M = −0.10, SD = 1.15, d = 0.08), t(72) ⩾ 1.54, p ⩾ .129. This established that claim belief was reduced significantly by a refutation (either with or without an additional norm) but not a stand-alone norm.","Belief change was significantly different from zero in all conditions of Experiment 3A (refutation: M = −8.93, SD = 9.71, d = 0.92; narrow-norm: M = −8.11, SD = 10.54, d = 0.77; wide-norm: M = −6.55, SD = 10.15, d = 0.65), as well as all conditions of Experiment 3B (refutation: M = −10.00, SD = 18.45, d = 0.54; refutation-plus-narrow-norm: M = −18.66, SD = 17.29, d = 1.08; refutation-plus-wide-norm: M = −15.04, SD = 14.36, d = 1.05), all t(75/77) ⩽ 4.79, all p < .001, establishing that claim belief was reduced significantly by a refutation or either type of norm."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"60ce4b23936febc5c9cd3cdbe8bbbee3","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Social norms"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Cookson, D., Jolley, D., Dempsey, R. C., & Povey, R. (2021). A social norms approach intervention to address misperceptions of anti-vaccine conspiracy beliefs amongst UK parents. PLOS ONE, 16(11), Article e0258985."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0258985"},"children":["https://doi.org/10.1371/journal.pone.0258985"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/rhb5p/"},"children":["https://osf.io/rhb5p/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Anti-vaccine conspiracy beliefs among parents can reduce vaccination intentions. Parents’ beliefs in anti-vaccine conspiracy theories are also related to their perceptions of other parents’ conspiracy beliefs. Further, research has shown that parents hold misperceptions of anti-vaccine conspiracy belief norms: UK parents overestimate the anti-vaccine conspiracy beliefs of other parents. The present study tested the effectiveness of a Social Norms Approach intervention, which corrects misperceptions using normative feedback, to reduce UK parents’ anti-vaccine conspiracy beliefs and increase vaccination intentions. At baseline, 202 UK parents of young children reported their personal belief in anti-vaccine conspiracy theories, future intentions to vaccinate, and their perceptions of other UK parents’ beliefs and intentions. Participants were then randomly assigned to a normative feedback condition (n = 89) or an assessment-only control condition (n = 113). The normative feedback compared participants’ personal anti-vaccine conspiracy beliefs and perceptions of other UK parents’ beliefs with actual normative belief levels. Parents receiving the normative feedback showed significantly reduced personal belief in anti-vaccine conspiracy beliefs at immediate post-test. As hypothesised, changes in normative perceptions of anti-vaccine conspiracy beliefs mediated the effect of the intervention. The intervention, did not directly increase vaccination intentions, however mediation analysis showed that the normative feedback increased perceptions of other parents’ vaccination intentions, which in turn increased personal vaccination intentions. No significant effects remained after a six-week follow-up. The current research demonstrates the potential utility of Social Norms Approach interventions for correcting misperceptions and reducing anti-vaccine conspiracy beliefs among UK parents. Further research could explore utilising a top-up intervention to maintain the efficacy.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"The study employed a 2 × 3 (intervention condition by time) mixed experimental design. Parents of young children reported on anti-vaccination conspiracy beliefs, then were randomly assigned to a normative feedback condition (revealing the actual norm amongst parents) or a control condition. Beliefs and intentions were assessed at 3 time points: baseline, immediately after the intervention (for the control condition, this was immediately after a 60-second delay), and a 6-week follow-up. Intervention: 1 page of normative feedback (a graph of personal belief against the norm amongst parents in the UK) along with an accompanying text.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"A scenario, in which participants are asked to imagine that they were the parent of an infant named Sophie, aged 8 months, and that their doctor had provided them with information regarding the (fictitious) disease ‘dysomeria’, which may lead to serious consequences with symptoms such as fever and vomiting.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Belief, intention to vaccinate, (change in) perceived norms",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study tested the effectiveness of a Social Norms intervention, which corrects misperceptions using normative feedback, to reduce UK parents’ anti-vaccine conspiracy beliefs and increase vaccination intentions."],"N":[202],"Effect size":["Partial eta squared (η2p): 0.03"],"Comments":["There was a significant interaction between time and condition on belief in anti-vaccine conspiracy theories, indicating the effectiveness of the intervention, F(1.56, 253.27) = 4.73, p = .016, ηp2 = .03. There was no difference in belief in anti-vaccine conspiracy theories from baseline to the six-week follow up."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"b53abaef72f5e60899e67a4022a89ca1","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Social norms"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Andı, S. & Akesson, J. (2021). Nudging away false news: Evidence from a social norms experiment. Digital Journalism, 9(1), 106–125."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1080/21670811.2020.1847674"},"children":["https://doi.org/10.1080/21670811.2020.1847674"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Many are concerned with the proliferation of false information on social media. This article explores whether “social norm-based nudges” can help address this issue by changing the sharing behaviour of social media users. In order to do so, we conduct an online survey experiment (n = 1,003), where participants are randomly exposed to a social norm-based message while choosing to read and/or share a false news article. The message warns participants that there is an abundance of “false information” online and tells them that most responsible people think twice before sharing articles with their network. Our analysis finds that the nudge reduced the proportion of people willing to share the article by 5.1 percentage points, with a 46.7% increase in the proportion of respondents stating that they do not want to share the article because it is false or inaccurate.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants read 1 of 3 fake news articles (2 right-leaning, 1 left-leaning) and were randomized to either a social norm or a control condition. The social norm condition combined descriptive and injunctive norms into a general warning that \"there is a lot of misinformation out there and most people think twice about sharing fake news with their friends and family.\"",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Three articles from false news sites that were active at the time of the experiment. Two of the articles were taken from conservative-leaning websites and one article was taken from a left-leaning website.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Willingness to share; motivation for sharing; understanding of the article's claims",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study experimentally tested whether  a social norm-based message would impact participants' willingness to share a false news article."],"N":[1004],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["5.17% reduction in willingness to share (SE = 0.02) or 27.5% reduction compared to control sharing (18.8%)"]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"fa95471c2c57528405b63241e0781ee4","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Clayton, K., Blair, S., Busam, J. A., Forstner, S., Glance, J., Green, G., Kawata, A., Kovvuri, A., Martin, J., Morgan, E., Sandhu, M., Sang, R., Scholz-Bright, R., Welch, A. T., Wolff, A. G., Zhou, A., & Nyhan, B. (2020). Real solutions for fake news? Measuring the effectiveness of general warnings and fact-check tags in reducing belief in false stories on social media. Political Behavior, 42(4), 1073–1095."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1007/s11109-019-09533-0"},"children":["https://doi.org/10.1007/s11109-019-09533-0"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YDC4XD"},"children":["https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YDC4XD"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Social media has increasingly enabled “fake news” to circulate widely, most notably during the 2016 U.S. presidential campaign. These intentionally false or misleading stories threaten the democratic goal of a well-informed electorate. This study evaluates the effectiveness of strategies that could be used by Facebook and other social media to counter false stories. Results from a pre-registered experiment indicate that false headlines are perceived as less accurate when people receive a general warning about misleading information on social media or when specific headlines are accompanied by a “Disputed” or “Rated false” tag. Though the magnitudes of these effects are relatively modest, they generally do not vary by whether headlines were congenial to respondents’ political views. In addition, we find that adding a “Rated false” tag to an article headline lowers its perceived accuracy more than adding a “Disputed” tag (Facebook’s original approach) relative to a control condition. Finally, though exposure to the “Disputed” or “Rated false” tags did not affect the perceived accuracy of unlabeled false or true headlines, exposure to a general warning decreased belief in the accuracy of true headlines, suggesting the need for further research into how to most effectively counter false news without distorting belief in true information.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Evaluated the effectiveness of labels on false news headlines to counter false stories on a 2 (general warning: yes, no) × 3 (label: none, \"disputed,\" \"rated false\") between-subjects design, including 6 treatment groups and 1 control group.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"6 false news headlines (3 Pro-Trump and 3 anti-Trump) from Snopes and Buzzfeed. 3 true political headlines from mainstream media sources. News sources (and authors)  were omitted to minimize potentially confounding variables and isolate the effects of warnings and tags on belief in false news headlines.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, use of social media, political preferences, voting behavior, and trust in fact-checking and the media; approval of Trump",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1: General warning","1: Disputed tag","1: Rated False tag"],"Description":["Participants received a general warning about misleading articles","Specific headlines are accompanied by a Disputed tag.","Specific headlines are accompanied by a Rated False tag."],"N":[1250,429,397],"Effect size":["Cohen’s d: 0.08","Cohen’s d: 0.26","Cohen’s d: 0.38"],"Comments":["Average belief in false headlines was slightly lower for participants who saw a general warning before seeing headlines than for participants who saw headlines with no warning (− 0.08; p < .05). However, the substantive magnitude of this reduction in perceived belief accuracy is small (Cohen’s d = 0.08).","Average perceived accuracy for participants who saw a headline with a Disputed tag was 0.24 points lower on our four-point scale than for participants who saw no tag (p < 0.01; Cohen’s d = 0.26).","Average perceived accuracy for participants who saw a headline with a Rated False tag was 0.34 points lower on our four-point scale than for participants who saw no tag (p < 0.01; Cohen’s d = 0.38)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"866bcae343ffafb5c7a19cfc45afe9aa","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Ecker, U. K. H., Lewandowsky, S., & Tang, D. T. W. (2010). Explicit warnings reduce but do not eliminate the continued influence of misinformation. Memory & Cognition, 38(8), 1087–1100."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.3758/MC.38.8.1087"},"children":["https://doi.org/10.3758/MC.38.8.1087"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Information that initially is presumed to be correct, but that is later retracted or corrected, often continues to influence memory and reasoning. This occurs even if the retraction itself is well remembered. The present study investigated whether the continued influence of misinformation can be reduced by explicitly warning people at the outset that they may be misled. A specific warning— giving detailed information about the continued influence effect (CIE)—succeeded in reducing the continued reliance on outdated information but did not eliminate it. A more general warning—reminding people that facts are not always properly checked before information is disseminated—was even less effective. In an additional experiment, a specific warning was combined with the provision of a plausible alternative explanation for the retracted information. This combined manipulation further reduced the CIE but still failed to eliminate it altogether.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Studied whether the continued influence of misinformation can be reduced by explicitly warning people at the outset that they may be misled. Study 1: RCT with 4 retraction conditions and a no-retraction control condition in a between-subjects design. Study 2: A between- subjects design with 3 conditions: (1) specific warning + alternative explanation, (2) no-retraction + specific warning, and (3) alternative-throughout.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 1: A folder with 14 statements with a fictitious account of a minibus accident. Condition 1: retraction only, Condition 2: retraction and an alternative explanation, Condition 3: specific warning, Condition 4: a general warning\n\nStudy 2: Materials identical to Study 1, with two exceptions: Specific warning was presented in all conditions, and, in the alternative-throughout condition, the passengers were initially reported to be young instead of old, and no further correction was presented",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Attention and manipulation checks",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study 1 experimentally tested whether the continued influence of misinformation can be reduced by explicitly warning people at the outset that they may be misled.","Study 2 experimentally tested whether alerting people to the effects of misinformation could eliminate the CIE in cases in which there is alternative information available."],"N":[125,92],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["A general warning did not reduce the level of continued influence effect (CIE) found with a mere retraction. Both a specific warning and the provision of an alternative explanation strongly reduced the CIE The alternative and the specific warning reduced references to misinformation  by 51%–65% (42%–67% in the alternative condition, 53%–63% in the specific warning condition).","The combined effect of the specific warning and the provision of an alternative account reduced reliance on misinformation more than the constituent strategies alone."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"39d948bfe0008a5703c4acd51ee4c7ef","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Pennycook, G., Bear, A., Collins, E. T., & Rand, D. G. (2020). The implied truth effect: Attaching warnings to a subset of fake news headlines increases perceived accuracy of headlines without warnings. Management Science, 66(11), 4944–4957."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1287/mnsc.2019.3478"},"children":["https://doi.org/10.1287/mnsc.2019.3478"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/b5m3n/"},"children":["https://osf.io/b5m3n/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"What can be done to combat political misinformation? One prominent intervention involves attaching warnings to headlines of news stories that have been disputed by third-party fact-checkers. Here we demonstrate a hitherto unappreciated potential consequence of such a warning: an implied truth effect, whereby false headlines that fail to get tagged are considered validated and thus are seen as more accurate. With a formal model, we demonstrate that Bayesian belief updating can lead to such an implied truth effect. In Study 1 (n = 5,271 MTurkers), we find that although warnings do lead to a modest reduction in perceived accuracy of false headlines relative to a control condition (particularly for politically concordant headlines), we also observed the hypothesized implied truth effect: the presence of warnings caused untagged headlines to be seen as more accurate than in the control. In Study 2 (n = 1,568 MTurkers), we find the same effects in the context of decisions about which headlines to consider sharing on social media. We also find that attaching verifications to some true headlines—which removes the ambiguity about whether untagged headlines have not been checked or have been verified—eliminates, and in fact slightly reverses, the implied truth effect. Together these results contest theories of motivated reasoning while identifying a potential challenge for the policy of using warning tags to fight misinformation—a challenge that is particularly concerning given that it is much easier to produce misinformation than it is to debunk it.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Study 1: In the control condition, 12 false and 12 true news headlines were displayed without any warnings. In the treatment condition,participants were shown 6 randomly selected false news headlines displayed with warnings (\"Disputed by 3rd Party Fact-Checkers\"), and the remainder of the items (6 false, 12 true) displayed without any warnings. Study 2: In the control condition, all headlines were presented in their original form. In the first treatment condition, three quarters of false headlines were stamped with “FALSE.” In the second treatment condition, three quarters of false headlines were stamped “FALSE” and three quarters of true headlines were stamped “TRUE.”",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"A series of false and true headlines. All headlines were presented in standard “Facebook format ”with picture, headline, lede sentence, and source. The false news headlines were selected from Snopes.com (verified as having been fabricated and entirely untrue).True news headlines were a selection of contemporary stories from mainstream news outlets that did not contain factual errors or fabrication.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study 1 tests the predictions regarding the existence of a warning effect and an implied truth effect when warnings are attached to a subset of false headlines.","Study 2 tests whether the effects found in Study 1 generalize to sharing intentions."],"N":[5271,1568],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["The warning decreases belief in items that are tagged (the warning effect) but increases belief in items that are untagged (the implied truth effect). Both the warning effect and the implied truth effect were quite small. Warning effect: False headlines in the warning treatment that were presented with warnings were perceived as less accurate (M=0.187) than false headlines in the control (M=0.220). The warning effect was roughly twice as large for politically concordant headlines (warning, M= 0.210; control, M= 0.253) as for politically discordant headlines (warning, M= 0.187; control, M=0.164)","Participants were less likely to consider sharing false headlines tagged with a warning (16.1%) compared with false headlines in the control (29.8%; p<0.001). The warning effect was significantly larger for concordant false headlines (warned: 16.7%; control: 33.7%) than for discordant false headlines (warned: 14.7%; control: 26.0%)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"a949df31de7c9d0b79b1a3757f0ed2cd","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Kim, A., Moravec, P. L., & Dennis, A. R. (2019). Combating fake news on social media with source ratings: The effects of user and expert reputation ratings. Journal of Management Information Systems, 36(3), 931–968."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1080/07421222.2019.1628921"},"children":["https://doi.org/10.1080/07421222.2019.1628921"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"As a remedy against fake news on social media, we examine the effectiveness of three different mechanisms for source ratings that can be applied to articles when they are initially published: expert rating (where expert reviewers fact-check articles, which are aggregated to provide a source rating), user article rating (where users rate articles, which are aggregated to provide a source rating), and user source rating (where users rate the sources themselves). We conducted two experiments and found that source ratings influenced social media users’ beliefs in the articles and that the rating mechanisms behind the ratings mattered. Low ratings, which would mark the usual culprits in spreading fake news, had stronger effects than did high ratings. When the ratings were low, users paid more attention to the rating mechanism, and, overall, expert ratings and user article ratings had stronger effects than did user source ratings. We also noticed a second-order effect, where ratings on some sources led users to be more skeptical of sources without ratings, even with instructions to the contrary. A user’s belief in an article, in turn, influenced the extent to which users would engage with the article (e.g., read, like, comment and share). Lastly, we found confirmation bias to be prominent; users were more likely to believe — and spread — articles that aligned with their beliefs. Overall, our results show that source rating is a viable measure against fake news and propose how the rating mechanism should be designed.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Studied the effectiveness of 3 types of source ratings (expert rating, user article rating, and user source rating) on belief in fake news with a repeated measures experiment. Participants received 2 headlines in the control treatment (no reputation ratings) and the remaining 6 headlines in 1 of 3 randomly assigned between-subjects treatments: expert rating, user article rating, or user source rating.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"8 news headlines on controversial political issues. The headlines focused on the issue of abortion, with four designed to appeal to politically left-leaning participants and the other four to right-leaning participants. To minimize any news source specific effect (e.g., trusted sources versus unknown sources), researchers invented eight names that sounded plausible.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Confirmation bias scale",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study tested the effectiveness of three different types of source ratings: expert rating, user article rating and user source rating.","Study tested the effectiveness of three different types of source ratings: expert rating, user article rating and user source rating."],"N":[590,299],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["When ratings were high, both expert rating and user article rating had significant effects, but user source ratings had no effect. For low-rated sources, all three mechanisms had significant effects; expert ratings had stronger impacts than did user source ratings (Chi-Squared = 15.39, 𝑝<0.001), as did user article ratings (Chi-Squared = 4.87, 𝑝<0.05); there was, however, no significant differences between expert rating and user article rating (Chi-Squared = 2.89, 𝑝>0.05).","High ratings had no significant effect across all rating mechanisms, while low ratings from expert rating and user article rating mechanisms had a negative influence on believability (H2 is partially supported); user source ratings had no effect. Comparing the rating mechanisms that had significant effects, there were no differences in the effects of the expert rating and user article rating mechanisms for low ratings (Chi-Squared = 0.64, p >.05)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"2d40ea4a4344c0867dd95806a3c9b3ac","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Amazeen, M. A., Thorson, E., Muddiman, A., & Graves, L. (2018). Correcting political and consumer misperceptions: The effectiveness and effects of rating scale versus contextual correction formats. Journalism & Mass Communication Quarterly, 95(1), 28–48."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/1077699016678186"},"children":["https://doi.org/10.1177/1077699016678186"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"While fact-checking has grown dramatically in the last decade, little is known about the relative effectiveness of different formats in correcting false beliefs or overcoming partisan resistance to new information. This article addresses that gap by using theories from communication and psychology to compare two prevailing approaches: An online experiment examined how the use of visual “truth scales” interacts with partisanship to shape the effectiveness of corrections. We find that truth scales make fact-checks more effective in some conditions. Contrary to theoretical predictions and the fears of some journalists, their use does not increase partisan backlash against the correction or the organization that produced it.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Examined how the use of visual “truth scales” interacts with partisanship to shape the effectiveness of corrections relying on a 3 × 3 (correction type: rating scale, context only, no correction\n× statement type: same party, opposing party, non-political) experimental design.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"3 controversial statements by fictitious public figures whose partisanship was described as being the same party as participants, opposing party, or non-political.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"manipulation checks, preference for context or rating scales, effectiveness for correction",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study experimentally tested effectiveness of fact-checking corrections with or without a visual rating scale"],"N":[1020],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["There was a significant effect of correction format type on non-political beliefs, F(2, 340) = 4.74, p < .01. In non-political condition, adding a rating scale to a contextual correction increased its effectiveness. Among participants exposed to the pooled political misinformation, there was a significant effect of correction format on political beliefs, as well, F(2, 674) = 10.35, p < .0001. Planned contrasts revealed that both the context only (M = 3.05, SD = 0.65) and context with ratings (M = 3.14, SD = 0.70) correction formats were equally successful at correcting beliefs compared with those receiving no correction (M = 2.81, SD = 0.72). The effectiveness of the correction was higher for those who have the same-party affiliation as the candidate (i.e., those predisposed to react with distrust toward the correction) and those who have the opposing affiliation (predisposed to believe the correction)."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"3272bad3c322720e0ab59ad68e896afb","nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Grady, R. H., Ditto, P. H., & Loftus, E. F. (2021). Nevertheless, partisanship persisted: Fake news warnings help briefly, but bias returns with time. Cognitive Research: Principles and Implications, 6(1), Article 52."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1186/s41235-021-00315-z"},"children":["https://doi.org/10.1186/s41235-021-00315-z"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/gtuha/"},"children":["https://osf.io/gtuha/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Politically oriented “fake news”—false stories or headlines created to support or attack a political position or person—is increasingly being shared and believed on social media. Many online platforms have taken steps to address this by adding a warning label to articles identified as false, but past research has shown mixed evidence for the effectiveness of such labels, and many prior studies have looked only at either short-term impacts or non-political information. This study tested three versions of fake news labels with 541 online participants in a two-wave study. A warning that came before a false headline was initially very effective in both discouraging belief in false headlines generally and eliminating a partisan congruency effect (the tendency to believe politically congenial information more readily than politically uncongenial information). In the follow-up survey two weeks later, however, we found both high levels of belief in the articles and the re-emergence of a partisan congruency effect in all warning conditions, even though participants had known just two weeks ago the items were false. The new pre-warning before the headline showed some small improvements over other types, but did not stop people from believing the article once seen again without a warning. This finding suggests that warnings do have an important immediate impact and may work well in the short term, though the durability of that protection is limited.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Studied the effect of warning labels in 3 conditions—before, during, and after headline and rating—in a 2-wave online study (2-week delay).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Series of headlines (3 false and 9 true) with images above them and questions below them, with four new headlines added at T2, drawn from recent news (one true Democrat-friendly, one true Republican-friendly, and one true politically neutral news item, one politically neutral fake news)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Political Interest, Partisan preferences, Feelings towards political groups, Social media use, Political news consumption",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Study 2 experimentally tested whether subtly making the concept of accuracy salient increased the quality of COVID-19 information that people were willing to share online."],"N":[418],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Participants in the Warning-Before condition believed the false news items less than those the Warning-After condition for both the congruent (b = 1.283, SE = 0.149, p < 0.001) and incongruent (b = 0.734, SE = .120, p < 0.001) items, while the interaction shows that the former was especially pronounced."]},"columns":[{"accessor":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"accessor":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"accessor":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"accessor":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"accessor":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"b04b5a4860ec78d09bafbe3f2f7a12e7","nested":true},"children":[]}]}]},{"accessor":"Intervention","name":"Intervention","type":"character","minWidth":80,"rowHeader":true,"style":{"fontWeight":600}},{"accessor":"References","name":"References","type":"character","minWidth":100},{"accessor":"Experimental setting","name":"Experimental setting","type":"character","minWidth":90,"aggregate":"frequency"},{"accessor":"Design","name":"Design","type":"character","minWidth":80,"aggregate":"frequency"},{"accessor":"Treatment","name":"Treatment","type":"character","minWidth":120},{"accessor":"Outcome variable","name":"Outcome variable","type":"character","minWidth":100},{"accessor":"Sample size","name":"Sample size","type":"numeric","minWidth":75,"format":{"cell":{"digits":0},"aggregated":{"digits":0}},"align":"left"},{"accessor":"Sample Country","name":"Sample Country","type":"character","minWidth":75,"aggregate":"frequency"},{"accessor":"Sample demographics","name":"Demographics","type":"character","minWidth":100,"aggregate":"frequency"},{"accessor":"Recruitment","name":"Recruitment","type":"character","minWidth":75},{"accessor":"Longevity","name":"Longevity","type":"character","minWidth":75},{"accessor":"Main findings","name":"Main findings","type":"character","minWidth":160}],"filterable":true,"searchable":true,"defaultPageSize":6,"showPageSizeOptions":true,"pageSizeOptions":[4,8,12],"paginationType":"numbers","showPageInfo":true,"minRows":1,"highlight":true,"bordered":true,"striped":true,"showSortable":true,"width":"1800px","theme":{"borderColor":"#dfe2e5","stripedColor":"#f6f8fa","highlightColor":"#f0f5f9","style":{"fontFamily":"-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, Arial, sans-serif"},"searchInputStyle":{"width":"100%"}},"elementId":"evidence-table","dataKey":"0529db4f877edb7a3a27653f65577a12"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
<p><a
href="https://ai_society.mpib.dev/intervention_toolbox/privacy.html">Privacy
policy</a> - <a
href="https://ai_society.mpib.dev/intervention_toolbox/terms.html">Imprint/Provider
Identification</a> <!-- ### Note on effect sizes: --></p>
<!-- *Cohen's d* -->
<!-- Interpretation suggested by Cohen: -->
<!--     .2: Small effect size -->
<!--     .5: Medium effect size -->
<!--     .8: or higher: Large effect size -->
<!-- *Partial eta squared (η2p)* "describes a proportion of variability in a sample associated with an independent variable; it is calculated as the ratio between the sum of squares for a particular factor in an ANOVA and that sum of squares combined with the sum of squares for its specific error term." -->
<!-- The following rules of thumb are commonly used to interpret values for Partial eta squared: -->
<!--     .01: Small effect size -->
<!--     .06: Medium effect size -->
<!--     .14 or higher: Large effect size -->
<!-- Note that these are general suggestions for interpretations, and that "the values should be considered in the context of research in an area." -->
<!-- https://www.bps.org.uk/psychologist/methods-why-are-effect-sizes-still-neglected -->
<style type="text/css">
p {
 font-size: 20px;
 width: 1200px;
  margin: 10px 15px 15px 0px;
 }

li {
  font-size: 20px;
  }


h1 {
  font-size: 32px;
  width: 1200px;
}



 button { color: white;
 background-color: rgba(14, 114, 186, 0.7);
 margin: 10px 15px 15px 0px;
 border: none;
 padding: 7px 7px;
 <!-- text-align: center; -->
 <!-- text-decoration: none; -->
 <!-- <!-- display: inline-block; --> -->
 <!-- font-size: 16px -->
  }
  
  
 .button2 { color: black;
 background-color: rgba(14, 114, 186, 0.1);
 margin: 10px 30px 0px 5px;
 border: none;
 padding: 4px 4px;
 font-size: 12px;
  }


body {
  # background-color: rgba(176, 219, 234, 0.5);
   <!-- margin-top: 10px; -->
  <!-- margin-left: 5px; -->
 padding-left: 0px;
   width: 50%;
}

.btn-workflowr {
  display: none
}


.btn-workflowr-sessioninfo {
  visibility: hidden
}

.btn-default {
  visibility: hidden
}

.package-detail {
  padding: 24px;
  box-shadow: inset 0 1px 3px #dbdbdb;
  background: hsl(213, 20%, 99%);
}

.detail-label {
  margin: 20px 0 4px;  
  font-size: 14px;
  color: rgba(0, 0, 0, 0.6);
}

.detail-header {
  margin-bottom: 16px;
  font-size: 20px;
  font-weight: 600;
}

.detail-title {
  margin-left: 18px;
  font-size: 15px;
  font-weight: 600;
  color: rgba(0, 0, 0, 0.8);
}

.detail-description {
  font-size: 14px;
}

.archived-table {
  border: 1px solid hsl(213, 33%, 93%);
  border-radius: 4px;
  box-shadow: 0 2px 7px 0 rgba(0, 0, 0, 0.05);
  font-size: 14px;
}

</style>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span>
Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre><code>R version 4.1.0 (2021-05-18)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 19042)

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] sparkline_2.0     jsonlite_1.7.2    htmltools_0.5.1.1 glue_1.4.2       
 [5] reactable_0.3.0   readxl_1.3.1      here_1.0.1        forcats_0.5.1    
 [9] stringr_1.4.0     dplyr_1.0.7       purrr_0.3.4       readr_1.4.0      
[13] tidyr_1.1.4       tibble_3.1.6      ggplot2_3.3.5     tidyverse_1.3.1  
[17] pacman_0.5.1      workflowr_1.7.0  

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.7        lubridate_1.7.10  getPass_0.2-2     ps_1.6.0         
 [5] assertthat_0.2.1  rprojroot_2.0.2   digest_0.6.27     utf8_1.2.1       
 [9] reactR_0.4.4      R6_2.5.0          cellranger_1.1.0  backports_1.4.1  
[13] reprex_2.0.0      evaluate_0.14     httr_1.4.2        pillar_1.6.4     
[17] rlang_0.4.11      rstudioapi_0.13   whisker_0.4       callr_3.7.0      
[21] jquerylib_0.1.4   rmarkdown_2.14    htmlwidgets_1.5.3 munsell_0.5.0    
[25] broom_0.7.10      compiler_4.1.0    httpuv_1.6.1      modelr_0.1.8     
[29] xfun_0.31         pkgconfig_2.0.3   tidyselect_1.1.1  fansi_0.5.0      
[33] crayon_1.4.1      dbplyr_2.1.1      withr_2.4.2       later_1.2.0      
[37] grid_4.1.0        gtable_0.3.0      lifecycle_1.0.0   DBI_1.1.1        
[41] git2r_0.28.0      magrittr_2.0.1    scales_1.1.1      cli_3.1.0        
[45] stringi_1.6.1     fs_1.5.0          promises_1.2.0.1  xml2_1.3.2       
[49] bslib_0.2.5.1     ellipsis_0.3.2    generics_0.1.0    vctrs_0.3.8      
[53] tools_4.1.0       crosstalk_1.1.1   hms_1.1.0         processx_3.5.2   
[57] yaml_2.2.1        colorspace_2.0-1  rvest_1.0.0       knitr_1.33       
[61] haven_2.4.1       sass_0.4.0       </code></pre>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
https://docs.mathjax.org/en/latest/web/configuration.html. This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>





</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
