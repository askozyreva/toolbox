<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Toolbox: Experimental evidence</title>

<script src="site_libs/header-attrs-2.24/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script src="site_libs/core-js-2.5.3/shim.min.js"></script>
<script src="site_libs/react-17.0.0/react.min.js"></script>
<script src="site_libs/react-17.0.0/react-dom.min.js"></script>
<script src="site_libs/reactwidget-1.0.0/react-tools.js"></script>
<script src="site_libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<link href="site_libs/reactable-0.4.4/reactable.css" rel="stylesheet" />
<script src="site_libs/reactable-binding-0.4.4/reactable.js"></script>

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/main/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>



<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>










<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Toolbox of interventions</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="table_concept.html">Conceptual toolbox</a>
</li>
<li>
  <a href="table_evidence.html">Evidence toolbox</a>
</li>
<li>
  <a href="table_examples.html">Examples of interventions</a>
</li>
<li>
  <a href="toolbox_map.html">Map of evidence</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Toolbox: Experimental evidence</h1>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span>
workflowr <span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span
class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2024-02-20
</p>
<p>
<strong>Checks:</strong> <span
class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 7
<span class="glyphicon glyphicon-exclamation-sign text-danger"
aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>toolbox/</code> <span
class="glyphicon glyphicon-question-sign" aria-hidden="true"
title="This is the local directory in which the code in this file was executed.">
</span>
</p>
<p>
This reproducible <a href="https://rmarkdown.rstudio.com">R Markdown</a>
analysis was created with <a
  href="https://github.com/workflowr/workflowr">workflowr</a> (version
1.7.1). The <em>Checks</em> tab describes the reproducibility checks
that were applied when the results were created. The <em>Past
versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date
</a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate"
class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git
repository, you know the exact version of the code that produced these
results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the
global environment can affect the analysis in your R Markdown file in
unknown ways. For reproduciblity it’s best to always run the code in an
empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20220228code">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Seed:</strong>
<code>set.seed(20220228)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20220228code"
class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20220228)</code> was run prior to running
the code in the R Markdown file. Setting a seed ensures that any results
that rely on randomness, e.g. subsampling or permutations, are
reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Session information:</strong>
recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded"
class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package
versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be
confident that you successfully produced the results during this
run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr
project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsarcgitmpibberlinmpgdeaisocietyinterventiontoolboxtreeb19567bf1157463db5747cbefc992e59873a517ctargetblankb19567ba">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Repository version:</strong>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/tree/b19567bf1157463db5747cbefc992e59873a517c" target="_blank">b19567b</a>
</a>
</p>
</div>
<div
id="strongRepositoryversionstrongahrefhttpsarcgitmpibberlinmpgdeaisocietyinterventiontoolboxtreeb19567bf1157463db5747cbefc992e59873a517ctargetblankb19567ba"
class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development
and connecting the code version to the results is critical for
reproducibility.
</p>
<p>
The results in this page were generated with repository version
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/tree/b19567bf1157463db5747cbefc992e59873a517c" target="_blank">b19567b</a>.
See the <em>Past versions</em> tab to see a history of the changes made
to the R Markdown and HTML files.
</p>
<p>
Note that you need to be careful to ensure that all relevant files for
the analysis have been committed to Git prior to generating the results
(you can use <code>wflow_publish</code> or
<code>wflow_git_commit</code>). workflowr only checks the R Markdown
file, but you know if there are other scripts or data files that it
depends on. Below is the status of the Git repository when the results
were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rproj.user/

Untracked files:
    Untracked:  data/~$toolbox_evidence.xlsx

Unstaged changes:
    Modified:   README.md
    Modified:   _workflowr.R
    Modified:   analysis/_site.yml
    Deleted:    code/README.md
    Deleted:    data/README.md
    Deleted:    images/Criticalignoring_lnos.png
    Deleted:    images/Debunking_lnos.png
    Deleted:    images/Screenshot 2023-10-10 at 08.24.44.png
    Deleted:    images/Screenshot 2023-10-10 at 08.25.18.png
    Deleted:    images/Screenshot 2023-10-10 at 08.26.04.png
    Deleted:    images/Screenshot 2023-10-10 at 08.27.33.png
    Deleted:    images/Screenshot 2023-10-10 at 08.46.36.png
    Deleted:    images/Screenshot 2023-10-10 at 08.51.23.png
    Deleted:    images/debunk_4steps.png
    Deleted:    images/debunk_steps.png
    Deleted:    images/debunking.png
    Deleted:    images/minimal-media-literacy-tips.jpeg
    Deleted:    images/warning-labels-01.png
    Deleted:    images/warning-labels-02.png
    Deleted:    images/warning-labels.png
    Deleted:    output/README.md
    Modified:   output/figure_map.pdf
    Deleted:    output/figure_map.png
    Deleted:    output/figure_map2.pdf
    Modified:   output/infos_raw.json
    Modified:   output/infos_refined.json
    Deleted:    output/table_concept.png
    Deleted:    output/tbl_concept.tex

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not
included in this status report because it is ok for generated content to
have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the repository in which changes were
made to the R Markdown (<code>analysis/table_evidence.Rmd</code>) and
HTML (<code>public/table_evidence.html</code>) files. If you’ve
configured a remote Git repository (see <code>?wflow_git_remote</code>),
click on the hyperlinks in the table below to view the files as they
were in that past version.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/1a1a155d89d156e606ac8a5fb1bcce8f5e5f4f54/public/table_evidence.html" target="_blank">1a1a155</a>
</td>
<td>
Kozyreva
</td>
<td>
2023-11-30
</td>
<td>
minor updates
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/5be9264d045f06d64f3b29c863dadac460d98e93/analysis/table_evidence.Rmd" target="_blank">5be9264</a>
</td>
<td>
Kozyreva
</td>
<td>
2023-11-24
</td>
<td>
what should be the final version of the toolbox 2.0
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/5be9264d045f06d64f3b29c863dadac460d98e93/public/table_evidence.html" target="_blank">5be9264</a>
</td>
<td>
Kozyreva
</td>
<td>
2023-11-24
</td>
<td>
what should be the final version of the toolbox 2.0
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/22a40df6ae28d62b7c0ea9115e7f0ec5f97b4e3b/public/table_evidence.html" target="_blank">22a40df</a>
</td>
<td>
Kozyreva
</td>
<td>
2023-10-24
</td>
<td>
v2 updates
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/663ae9cced3f32b917933b5d6cb6d5a74b749056/public/table_evidence.html" target="_blank">663ae9c</a>
</td>
<td>
Kozyreva
</td>
<td>
2023-10-24
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/3efec23801d2f2de155fa947451f7caa5a436a36/analysis/table_evidence.Rmd" target="_blank">3efec23</a>
</td>
<td>
Kozyreva
</td>
<td>
2023-10-24
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/c2b8b9052ef16d5018e3f1585d26d8da7cd4c1d9/analysis/table_evidence.Rmd" target="_blank">c2b8b90</a>
</td>
<td>
Kozyreva
</td>
<td>
2023-10-12
</td>
<td>
v2 branch update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/c2b8b9052ef16d5018e3f1585d26d8da7cd4c1d9/public/table_evidence.html" target="_blank">c2b8b90</a>
</td>
<td>
Kozyreva
</td>
<td>
2023-10-12
</td>
<td>
v2 branch update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/168d4c7d2eda45ecafaa3c5f6bab4c5bb152ea8e/public/table_evidence.html" target="_blank">168d4c7</a>
</td>
<td>
Kozyreva
</td>
<td>
2023-02-28
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/8c384423d5c386372044901e44ab68713380cf40/public/table_evidence.html" target="_blank">8c38442</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2023-01-05
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/0b025346927bc95988769e5aeacec21a9ba73ba3/public/table_evidence.html" target="_blank">0b02534</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-12-16
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/e57774a5a05fffaaaffb48db594dadcb38f7b2c8/analysis/table_evidence.Rmd" target="_blank">e57774a</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-12-05
</td>
<td>
update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/295f5803709e0b166616a85db09fa4d41a9ed7cb/public/table_evidence.html" target="_blank">295f580</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-12-05
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/1a9c195562c0fa041a4397af710f39c7c73c0b99/analysis/table_evidence.Rmd" target="_blank">1a9c195</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-12-01
</td>
<td>
update and folder reorg
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/2b13b6294025066a0d55eaf7c158c49f527f568c/public/table_evidence.html" target="_blank">2b13b62</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-12-01
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/b8df76ec52d184e6dee941bbe304acd2b0ed0391/public/table_evidence.html" target="_blank">b8df76e</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-11-16
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/51ffb876a4b4292754b481023573fb265b9fa69d/public/table_evidence.html" target="_blank">51ffb87</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-11-16
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/1fcecbdd076a7323019612211b17d03c9b60dca7/analysis/table_evidence.Rmd" target="_blank">1fcecbd</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-11-16
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/421eeda63cadf22f10eb25615e972ee265c0ad28/public/table_evidence.html" target="_blank">421eeda</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-11-08
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/a8e3703733ebe277ddd444a40536d3d81de457e7/analysis/table_evidence.Rmd" target="_blank">a8e3703</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-11-08
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/355e92af17922bf1f7ec1bebf9c3acb628544e9e/public/table_evidence.html" target="_blank">355e92a</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-10-11
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/e3720bd1ae54107f7bd86320ac4f44ff1d1f88fe/analysis/table_evidence.Rmd" target="_blank">e3720bd</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-10-11
</td>
<td>
tables update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/e3720bd1ae54107f7bd86320ac4f44ff1d1f88fe/public/table_evidence.html" target="_blank">e3720bd</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-10-11
</td>
<td>
tables update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/9986d6da064b40e41c82f711ccf242c32c00c432/public/table_evidence.html" target="_blank">9986d6d</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-08-22
</td>
<td>
update
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/0cec29f87ad90622ca41c77ad645aa9e207ae897/analysis/table_evidence.Rmd" target="_blank">0cec29f</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-08-17
</td>
<td>
tables update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/0cec29f87ad90622ca41c77ad645aa9e207ae897/public/table_evidence.html" target="_blank">0cec29f</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-08-17
</td>
<td>
tables update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/b7940992bf1a4ce742b1b6c2eb7feae2bfcaa71e/public/table_evidence.html" target="_blank">b794099</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-08-04
</td>
<td>
tables update
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/a58ab0a09817b92e9fbdd6f4ad5e8b78b3453354/analysis/table_evidence.Rmd" target="_blank">a58ab0a</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-28
</td>
<td>
Minor updates
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/a58ab0a09817b92e9fbdd6f4ad5e8b78b3453354/public/table_evidence.html" target="_blank">a58ab0a</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-28
</td>
<td>
Minor updates
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/7c28e6793a034b9817f978abe680b8630a3b52a0/public/table_evidence.html" target="_blank">7c28e67</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-28
</td>
<td>
design and tables update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/4effdc51fc34c82fa6584eca6556bcf35fed08b3/public/table_evidence.html" target="_blank">4effdc5</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-27
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/d069c9f340fc0e6bdec3c51389a884539efc585f/analysis/table_evidence.Rmd" target="_blank">d069c9f</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-27
</td>
<td>
update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/d069c9f340fc0e6bdec3c51389a884539efc585f/public/table_evidence.html" target="_blank">d069c9f</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-27
</td>
<td>
update
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/9573d602e5c44bb4a0e69ff375fd9a787f8d66ca/analysis/table_evidence.Rmd" target="_blank">9573d60</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-27
</td>
<td>
stylistic update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/9573d602e5c44bb4a0e69ff375fd9a787f8d66ca/public/table_evidence.html" target="_blank">9573d60</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-27
</td>
<td>
stylistic update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/d9e3edd86c218c79c75863e94035d28979153c94/public/table_evidence.html" target="_blank">d9e3edd</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-26
</td>
<td>
small updates in the evidence table
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/8c3b8f3490d1f5b3fdd62f1ea409d69124fbbdda/analysis/table_evidence.Rmd" target="_blank">8c3b8f3</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-25
</td>
<td>
json code update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/8c3b8f3490d1f5b3fdd62f1ea409d69124fbbdda/public/table_evidence.html" target="_blank">8c3b8f3</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-25
</td>
<td>
json code update
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/d8a361848d30bf25e7f9fe75ed7119db5c51375a/analysis/table_evidence.Rmd" target="_blank">d8a3618</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-21
</td>
<td>
update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/d8a361848d30bf25e7f9fe75ed7119db5c51375a/public/table_evidence.html" target="_blank">d8a3618</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-21
</td>
<td>
update
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/f182d728e90a4942a02ebd51dead53bf0bb599ae/public/table_evidence.html" target="_blank">f182d72</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-20
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/de4399c0f3eb0f54b9a3f61d56850281d0c79b34/analysis/table_evidence.Rmd" target="_blank">de4399c</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-07-20
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/f9405d666c6edecf3dd8ff85e4d7dd6f1324d2c5/public/table_evidence.html" target="_blank">f9405d6</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-06-27
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/fe780ff805d53704b1bac2650a4db66834d2b234/public/table_evidence.html" target="_blank">fe780ff</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-06-27
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/5f9f2989244f60195028678a5850edea129a4df5/public/table_evidence.html" target="_blank">5f9f298</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-06-27
</td>
<td>
lnos test
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/8ab172cc3f1d6e6a1a2e1a5a725064cb52cbda8d/public/table_evidence.html" target="_blank">8ab172c</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-06-08
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/2eaf83118e46e03ac0c60860d052158c2b4dfae8/public/table_evidence.html" target="_blank">2eaf831</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-06-02
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/4f9aedf653c62954ea8f33d4624d895308c09ed6/analysis/table_evidence.Rmd" target="_blank">4f9aedf</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-06-02
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/adf0386aa151a45e7f6e84758869018a8efd7435/public/table_evidence.html" target="_blank">adf0386</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-05-20
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/d3215247cd5ffa2bb90adea18884744db154bd18/analysis/table_evidence.Rmd" target="_blank">d321524</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-05-20
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/33c2f643f77cdb0894b27805305b486737d2d953/public/table_evidence.html" target="_blank">33c2f64</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-05-19
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/ad714146cd0adb86e215b044f55f43a118b30ed8/analysis/table_evidence.Rmd" target="_blank">ad71414</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-05-19
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/77a734a9e8267e020fb3c12eabd25ed9ca17c965/public/table_evidence.html" target="_blank">77a734a</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-05-17
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/1504401af906bb6f12e4b62048c8b3b4c817f32a/public/table_evidence.html" target="_blank">1504401</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-05-16
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/7a81fdc492f9cfc444f6eaa5da3b36920b6c2b98/public/table_evidence.html" target="_blank">7a81fdc</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-04-28
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/c79c85805c4f7fb3c2e04c38b4d332b20187f32a/public/table_evidence.html" target="_blank">c79c858</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-04-27
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/b8f7c8d8e1462e081027eea0f17133036204ebb1/analysis/table_evidence.Rmd" target="_blank">b8f7c8d</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-04-27
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/280c6e4bbc22c57a827a7764eef2f1fa6206b2b7/public/table_evidence.html" target="_blank">280c6e4</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-04-27
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/3524d8867c7bb0d58aef7fbec529cae4128b15cc/analysis/table_evidence.Rmd" target="_blank">3524d88</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-04-27
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/c35a82df639ed4f66a5cb21cd90284e1aa7300e7/public/table_evidence.html" target="_blank">c35a82d</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-04-27
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/47e86c9788aae194b6c78211634e55af56f17f01/analysis/table_evidence.Rmd" target="_blank">47e86c9</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-04-27
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/14c638fa168265fa10c3d843ae119b0da2c9c399/public/table_evidence.html" target="_blank">14c638f</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-03-01
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/658b5b44ce7cf36f49dfc6f0040875c1aa332cdf/analysis/table_evidence.Rmd" target="_blank">658b5b4</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-03-01
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/7ea496ea95f7053c3ae5cf2ce82b6acbdc8d9d75/public/table_evidence.html" target="_blank">7ea496e</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-02-28
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/b39ffdb99ddfbb2b160579b8e8315ae7a6aa7aeb/analysis/table_evidence.Rmd" target="_blank">b39ffdb</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-02-28
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/17e42921e238142a19bef3a84bfed50f1ae08bcf/public/table_evidence.html" target="_blank">17e4292</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-02-28
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/6ab970baf6b2147f053bc39ac379876b25f20ba8/public/table_evidence.html" target="_blank">6ab970b</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-02-28
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://arc-git.mpib-berlin.mpg.de/ai_society/intervention_toolbox/blob/6aadd32df4eff9448693c36c8627961ca0794b20/analysis/table_evidence.Rmd" target="_blank">6aadd32</a>
</td>
<td>
Anastasia Kozyreva
</td>
<td>
2022-02-28
</td>
<td>
wflow_publish(files = files, republish = TRUE, delete_cache = TRUE,
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<p>This part of the online supplement is a digital toolbox in the form
of a dynamic table that offers an overview of published evidence . For
publication details, links to articles, and detailed results,
<strong><em>click on the arrow or the “Expand” button</em></strong> to
the left of the intervention type. To search through the whole table,
use the search function below. Use the smaller search fields under a
column’s header to search within that column. You can sort a column by
clicking on its header, or sort multiple columns by holding the shift
key while sorting.</p>
<button onclick="Reactable.toggleAllRowsExpanded(&#39;evidence-table&#39;)">Click here to expand/collapse all rows</button>
<div id="evidence-table" class="reactable html-widget " style="width:1800px;height:auto;"></div>
<script type="application/json" data-for="evidence-table">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"Intervention":["Accuracy prompts","Accuracy prompts","Multiple interventions: Accuracy prompts, Media-literacy tips","Accuracy prompts","Accuracy prompts","Multiple interventions: Accuracy prompts & Media-literacy tips","Multiple interventions: Accuracy prompts & Media-literacy tips & Fact-checking labels","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Debunking and Rebuttals","Friction","Friction","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Inoculation","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Lateral reading and verification strategies","Media-literacy tips","Media-literacy tips","Media-literacy tips","Media-literacy tips","Source-credibility labels","Source-credibility labels","Source-credibility labels","Source-credibility labels","Social norms","Social norms","Social norms","Multiple interventions: Social norms and Source-credibility labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels","Warning and fact-checking labels"],"References":["Pennycook et al., Psych Sci (2020).","Pennycook et al., Nature (2021).","Epstein et al., Harv Misinfo Review (2021).","Roozenbeek et al., Psych Sci (2021).","Pennycook & Rand, Nat Comms (2022).","Arechar et al., Nat Hum Beh (2023).","Offer-Westort et al., preprint (2022).","Swire et al., J Exp Psychol: Learn Mem Cogn (2017).","Huang, Brit J Polit Sci (2017).","Gesser-Edelsburg et al., PLOS ONE (2018).","Paynter et al., PLOS ONE (2019).","Schmid et al., NHB (2019).","Schmid et al., J Cogn (2020).","Ecker et al., Cogn Res: Princ Implic (2020).","Bowles et al, PLOS ONE (2020).","Porter and Wood, Proc Nat Acad Sci (2021).","Winters et al., BMJ (2021).","Tay et al., Br J Psychol (2022).","Schmid and Betsch, Sci Comm (2022).","Bauer and Wilson, China Quart (2022).","Schroeder and Kucera, Ed Psyc Rev (2022).","Yu et al., Telemat Inform (2022).","Batista Pereira et al., JoP (2022).","Badrinathan and Chauchard, IJPP (2023).","Porter et al., Ro Soc Op Sci (2023).","Armand, Augsburg et. al, preprint (2021).","Hirshleifer et al, preprint (2022).","Bruns et al., preprint (2023).","Fazio, Harv Misinfo Review (2020).","Pillai and Fazio, Collabra Psychol (2023).","van der Linden et al., Global Challenges (2017).","Cook et al., PLOS ONE (2017).","Roozenbeek et al., Palgrave Commun (2019).","Roozenbeek et al., Harv Misinfo Review (2020).","Basol et al., BD & S (2021).","Maertens et al., J Exp Psychol: Appl (2021).","Roozenbeek et al., Sci Adv (2022).","Lu et al., JMIR (2023).","Wong and Wu, J Risk Res (2023).","Batista Pereira et al., JEPS (2023).","Armand, Fracchia et al., preprint (2021).","Garg and Yadav, preprint (2022).","Bowles et al., preprint (2023).","Athey et al., preprint (2023).","Wineburg et al., Teach Coll Rec (2019).","McGrew et al., Br J Educ Psychol (2019).","McGrew, Comput Educ (2020).","Donovan et al., Mem Cogn (2020).","Brodsky et al., Cogn Res: Princ Implic (2021).","Breakstone et al., HKS Misinfo Rev (2021).","Yang et al., Comput Educ (2021).","Kobayashi et al., PLOS ONE (2021).","Wineburg et al., J Educ Psychol (2022).","Panizza et al., Sci Rep (2022).","McGrew and Breakstone, AERA Open (2023).","Fendt et al., Comp Hum Beh (2023).","Barzilai et al., Comput Educ (2023).","Apuke & Gever, J Acad Libr (2023).","Resnick et al., preprint (2021).","Guess et al., PNAS (2020).","Badrinathan, Am Political Sci Rev (2021).","Ali & Qazi, JDE (2023).","Qian et al, J Comput-Mediat Comm (2023).","Kim et al., JMIS (2019).","Aslett et al, Sci Adv (2022).","Shahid et al,  Proc. ACM Hum Comput Interact (2022).","Celadin et al., Journal of Online Trust and Safety (2023).","Cookson et al., PLOS ONE (2021).","Andi et al., Digit Journal (2021).","Ecker et al., QJEP (2022).","Prike et al, preprint (2023).","Ecker et al., (2010). Mem Cogn.","Kim et al., Manag Inf Syst (2019).","Mena, Pol & Int (2019).","Clayton et al., Political Behav (2020).","Pennycook et al., Manag Sci (2020).","Ecker et al., Brit Psych Soc (2020).","Grady et al., Cogn Res: Princ Implic (2021).","Lee et al., Health Comm (2023).","Koch et al, J App Soc Psych (2023).","Moon et al, Mass Comm and Soc (2023)."],"Experimental setting":["Online","Online; Field","Online","Online","Online","Online","Online","Laboratory","Online","Classroom and online","Field","Online","Laboratory","Online","Online","Online","Online","Online","Online","Online","Online; Field","Online","Field","Online","Online","Field\nmobile phone-based campaign","Social media platform","Online","Online","Online","Online","Online","Online","Online","Online","Online;Laboratory","Online; Field","NA","Online","Online","Field\nphone survey/SMS","Custom mobile app","Online","Online","Laboratory","Field (Higher education)","Field (High school)","Online; Laboratory","Field (Higher education)","Field (Higher education)","Online","Online","Field (High school)","Online","Field (Secondary school)","Laboratory","Field (Lower-secondary schools)","Field (Higher education)","Online","Online; Field","Field","Field","Online","Online","Online; Field","Online","Online","Laboratory","Laboratory","Laboratory","Online","Laboratory","Online","Online","Online","Online","Online","Online","Online","Online","Online"],"Design":["RCT","RCT","RCT","RCT (replication test)","Internal meta-analysis","RCT","RCT","Between-subjects pre–post design","Between-subjects design; RCT","RCT","RCT","Within-subjects pre–post design; Between-subjects pre–post design","Within-subjects pre–post design; Between-subjects pre–post design","Mixed-design RCT","RCT","Between-subjects design; RCT; internal meta-analysis","RCT","RCT","Between-subjects pre–post design","Between-subjects design; RCT","Meta-analysis","Between-subjects RCT","RCT","RCT","RCT","RCT","RCT","Between-subjects pre–post design","RCT","RCT","Pre–post, within-subjects, mixed-design RCT","Mixed-design RCT","Within-subjects pre–post design","Mixed-design RCT","Within-subjects pre–post design; RCT","Longitudinal RCT","RCT","Meta-analysis","Between-subjects RCT; qualitative analysis of focus groups","RCT","RCT","RCT","RCT","Pre-post test RCT","Expert–novice study","RCT; Within–subjects pre-post design","Within-subjects pre–post design","Mixed-design RCT","RCT (matched-control design)","Within-subjects, treatment-only, pre–post design","Between-subjects RCT","RCT","RCT (matched-control design)","RCT","A pre-and-posttest, treatment-only intervention","A pre-post 3 × 2 factorial design","Between-subjects RCT","Between-subjects RCT","Mixed-design RCT","Longitudinal RCT","RCT (randomized block design)","Longitudinal RCT","RCT","Mixed within-between design (credibility level within; type of credibility rating between)","RCT","Between subjects design","Between-subjects experiment","Longitudinal RCT","RCT","Between-subjects pre–post design","2x2 Between-subjects design","Between-subjects design; RCT","Within–between design with repeated measures","2x2 between-subjects design","Between-subjects pre–post design","RCT","Within-subject and within-between subjects","Longitudinal between-subjects design","Between-subjects design","2x2 between-subjects design","2x3 factorial design"],"Treatment":["Accuracy prompt: accuracy rating question.","Accuracy prompt: accuracy rating question.","Accuracy prompt in 3 formats: Accuracy rating, long evaluation, and importance of sharing; Media literacy tips","Accuracy prompt: accuracy rating question.","Accuracy prompts: evaluation, importance, norms, PSA video, reason, and tips.","Accuracy prompt: accuracy rating question and media literacy tips.","Headline- level treatments (flags, warning labels); Respondent-level messaging treatments, (tips for spotting fake news, training videos, nudges)","Misinformation corrections and factual affirmations in the form of brief retraction, detailed refutation, brief affirmation, and detailed affirmation.","Debunking from social media platforms, police station, a prominent businessman and government critic","Information corrections (transparent, addressing public concerns)","Training materials combining basic refutational approach with 6 boosts: source credibility, self-affirmation, social norming, warning, graphical representations, and salience.","Topic rebuttal, technique rebuttal.","Study 1: Outnumbering and rebuttal; Study 2: Outnumbering, rebuttal, and forewarning; Study 3: Outnumbering from multiple sources, rebuttal, and forewarning.","Narrative and non-narrative corrections focusing on factual evidence.","WhatsApp messages from a trusted civil society organization targeting COVID-19 misinformation","Brief summaries of false claims (both country specific and global misinformation); fact-checks (simple corrections) from fact-checking organizations","Risk communication information in the form of four-episode audio\ndramas delivered via WhatsApp","Prebunking and debunking in the form of intervention articles.","Text-based debunking","Debunking from government agency","Refutation texts in comparison to other learning methods and controls","Text-based correction: correction sources (official vs. professional vs. layperson); Tone (formal vs. conversational)","Short fact-checking statement refuting previously shown information","Text-based correction: Domain expert; Fact checker; Unsubstantiated correction.","Correction of misinformation from fact-checking organizations","Voice and text-based corrections (doctors from locally-renowned hospitals)","Post moderation and removal, rebuttal of misinformation\n","Text-based debunking with and without source information","Friction in the form of an explanation prompt (why a headline is true or false)","Friction in the form of an explanation prompt (why a headline is true or false)","Inoculation: forewarning and preemptive refutation.","Inoculation: explanation messages.","Learning game of misinformation techniques: impersonation, emotional language, group polarisation, floating conspiracies, discrediting opponents, trolling.","Inoculation in the form of manipulation techniques learned in the game: trolling, exploiting emotional language, artificially amplifying the reach, conspiracy theories, and polarisation.","Study 1: Inoculation in the form of manipulation techniques learned in the GoViral! game (using moral–emotional language, using fake experts, and conspiratorial reasoning); Study 2: Real-world infographics.","Inoculation in the form of manipulation techniques learned in the game: using moral–emotional language, using fake experts, and conspiratorial reasoning.","5 short (≈ 1.5 min.) inoculation videos, each inoculating against a specific manipulation technique (emotional manipulation, incoherence, false dichotomies, scapegoating, ad hominem attacks).","All inoculation interventions (fact-based, technique-based etc)","Learning game on fake news about COVID-19 vaccines that was modeled on the Bad News game. Deception techniques learned: impersonation, emotional content, polarisation, conspiracy, and discrediting of opponents.","Preemptive interventions including misinformation awareness and media literacy tips","Endorsement: Provides COVID-19 vaccine info and encourages its use. Social memory: Recalls successful polio eradication in Mozambique.\nInoculation: Promotes critical thinking, reduces sharing of fake news in interactions.","Weekly digests of misinformation with fact-checks with explanations; Two address misinformation related to Muslims with facts and narratives of those impacted by misinformation.","Fact-checks from \"Africa Check\" in diverse formats: as a text message, a condensed podcast, or a standard-length podcast (empathetic content).","Text-message courses developed with \"First Draft\": Emotion-based techniques course, reasoning based techniques course, combo (Emotions + reasoning) course","Lateral reading through think-aloud protocols.","Classroom-based lateral reading to evaluating the credibility of online information.","Classroom-based lessons on evaluating online information.","Studies 1, 2: Encouragement to research online with plain access.","Classroom-based lesson on the Digital Polarization Initiative's 4 fact-checking moves.","1-hour course modules: instructional videos; exercises in evaluation of online sources about nutrition; and instructional screencasts","Online educational game \"Trustme!\" designed to foster evaluation strategies.","Requirement to search for evidence either for or against the claims.","Classroom-based lateral reading as part of Civic Online Reasoning curriculum.","Social media pop-up with a list of civic online reasoning techniques (e.g., lateral reading, click restraint) as tips to verify the information in the post; pop-up combined with monetary incentives; monetary incentives alone.","Classroom-based lessons on lateral reading.","Text-based and voice-based guide outlining lateral reading techniques (identifying disinformation); Cognitive apprenticeship methods (coaching, and scaffolding).","\"Misinformation Is Contagious\" game encouraging strategies for assessing online information and responsible sharing.","8-week course on social media literacy.","Requirement to search for corroborating evidence.","Tips on how to spot false news.","Media literacy in-person training, flyer with tips.","Tips presented in a video and personalised feedback.","Digital media literacy education infographic.","Three different types of source-credibility rating (expert-generated vs. user-generated based on either aggregated article ratings or direct source ratings)","Treatment group was encouraged to instal NewsGuard add-on on their desktops' browsers.","One of the seven source conditions: No source (baseline), Strangers, Friends, Family, Celebrity, Journalist, and News Media","Posts with vs. without (expert or lay person-based) source-credibility rating (x/5 stars).","Normative feedback in the form of a text.","A warning in the form of descriptive and injunctive social norms.","Descriptive(fictional) norms contesting belief-congruent claims; refutations; norms and refutations.","Combined injunctive/descriptive social norm; source-credibility scores for both post sources and their own \"account\".","Specific warning(detailed information about the continued influence effect); general warning (reminder that facts are not always properly checked before information is disseminated).","Three types of source ratings: expert rating, user article rating, and user source rating.","Warning labels “Disputed by Snopes.com and PolitiFact”","General warning about misleading articles; specific warnings about individual articles questioned by fact-checkers (“Disputed” or “Rated false” tags under false headlines).","Warning labels: \"Disputed by 3rd Party Fact-Checkers\" and \"False\" stamped over a headline.","Affirmations; Plain retractions; Detailed refutations; Inference questions.","Warning labels indicating that a headline is false presented before, during, or after the headline.","Fact-checking label that refutes the misinformation participants initially saw.","Warning labels “Challenged by fact-checkers” and “learn more about why the post is contested.”","Fact-checking message displayed above the false news story"],"Paradigm":["Headline-discernment paradigm","Headline-discernment paradigm; Field study","Headline-discernment paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm; Field experiment","Misinformation-correction paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Headline-discernement paradigm","Headline-discernment paradigm","Headline-discernement paradigm","Headline-discernment paradigm","Technique-adoption paradigm; Headline-discernment paradigm","Meta-analysis","Headline-discernment paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Headline-discernment discernment","Misinformation-correction paradigm; Technique-adoption paradigm","Headline-discernment paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Technique-adoption paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Technique-adoption paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Technique-adoption paradigm","Headline-discernement paradigm","Field study","Headline-discernement paradigm","Headline-discernment paradigm","Misinformation-correction paradigm","Headline-discernment paradigm","Misinformation-correction paradigm","Headline-discernment paradigm","Misinformation-correction paradigm","Misinformation-correction paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Misinformation-correction paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Headline-discernment paradigm","Headline-discernment paradigm"],"Outcome variable":["Sharing discernment","Sharing discernment","Sharing discernment","Sharing discernment","Sharing discernment","Sharing discernment","Sharing discernment","Direct belief ratings, reliance on misinformation in inference questions","Belief in rumor and political trust","Trust in and satisfaction with the Health Ministry, inclination to seek further information, decision to send a child to kindergarten, and intent to vaccinate the child.","Support for evidence-based autism treatments, use intention","Attitudes towards behaviors favored by science and intentions to perform these behaviors","Attitudes towards vaccination, intention to vaccinate","Reliance on misinformation in inference questions, misinformation beliefs","COVID-19 knowledge and behavior","Belief in misinformation","Belief that typhoid is caused by mosquitoes, Belief that typhoid can only co-occur with malaria","Reliance on misinformation in inference questions, willingness to purchase fair-trade products","Credibility judgments of (mis)information","Accuracy/credibility ratings and political attitudes","Learning outcomes, incl. conceptual change, transfer tests, comprehension or retention tests, or unspecified learning tests","Correction believability","Beliefs in false rumors.","Accuracy discernment","Belief accuracy, intent to vaccinate, and vaccine attitudes.","Preventive practices against COVID-19, compliance with evidence-based recommendations, trust in information, fact-checking and beliefs.","Platform usage (intensive and extensive margins); dissemination of false and accurate posts on COVID-19","Agreement with the claim; credibility assessment; intentions to share","Likelihood to share false headlines","Likelihood to share false headlines","Estimate of the scientific agreement on human-caused climate change (0%–100%)","Perceived consensus, acceptance of anthropogenic global warming","Reliability ratings of tweets","Susceptibility to political misinformation; confidence in reliability ratings; likelihood to share posts with others","Manipulativeness ratings, confidence in manipulativeness rating, willingness to share","Reliability ratings of news headlines","Studies 1–6: manipulation technique recognition, attitudinal certainty (confidence), trustworthiness discernment, sharing intentions; \nStudy 7: manipulation technique recognition","Perceived credibility of misinformation and real information, credibility discernment, sharing discernement","Fake news identification","Rumor acceptance; change in response to the repeated rumor; acceptance of true news; acceptance discernement","Acceptance of COVID-19 vaccines, trust in health institution, and behavioral measures","Accuracy discernment, skepticism, factual beliefs, policy attitudes and donation behavior towards out-group","Discernment of truth and false information/conspiracy theories; verification knowledge, trust in and consumption of media sources; verification and information sharing behavior; attitudes and behaviors (COVID-19, politics)","Sharing intention, Accuracy discernment","Evaluation of credibility of online sources","Evaluation of credibility of online sources","Civic online reasoning skills including ad identification, lateral reading, analyzing evidence, claim research","Accuracy of reproduced information, self-reported search rates","Scores for evaluations of the trustworthiness of online content, self-reported use of lateral reading","Evaluation of credibility of online sources","Information accuracy discernment and skepticism toward online information","Belief in misinformation (subjective truthfulness of the statement presented)","Assessment score for judgments of credibility of online sources on a 3-point rubric (beginning, emerging, mastery)","Accuracy score, correct guessing (binary), technique adoption (search behavior, self-report)","Evaluation of the credibility of online sources","Evaluation of the credibility of online news articles","Sharing intentions and discernment; accuracy judgments and discernment; metastrategic knowledge about evaluation strategies; sharing misinformation intention","Social media knowledge; evaluation of the objectivity, professionalism, argument strength, trustworthiness, sharing likelihood; fake news verification capability","Correlation of lay ratings with journalist ratings","Accuracy discernment between false and mainstream headlines","Binary identification of misinformation","Identification of misinformation (agreement with news statement)","Perceived credibility of visual posts, credibility discernment, intention to use reverse image search","Believability of headlines","Information diet quality (specifically, news consumed from publishers of low-quality news sources)","Perceived trust; intention to share","Sharing intent","Personal belief in anti-vaccine conspiracy theories, intention to vaccinate,perceptions of other UK parents’ beliefs and intentions","Willingness to share a false news article","Endorsement of belief-congruent claims (in beliefs, predictive estimates, inferential reasoning)","Engagement (composite score of flagging (-1), skipping (0), liking (+1), sharing (+2)) and Claim Belief","Reliance on misinformation in inference questions","Rating of article's believability and likelihood to engage with it","Sharing intentions; message credibility","Accuracy ratings of news headlines, willingness to “like” or share a given headline on Facebook","Accuracy ratings, sharing intentions","Claim belief ratings; and inferential-reasoning","Accuracy judgment (on a binary scale)","MMR conspiracy belief, intentions to engage in misinformation about the MMR vaccine, and MMR vaccination intentions.","Credibility judgements; engagement (liking and sharing).","Credibility of fact-checking messages (accuracy, trustworthiness, objectivity)."],"Sample size":[856,7955,2391,1583,26863,33480,15292,202,1430,243,47,1773,887,2279,868,8000,736,735,1387,561,3.869,3000,2236,5104,13060,3991,3698,5228,501,499,2167,1106,14163,681,3548,315,29116,42530,122,2037,2916,1301,4543,8684,45,67,68,561,230,87,210,3221,499,3520,574,312,215,470,1301,11924,1309,750,880,889,3337,478,1627,202,1003,441,415,217,889,501,2994,6839,900,418,206,571,354],"Sample Country":["United States","United States","United States","United States","United States","Argentina, Australia, Brazil, China, Egypt, India, Italy, Mexico, Nigeria, Philippines, Russia, Saudi Arabia, South Africa, Spain, United Kingdom, United States","Kenya, Nigeria","Australia","China","Israel","Australia","Germany, United States","Germany","United States","Zimbabwe","Argentina, Nigeria, South Africa, United Kingdom","Sierra Leone","United States","Germany","Taiwan (Republic of China)","NA","China","Brazil","India","Brazil, France, Germany, India, Indonesia, Mexico, Nigeria, Peru, South Africa and the United States","India","Pakistan","Germany, Greece, Ireland, Poland","United States","United States","United States","United States","NA","International (at least 50% United States)","Study 1: International English-speaking community; majority from Europe (59.3%) and North America (22.7%). Study 2: Germany, France, United Kingdom","International","United States","Global (but most studies were conducted in the United States)","Singapore","Brazil","Mozambique","India","South Africa","Kenya","United States","United States","United States","NA","United States","United States","South Korea","Japan","United States","United Kingdom","United States","Germany","Israel","Nigeria","United States","United States, India","India","Pakistan","United States","United States","United States","India","United States","United Kingdom","United States","Australia","United States","Australia","United States","United States","United States","United States","United States","United States","United States","Germany","United States"],"Sample demographics":["Quota-matched","Convenience; Quota-matched; Twitter users","Quota-matched","Quota-matched","Convenience; Quota-matched; Sample matched","Quota-matched","Convenience","Convenience","Convenience","University students","Early-intervention professionals working with preschool children with autism","Convenience; Quota-matched","Convenience","Convenience","Convenience","Quota-matched, with survey weights","Random sampling","Convenience","Convenience; quota sampling procedure to match the distribution of gender × age with that of the German population","Convenience","Varied","Quota-matched","Quota-matched.","Convenience","Convenience","Slum residents in the two largest urban agglomerations in UP, Lucknow, and Kanpur","Convenience, users of Baang social media platform","Convenience; quota sampling based on age, gender, and geographic  region","Convenience","Convenience","Convenience","Quota-matched","Convenience","Convenience","Convenience; Quota-matched","Convenience","Studies 1–6: Quota-matched; Study 7: Geographic, age, and online behavior quota","Adult participants","Adult participants","Convenience.","Convenience; stratified.","Social media users between 18-60 years of age, resident in state of UP, primarily an Android phone user, and using WhatsApp most frequently as their private messaging app.","Convenience","English-speaking Facebook users in Kenya who were at least 18\nyears old","Stanford University undergraduates, PhD historians, professional fact-checkers","Convenience","Convenience","Convenience","Convenience","Convenience","Convenience sample of adult participants","Convenience","Students enrolled in a required school government course in an urban district","Convenience","Convenience. Ninth grade students at a large suburban high school in the United States","Convenience.","Convenience sample from heterogeneous middle-schools","Convenience sample of undergraduate students from a Nigerian public university","Convenience; Professionals","Quota-matched; Convenience; Probability sample","Random, drawn from the city of Gaya in the state of Bihar","Cluster: low- and middle-income urban areas","Convenience","Convenience","Quota-matched","Rural and urban social media users","Quota-matched","Convenience: Parents of young children","Convenience","Convenience","Convenience","Convenience","Convenience","Convenience","Convenience","Convenience","Convenience","Convenience","Convenience","Convenience, majority students","Convenience"],"Recruitment":["Lucid","MTurk; Lucid; Twitter","Lucid","Respondi","MTurk; Lucid; YouGov","Lucid","Facebook advertisement","University campus; Local community","Zhubajie.com (China's counterpart to MTurk)","University campus","Professional organizations","University Campus; Panel Survey Company (Germany); MTurk","University campus","Prolific","A civil society organization's WhatsApp subscribers","Ipsos MORI in UK, South Africa, and Argentina. YouGov in Nigeria.","In person by trained enumerators","MTurk","Respondi","The Election Study Center of National Chengchi University","Varied","Chinese marketing research company","Representative sample of voters from the state of Minas Gerais..","Recruited via facebook ads","YouGov","Random sampling from census","Baang social media platform","Ipsos NV","MTurk","Cloud Research approved mTurkers","Study 2: MTurk","Qualtrics","Online; Press release","Prolific","Online; Prolific","Prolific","Studies 1–6: Prolific \n\nStudy 7: YouTube","Varied by study","A combination of convenience, snowballing and random sampling using a social media group called ‘SG Research Lobang’ on Telegram.","Quaest Consultoria & Pesquisa (online panel provider).","In-\ffield random sampling","Facebook ads","Facebook ads","Facebook ads","University campus; Professional organizations","University campus","High school","MTurk; Northwestern University’s subject pool","University campus","All students in two sections university course invited to participate.","Marketlink, a survey company in S.Korea","Study 1: online panel of a leading online survey\nStudy 2: leading online crowdsourcing platform","All schools in one urban district","Prolific","Large suburban high school","Snowball sampling by graduate and undergraduate students at a European university","Entire classes in the collaborating schools were invited to participate.","Voluntary recruitment at the university","MTurk; Professional organizations","YouGov; MTurk; Internet Research Bureau; Morsel (polling firm)","Random walk sampling","Random grid sampling","Prolific","Qualtrics","YouGov Pulse Panel","Urban: MTurk: Rural: contacts of a grassroots organization in Uttar Pradesh and snowball sampling","Lucid","Prolific","MTurk","University campus","Prolific","University campus","Qualtrics","MTurk","MTurk","MTurk","MTurk","MTurk","MTurk","Social media platforms, university mailing lists, and websites for psychological studies","MTurk"],"Main findings":["Accuracy prompt intervention significantly increased participants’ level of discernment between sharing true headlines about COVID-19 and sharing false headlines about COVID-19, mainly by increasing sharing intentions for true headlines.","Priming accuracy improved sharing discernment between false and true headlines in online experiments. In the field experiment, an accuracy message increased the average quality of the news sources shared.","Different accuracy prompts interventions increased sharing discernment by 3–6 percentage points. The media literacy tips intervention increased sharing discernment by 3 percentage points.","The first stage of the replication test was unsuccessful. After collecting a second round of data, the authors found a small but significant interaction between treatment condition and truth discernment.","Accuracy prompts increased the quality of news that people shared (sharing discernment) relative to control, primarily by reducing sharing intentions for false headlines by 10% relative to control.","Prompt condition increased sharing discernment relative to the baseline sharing condition. There was significant variation across countries in the magnitude of this effect.","There are null effects from fact-checking labels and related article suggestions, tactics used by social media platforms. However, tips to identify misinformation and nudges to consider information's accuracy reduced misinformation sharing.","Adults over the age of 65 were worse than younger adults at sustaining their postcorrection belief that myths were inaccurate. A greater level of explanatory detail promoted more sustained belief change. Fact affirmations promoted more sustained belief change than did myth retractions over 1 week (but not over 3 weeks).","In both studies, rumors reduced political trust. In study 1, where debunking was in the form of simple denials, debunking reduced the levels of belief in the rumors but did not improve political trust relative to the rumor condition. In study 2, where debunking was more detailed and/or from a government critic, debunking reduced the levels of belief in rumors and improved political trust relative to the rumor condition.","Communicating information transparently and addressing the public’s concerns, as opposed to common  information corrections, leads to increased trust and satisfaction in the Health Ministry, which are positively associated intentions to send child to kindergarten and about vaccinate child.","The benefits of optimized debunking were greater immediately after training, but this effect was not sustained at follow-up.","Not responding to science deniers had a negative effect on attitudes towards behaviors favored by science (e.g., vaccination) and intentions to perform these behaviors. Providing the facts about the topic or uncovering the rhetorical techniques typical for denialism had positive effects. There was no evidence that complex combinations of topic and technique rebuttals are more effective than single strategies, nor that rebutting science denialism in public discussions backfires. This was even the case for vulnerable groups (e.g., U.S. conservatives).","The study found that damage from science denialists could be mitigated through rebuttals from science advocates. Forewarnings were also an effective weight-of-evidence strategy to mitigate science denialism influence.The strategy of outnumbering science deniers had no success across 3 studies.","Corrections reduced misinformation inferences or beliefs regardless of format, both immediately and after a 2-day delay.","Treatment increased knowledge about COVID-19 and compliance of lockdown policies.","In all countries studied, fact-checks caused significant gains in factual accuracy. The observed accuracy increases attributable to fact-checks were durable, with most detectable more than 2 wk after initial exposure to the fact-check.","Both interventions substantially reduced belief in misinformation compared with the control group. Both interventions improved people’s knowledge and self-reported behaviour around typhoid risk reduction, and yielded self-reported increases in an important preventive method, drinking treated water. The intervention that  discussed misinformation and explained why it was incorrect was more effective.","Debunking worked as well as or better than prebunking, with little impact of misinformation type, thus reducing references to misinformation and increasing positive sentiment expressed in social media posts. There was no good evidence for an effect of debunking on a willingness-to-pay measure.","Participants who received the debunking text rated the false antivaccine news headline that mRNA vaccines alter the human genome as less credible compared to the control condition. This effect was not observed after 2 months delay. Results revealed a potential backfire effect of debunkings among individuals with high religiosity.","Rebuttal reduced credibility ratings of the rumor, particularly among supporters of the pro-unification opposition party, and affected political attitudes","Analysis of 44 independent comparisons showed that refutation text, is associated with a positive, moderate effect on learning compared to other learning conditions.","The believability of correction messages by government sources was significantly higher than that of professional, lay, and mass media sources. The believability of the correction message in a formal tone was higher than in a conversational tone. Yet this effect was moderated by topic and attitude congruence. \nParticipants who had a congruent attitude with the correction perceived the correction as more believable compared to participants who had an incongruent attitude with the correction.","Fact-checking corrections were  ineffective in reducing misinformation beliefs, both for nonpolitical and political rumors.","Exposure to any type of user-driven corrective messages significantly improves accuracy ratings of false claims.","Exposure to misinformation corrections increases belief accuracy by 0.16 on a 4-point scale, while exposure to misinformation decreases belief accuracy by 0.09 on the same scale. There is evidence that either misinformation or factual corrections affect the intent to vaccinate or attitudes toward vaccines.","The doctor's message significantly raises awareness and compliance with evidence-based COVID-19 prevention methods, such as wearing face masks, hand-washing, and physical distancing. However, it doesn't influence agreement with non-evidence-based methods, like relying on the Indian immune system or vegetarianism. Additionally, while the doctor's message reduces fact-checking, it doesn't affect trust in information from doctors or peers.","On average, both treatments reduce platform usage on extensive (number of users/day) and intensive margin (minutes/day). There is less dissemination of both official information and accurate user-generated information on COVID-19 in the treatments relative to the control condition. Exposure to misinformation is no different between control and sunshine treatment.","Debunking interventions significantly and substantially reduced agreement with the misleading article's claim, reduced credibility assessment of the misleading article and reduced participants' intentions to share the misleading article.","Explaining why a headline was true or false reduced participants’ intention to share false headlines, but had no effect on their intention to share true headlines. The effect of providing an explanation was larger when participants were seeing the headline for the first time. The intervention was less effective for headlines that had been seen previously in the experiment.","Explaining why headlines were true or false selectively reduced intentions to share false headlines. Encountering the explanation prompts during the first block did not reduce subsequent intentions to share false information when the explanation prompts were removed.","Communicating the scientific consensus on human-caused climate change significantly increased public perception of the expert consensus. The introduction of misinformation contesting the existence of a scientific consensus neutralized the positive effect of highlighting normative expert agreement. Preemptively warning people about politically motivated attempts to spread misinformation helped promote and protect (“inoculate”) public attitudes about the scientific consensus.","Studies 1 and 2, employing different styles of misinformation, both found that inoculation neutralized the negative influence of misinformation on perceived consensus about anthropogenic global warming.","The study provides preliminary evidence that active inoculation through the Bad News game significantly reduced the perceived reliability of tweets that embedded several common online misinformation strategies.","Participants who played the game found misinformation significantly less reliable after playing, were significantly more confident in their assessment, and were significantly less likely to report sharing misinformation, thereby supporting the effectiveness of Harmony Square as a tool to inoculate people against online manipulation.","The game reduced the perceived reliability of misinformation, increased people's confidence in their assessment of the reliability of misinformation, and reduced intentions to share misinformation with others.","Inoculation-based media and information literacy interventions such as the Bad News Game were found to confer protection against the influence of misinformation over time. With regular assessment, the positive effects could be maintained for at least 3 months. Without regular “boosting,” the effects dissipated within 2 months.","The videos were effective at conferring resilience against manipulation techniques commonly used in misinformation (improving technique discernment, confidence in identifying manipulative content, trustworthiness discernment, and sharing discernment). Findings were not moderated by outcome measure response order, nor did any covariates consistently interact with the main effect (including political ideology, bullshit receptivity, and education). On YouTube, the videos were effective at improving manipulation technique recognition.","Psychological inoculation reduces credibility of misinformation, improves assessments of real information and credibility discernment and sharing discernment. However, psychological inoculation does not significantly influence misinformation sharing intention.","Playing the game did not have a significant effect on people’s ability to identify fake news compared to the control group. Of the five deception strategies, participants were most vulnerable impersonation and least likely to fall for conspiracy-driven techniques. Socio-demographic factors did not influence the effects of the game. Questionnaire and focus group data revealed a high level of trust in abilities of  mainstream media, the government and public institutions to deal with fake news.","The findings show that preemptive interventions reduced rumor acceptance, but did not lead to more skepticism regarding true news stories.","The combination of all the three interventions effectively increases the acceptance of COVID-19 vaccines and improves the levels of trust in health institutions. While social memory and inoculating against fake news are particularly important in driving effects on vaccine acceptance, effects on trusting institutions seem to stem primarily from simple endorsements.","The intervention increases the ability to identify misinformation, however, there is also an increase in skepticism about true information.  Intervention results in more accurate factual beliefs, favorable policy attitudes, and higher donations in support of out-group.","Relatively small financial incentives generated substantially greater engagement with fact-checks during the intervention. Sustained exposure to fact-checks significantly increased demand for future fact-checks and helped inoculate citizens against misinformation upon exposure, increasing discernment of truth and false information and conspiracy theories, and verification knowledge. However, this had no effect on news consumption or verification behavior. The method of delivery also mattered, with short text messages and empathetic podcasts showing the most impact, and text messages even increasing government approval and compliance with COVID-19 policies.","All treatment courses reduce misinformation sharing and increase discernment relative to baselines, with the Emotions course being the most effective. Teaching reasoning-based techniques in addition to emotion-based techniques provides no added benefit. The treatment effects on discernment are smaller, partially because the treatment courses also decrease non-misinformation sharing.","Compared to the other groups, fact-checkers arrived at more warranted conclusions in a fraction of the time using lateral reading strategy. Undergraduates and professors stayed on an unfamiliar website to decide whether it was credible, whereas fact-checkers opened new tabs and verified the website’s credibility by checking other, trusted sites.","Students in the treatment group were over twice as likely to score higher at posttest than at pretest, while students in the control condition were equally likely to score higher at posttest than at pretest.","Students' scores improved significantly from pretest to posttest on 3 of the 4 tasks—investigating the source of a website, critiquing evidence, and locating reliable sources—during an open internet search.","The opportunity to search reduced inaccurate reproductions, particularly for topics about which participants were unlikely to possess sufficient background knowledge to validate independently.","At posttest, students in the treatment condition were more likely than students in the control classes to engage in lateral reading and to accurately assess the trustworthiness of online sources.","Students’ evaluation of online sources improved significantly after a series of course-embedded activities. Students employed the strategy of lateral reading more often on the posttest.","The game enhanced information discernment skills but not skepticism toward online information. The game was found to be effective regardless of the participant’s perceived level of intellectual civic skills, while the quiz with no game element was found to be effective only when the participant’s perceived level of intellectual civic skills was high.","By searching for reliable sources of information on the internet, those who are susceptible to false claims can revise their erroneous beliefs and adopt a more accurate view of reality.\nConfirmation bias does not affect the usefulness of the online search: regardless of the motivation of the search, beliefs about misinformation are reduced by performing an online search.","Less than 6 hours of classroom instruction based on lateral reading strategy significantly improved students’\njudgment about the credibility of online sources.","Monetary incentives were overall effective in increasing accuracy. The pop-up worked when the source of information was unknown. Pop-up and incentives, when used together, produced a cumulative effect on accuracy.","Pretest/posttest data showed statistically significant growth in students’ ability to evaluate the credibility of online content.","The study found significant effects of the training type and marginal effects of the trainer education, but no significant differences between cognitive apprenticeship with written instructions and cognitive apprenticeship with human trainers. The training effects pertained mainly to participants’ fake news identification ability, whereas their truthful news evaluation was changed little. In sum, participants’ discernment of truthful vs. fake news was improved. Consequently, lateral reading training based on written instructions appears to be an effective and scalable media education intervention.","Playing the misinformation game had a positive effect on accuracy discernment, sharing discernment, and metastrategic knowledge about corroboration. In Study 1, the effects on discernment scores were mainly due to higher ratings of accurate messages; whereas in Study 2, these effects were mainly due to lower ratings of inaccurate messages. In both studies, accuracy discernment mediated the effect of playing the misinformation game on sharing discernment. In Study 2, the misinformation game had an additional direct effect on sharing discernment. There were no effects on self-reported stances regarding sharing misinformation.","Following the intervention, participants in the treatment group were better able to recognize false news, were less likely to express intent to share false news, and self-reported that they had greater knowledge of social media and of how to verify information on social media, compared to the control group participants.","Requirement to seek out or consider external evidence in 2 treatment conditions improved misinformation judgments. Judgments in treatment conditions were also more internally consistent between raters, showed less partisan divide, and were better correlated with expert journalist judgments. When averaging ratings from several raters, the correlation with a journalist was higher in the individual research condition.","The intervention improved discernment between mainstream and false news headlines among both a nationally representative sample in the United States (by 26.5%) and a highly educated online sample in India (by 17.5%). This increase in discernment remained measurable several weeks later in the United States, but not in India. However, no effects were found among a representative sample of respondents in a largely rural area of northern India, where rates of social media use are far lower.","The intervention had no effect on average. However, it improved misinformation identification skills for one set of respondents (non-BJP respondents) and had a negative effect on BJP partisans.","The study did not find a significant effect of video-based general educational messages about misinformation. However, when such messages were augmented with personalized feedback based on individuals' past engagement with fake news, there was an improvement of 3.3 percentage points (4.5%) in accurately identifying any type of news and 5.6 percentage points (7.9%) in accurately identifying fake news","The study found that the active intervention, where participants were taught and practiced reverse image search, did not significantly increase the perceived credibility of accurately attributed visual posts compared to passive intervention and control groups. Similarly, it did not significantly affect the perceived credibility of misattributed visual posts. However, it was observed that the active intervention led to a higher intention to use reverse image search in the future and participants in this group spent more time evaluating the posts, indicating a potential positive effect on engagement with the task","Source ratings affected believability (and to a lesser extent liking) of news headlines; however, these effects largely did not replicate in a second experiment, with only the stronger effects of a low expert or user-article-based rating reducing believability.","The study found no significant effect of the source reliability labels on the quality of news consumption of online users. Even after the intervention, the news consumption patterns did not change significantly. However, a slight effect was observed among those who consume the most low-quality news.","Source effects have a significant impact on participants' perceived trust rating and sharing attitude towards Facebook posts. Rural participants tended to trust posts from journalists the most, while urban residents exhibited more trust in their personal contacts. The study also found that participants were more likely to share posts from credible sources than fake sources.","Sharing discernment was increased by both fact-checker and layperson ratings, via an effect on sharing of false headlines. Effect size was larger for the fact-checker than the layperson ratings.","Compared to an assessment-only control, UK parents of young children who were exposed to the normative feedback intervention showed reduced belief in anti-vaccine conspiracy theories at immediate follow-up. Moreover, mediation analysis showed that the intervention reduced perceived norms of anti-vaccine conspiracy beliefs, which in turn reduced personal beliefs.","Participants exposed to a social norm-based message with both descriptive and injunctive elements were 5 percentage points less likely to say that they were willing to share a false news article with their social network.","Experiment 1 found that using a single-point estimate to communicate a norm affected belief but had less impact than a refutation. Experiment 2 used a verbally presented distribution of 4 values to communicate a norm, which was largely ineffective. Experiment 3 used a graphically presented social norm with 25 values, which was found to be as effective at reducing claim belief as a refutation, with the combination of both interventions being most impactful.","The social-norm intervention led to reduced belief in false claims and improved discrimination between true and false claims. It also had some positive impact on social-media engagement, although some effects were not robust to alternative analysis specifications","A specific warning succeeded in reducing the continued reliance on outdated information but did not eliminate it. A more general warning was even less effective. A combination of specific warning and a plausible alternative explanation for the retracted information further reduced the continued influence effect but did not eliminate it altogether.","Ratings affected believability, but only when sources were rated low: Articles from low-rated sources—when the ratings were from experts or users evaluating articles—were less believable, but articles from high-rated sources were not more believable. Believability and confirmation bias both affected user actions.","Respondents who saw a simulated Facebook post with a warning label had lower intentions to share the deceiving information on their timeline than respondents who did not see such label","False headlines were perceived as less accurate when participants received a general warning about misleading information on social media or when specific headlines were accompanied by a “Disputed” or “Rated false” tag. Though the magnitudes of these effects were relatively modest, they generally did not vary by whether headlines were congenial to participants’ political views. Adding a “Rated false” tag to an article headline lowered its perceived accuracy more than adding a “Disputed” tag (Facebook’s original approach) relative to a control condition.","Attaching warning labels to false news headlines had small to moderate effects on accuracy ratings and sharing intentions (no partisanship effect). An unintended consequence of warning labels, the implied truth effect, emerged:Untagged headlines (even if false) were seen as more accurate and were given more consideration for sharing on social media.","Refutations were not more effective than plain retractions at reducing claim belief after a 1-day retention interval. However, refutations were more effective than plain false-tag retractions at reducing claim belief after a 1-week retention interval. Short-format refutations were found to be more effective than simple retractions after a 1-week delay but not a one-day delay.","Presenting the warning before a false headline was effective initially, though it was not significantly better in most areas than the label under the headline. Two weeks later, however, across conditions, people once again believed items they once knew were false, especially when those items supported their political views.","Fact-checking labels attached to conspiratorial misinformation posts significantly reduced MMR vaccine conspiracy beliefs. However, the fact-checking labels did not have a significant effect on participants' intentions to engage in misinformation about the MMR vaccine or their vaccination intentions.","Warning labels reduced the perceived credibility of fake news posts, but social endorsement cues had no such effect on credibility perceptions.","Those who read an article from their supporting party reported greater perceived credibility of the article than those who read the article from their opponent party."],"Longevity":["Not measured","Not measured, but study 7 had a test window of 24 hours","Not measured","Not measured","Not measured","Not measured","Not measured","Intervention still effective after one month, but belief regression was evident.","Not measured","Not measured","Yes, 6 weeks; effects still present in pre-post comparison, but no longer an advantage of the optimized debunking vs. the treatment-as-usual debunking","Not measured","Not measured","Yes, 2-day delay; intervention still effective","Not measured","Fact-checking effects still detectable after 2 weeks.","Not measured","Not measured","Intervention not more effective than control after 2 months","Not measured","The effectiveness of refutation text did not significantly differ depending on when the outcome test occurred","Not measured","Not measured","Not measured","They recontacted participants two weeks later, and observed 39% of the initial accuracy increase (p = 0.06).","Yes, but results do not differentiate between waves","No measured","Not measured","Not measured","Effect disappeared when prompt was removed (within the same study).","Not measured","Not measured","Not measured","Not measured","1-week follow-up with new items. Main effect for manipulativeness and confidence were still present for the Go Viral game. Effect for sharing no longer significant. Prebunking infographics no longer significant after 1 week.","Study 1 and 3: Follow-ups at several time intervals (from 1 week to up tor 13 weeks): intervention effect was still significant. Study 2: inoculation effect decays and is no longer significant after 2 months with no \"boosters\" in between.","For study 7, median time between seeing the YouTube ad and answering the survey question was 18.4 hours.","Inoculation effects for misinformation credibility assessment remain significant over time, but decrease after two weeks.","The test was readministered a few days after the treatment. The intervention had no significant effects.","Not measured","Not measured","Not measured","Six months of exposure to fact-checks","88% of the treatment effect persists 2 months post treatment","Not measured","Yes. Posttest was administered 5 weeks after completion of the intervention.","Yes. Posttest was administered two weeks after the final treatment lesson.","Not measured","Not measured","Not measured","Not measured","Not measured","Yes. The intervention was conducted mid semester and posttest was administered at the end of the semester.","Not measured","Longevity not measured directly, but students completed the posttest weeks after completing the final lesson of the intervention.","Not measured.","Not measured","Not measured","Not measured","Lasts up to 3 weeks in the U.S. (but decays by about half), does not last in India.","Outcomes were collected only ~2 weeks later","Yes, similar effect size (but less precisely estimated) 4-6 weeks later","No measured","No measured","Yes","Not measured","Not measured","6-week delay, no effect remained","Not measured","Not measured","Not measured","Not measured","Not measured","Not measured","Not measured","Not measured","Yes, 1 week","Yes, 2 weeks, no effect remained","Not measured","Not measured","Not measured"]},"columns":[{"id":".details","name":"Details","type":null,"minWidth":80,"sortable":false,"resizable":false,"filterable":false,"searchable":false,"width":75,"align":"center","cell":[{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]},{"name":"button","attribs":{"className":"button2"},"children":["Expand"]}],"details":[{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Accuracy prompts"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Pennycook, G., McPhetres, J. Zhang, Y., Lu, J. G. & Rand, D. G. (2020). Fighting COVID-19 misinformation on social media: Experimental evidence for a scalable accuracy-nudge intervention. Psychological Science, 31(7), 770–780."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/0956797620939054"},"children":["https://doi.org/10.1177/0956797620939054"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/7d3xh/"},"children":["https://osf.io/7d3xh/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Across two studies with more than 1,700 U.S. adults recruited online, we present evidence that people share false claims about COVID-19 partly because they simply fail to think sufficiently about whether or not the content is accurate when deciding what to share. In Study 1, participants were far worse at discerning between true and false content when deciding what they would share on social media relative to when they were asked directly about accuracy. Furthermore, greater cognitive reflection and science knowledge were associated with stronger discernment. In Study 2, we found that a simple accuracy reminder at the beginning of the study (i.e., judging the accuracy of a non-COVID-19-related headline) nearly tripled the level of truth discernment in participants’ subsequent sharing intentions. Our results, which mirror those found previously for political fake news, suggest that nudging people to think about accuracy is a simple way to improve choices about what to share on social media.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Presented true and false COVID-related headlines and asked participants whether they would be willing to share the headline on social media. Participants were randomly assigned to either a control condition (news-sharing task) or a treatment condition (rating the accuracy of 1 of 4 headlines—all politically neutral and unrelated to COVID-19—before beginning the news-sharing task).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 2: False (source: snopes.com, factcheck,org) and true (source: Harvard GHI) news headlines relating to COVID-19, the nudge in the form of politically neutral non-Covid headlines",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"6-item cognitive reflection test, general science knowledge quiz, Medical Maximizer-Minimizer scale, political ideology on both social and fiscal issues, and Democrat versus Republican Party alignment",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The paper includes 2 studies, but only study 2 was testing the accuracy prompt intervention. Study 1 (not included here) tested for a dissociation between accuracy judgments and sharing intentions when participants evaluated a set of true and false news headlines about COVID-19.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["2"],"Description":["Study 2 experimentally tested whether subtly making the concept of accuracy salient increased the quality of COVID-19 information that people were willing to share online."],"N":[856],"Effect size":["Cohen’s d: 0.14 (95%-CI: 0.05-0.23)"],"Comments":["In the treatment condition, sharing intentions for true headlines were significantly higher than for false headlines."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"dfeb44300fdc46fbc551f576f7d15194","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Accuracy prompts"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Pennycook, G., Epstein, Z., Mosleh, M., Arechar, A. A., Eckles, D., & Rand, D. G. (2021). Shifting attention to accuracy can reduce misinformation online. Nature, 592(7855), 590–595."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1038/s41586-021-03344-2"},"children":["https://doi.org/10.1038/s41586-021-03344-2"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/p6u8k/"},"children":["https://osf.io/p6u8k/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"In recent years, there has been a great deal of concern about the proliferation of false and misleading news on social media. Academics and practitioners alike have asked why people share such misinformation, and sought solutions to reduce the sharing of misinformation. Here, we attempt to address both of these questions. First, we find that the veracity of headlines has little effect on sharing intentions, despite having a large effect on judgments of accuracy. This dissociation suggests that sharing does not necessarily indicate belief. Nonetheless, most participants say it is important to share only accurate news. To shed light on this apparent contradiction, we carried out four survey experiments and a field experiment on Twitter; the results show that subtly shifting attention to accuracy increases the quality of news that people subsequently share. Together with additional computational analyses, these findings indicate that people often share misinformation because their attention is focused on factors other than accuracy—and therefore they fail to implement a strongly held preference for accurate sharing. Our results challenge the popular claim that people value partisanship over accuracy, and provide evidence for scalable attention-based interventions that social media platforms could easily implement to counter misinformation online.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Presented true and false COVID-related headlines and asked participants whether they would be willing to share the headline on social media. In Studies 3 and 4, participants were randomly assigned to a control condition (news-sharing task) or a treatment condition (rating the accuracy of a single headline—unrelated to politics—before beginning the news-sharing task). Study 4 also included two other conditions: active control (rating the funniness of a headline—unrelated to politics—before beginning the news-sharing task) and importance (participants were asked whether they think it's important to only share accurate content before they began the news-sharing task). In Study 7, a digital field experiment was carried out on Twitter where \"bots\" were used to follow people who had retweeted dubious content. Users who followed back the bots were sent a message asking about accuracy (for different users at different times, maintaining experimental control). Tweet history (news quality ratings) was monitored for people who had received the message in the previous 24 hours and those who had not received it yet.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Pro-democratic or pro-Republican false (source: snopes.com) and true (source: mainstream news outlets) news headlines, lede sentence, and image, the nudge in the form of politically neutral headlines.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"CRT, political-knowledge questionnaire, PANAS, political partisanship, Importance of sharing only accurate news on social media, the positive and negative affective schedule",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The paper includes 7 studies, of which 5 were testing accuracy prompt interventions. Study 1 and 2 (not included here) tested for a dissociation between accuracy judgments and sharing intentions when participants evaluated a set of true and false news headlines.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["3","4","5","5 (Importance treatment)","7"],"Description":["Treatment condition's impact on sharing discernment.","Treatment condition's impact on sharing discernment.","Treatment's impact on sharing discernment relative to controls.","Importance treatment's impact on sharing discernment relative to controls.","Accuracy message's impact on the average quality of news sources shared."],"N":[727,780,671,671,5379],"Effect size":["Beta coefficient: 0.05 (95%-CI: 0.03-0.07)","Beta coefficient: 0.06 (95%-CI: 0.04-0.09)","Beta coefficient: 0.05 (95%-CI: 0.02-0.09)","Beta coefficient: 0.04 (95%-CI: 0.01-0.06)","Beta coefficient: 0.01 (95%-CI: Not specified-Not specified)"],"Comments":["Treatment condition significantly increased sharing discernment.","Treatment condition significantly increased sharing discernment.","Treatments significantly increased sharing discernment relative to the controls.","Importance treatment significantly increased sharing discernment relative to the controls.","Accuracy message increased the average quality of the news sources shared."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"c0535bb1cbb825131a0a3487b155a3af","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Multiple interventions: Accuracy prompts, Media-literacy tips"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Epstein, Z, Berinsky, A. J., Cole, R., Gully, A., Pennycook, G., & Rand, D. G. (2021). Developing an accuracy-prompt toolkit to reduce COVID-19 misinformation online. Harvard Kennedy School Misinformation Review, 2(3)."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.37016/mr-2020-71"},"children":["https://doi.org/10.37016/mr-2020-71"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/hu4k2/"},"children":["https://osf.io/hu4k2/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Recent research suggests that shifting users’ attention to accuracy increases the quality of news they subsequently share online. Here we help develop this initial observation into a suite of deployable interventions for practitioners. We ask (i) how prior results generalize to other approaches for prompting users to consider accuracy, and (ii) for whom these prompts are more versus less effective. In a large survey experiment examining participants’ intentions to share true and false headlines about COVID-19, we identify a variety of different accuracy prompts that su­ccessfully increase sharing discernment across a wide range of demographic subgroups while maintaining user autonomy.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"To assess the impact of 8 experimental treatments on sharing intentions for true and false headlines, each treatment was administered prior to a news-sharing task. Participants then saw true and false COVID-related headlines and were asked about their sharing intentions and accuracy judgments. Of the 8 treatments, 3 were accuracy prompts. In the evaluation treatment, as in Pennycook et al. (2021) and Pennycook, McPhetres, et al. (2020), participants evaluated the accuracy of a non-COVID-related headline—thereby priming the concept of accuracy when the participants continued on to the sharing task. In the long evaluation treatment, participants evaluated the accuracy of 8 non-COVID-related headlines (half true, half false). After each headline, they learned whether their answer was correct or incorrect and whether the headline had been a real news headline or a fake news headline. In the importance treatment, as in Pennycook et al. (2021), participants were asked “How important is it to you that you share only news articles on social media (such as Facebook and Twitter) if they are accurate?” Facebook’s “Tips to Spot False News” were used as a treatment as well.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"False and true news cards (e.g., the combination of a headline, an image, and a source) relating to COVID-19",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Level of concern about COVID-19, the extent to which they had been following COVID-19–related news, CRT, importance of sharing only accurate news",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Several interventions were comparatively tested in this study. However, not all of them were accuracy prompts. Only the accuracy prompts interventions are included here: Evaluation, Long Evaluation, Importance.  Other tested  interventions: media literacy tips, generic norms, partisan norms, tips + norms, importance + norms. Tips and Norms are included in respective sections on Media Literacy Tips and Social Norms.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["Evaluation","Long Evaluation","Importance","Tips"],"Description":["Asking participants to judge the accuracy of a non-COVID-19 related headline","Asking participants to judge the accuracy of a series of 4 non-COVID-19–related headlines (and providing corrective feedback on their responses)","Asking participants how important it was to them to share only accurate news","Participants were provided with four simple digital literacy tips, taken from an intervention developed by Facebook"],"N":[935,410,1046,906],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["Intervention increased sharing discernment by roughly 50% (3 percentage points)","Intervention increased sharing discernment by roughly 100% (6 percentage points)","Intervention increased sharing discernment by roughly 50% (3 percentage points)","Intervention increased sharing discernment by roughly 50% (3 percentage points)"]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"f0a65b8db2dfa37dd2ba8f2d58f7a959","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Accuracy prompts"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Roozenbeek, J.,  Freeman, A. L. J., & van der Linden, S. (2021). How accurate are accuracy-nudge interventions? A preregistered direct replication of Pennycook et al. (2020). Psychological Science, 32(7), 1169–1178."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/09567976211024535"},"children":["https://doi.org/10.1177/09567976211024535"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/rkfq5/"},"children":["https://osf.io/rkfq5/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"As part of the Systematizing Confidence in Open Research and Evidence (SCORE) program, the present study consisted of a two-stage replication test of a central finding by Pennycook et al. (2020), namely that asking people to think about the accuracy of a single headline improves “truth discernment” of intentions to share news headlines about COVID-19. The first stage of the replication test (n = 701) was unsuccessful (p = .67). After collecting a second round of data (additional n = 882, pooled N = 1,583), we found a small but significant interaction between treatment condition and truth discernment (uncorrected p = .017; treatment: d = 0.14, control: d = 0.10). As in the target study, perceived headline accuracy correlated with treatment impact, so that treatment-group participants were less willing to share headlines that were perceived as less accurate. We discuss potential explanations for these findings and an unreported change in the hypothesis (but not the analysis plan) from the preregistration in the original study.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Presented true and false COVID-related headlines and asked participants whether they would be willing to share the headline on social media. Participants were randomly assigned to a treatment or a control group. Participants in both groups were shown 15 real and 15 false headlines related to COVID-19 and asked whether they would be likely to share it on social media (response options ranged from extremely unlikely to extremely likely). In the treatment group, participants first saw a headline unrelated to COVID-19 and were asked about its accuracy (yes/no response options).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"False (source: snopes.com, factcheck,org) and true (source: Harvard GHI) news headlines relating to COVID-19, format of Facebook posts, including an image, a headline, and a lede sentence, the nudge in the form of politically neutral non-Covid headlines",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"type of social media accounts, 6-item CRT, general science knowledge quiz, Medical Maximizer-Minimizer scale, political ideology on both social and fiscal issues, and Democrat versus Republican Party alignment",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study 1 attempted to replicate the finding that subtly making the concept of accuracy salient increased the quality of COVID-19 information that people were willing to share online.","Study 2 recruited additional 882 participants to attempt replication of the finding that subtly making the concept of accuracy salient increased the quality of COVID-19 information that people were willing to share online."],"N":[701,1583],"Effect size":["(no standard effect size available/yet extracted)","Cohen's d: 0.14 (95%-CI: 0.12-0.17)"],"Comments":["No significant interaction between headline veracity and treatment, β = 0.0046, 95% confidence interval (CI) = [−0.016, 0.026], F(3, 21030) = 1.53, p = .67","Significant interaction effect between headline veracity and treatment, β = 0.015, 95% CI = [0.0027, 0.027], F(3, 47490) = 4.52, treatment-group effect size: d = −0.14, 95% CI = [−0.17, −0.12]. The effect size for the control group was directionally similar (d = −0.10, 95% CI = [−0.13, −0.078]), but sharing discernment was still 1.4 times higher in the treatment group than in the control group, an attenuation of about 50% compared with the effect sizes reported in the target study."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"2b724831aa8a98ea3e1a63797d124060","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Accuracy prompts"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Pennycook, G., & Rand, D.G (2022). Accuracy prompts are a replicable and generalizable approach for reducing the spread of misinformation. Nature Communications, 13(1), Article 2333."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1038/s41467-022-30073-5"},"children":["https://doi.org/10.1038/s41467-022-30073-5"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/4mv9z"},"children":["https://osf.io/4mv9z"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Interventions that shift users attention toward the concept of accuracy represent a promising approach for reducing misinformation sharing online. We assess the replicability and generalizability of this accuracy prompt effect by meta-analyzing 20 experiments (with a total N = 26,863) completed by our group between 2017 and 2020. This internal meta-analysis includes all relevant studies regardless of outcome and uses identical analyses across all studies. Overall, accuracy prompts increased the quality of news that people share (sharing discernment) relative to control, primarily by reducing sharing intentions for false headlines by 10% relative to control in these studies. The magnitude of the effect did not significantly differ by content of headlines (politics compared with COVID-19 related news) and did not significantly decay over successive trials. The effect was not robustly moderated by gender, race, political ideology, education, or value explicitly placed on accuracy, but was significantly larger for older, more reflective, and more attentive participants. This internal meta-analysis demonstrates the replicability and generalizability of the accuracy prompt effect on sharing discernment.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In this internal meta-analysis of 20 experiments of accuracy prompt experiments conducted between 2017 and 2020, internal lab records were searched to identify raw data for studies satisfying the inclusion and exclusion criteria. Key dimensions of variation across included studies: the subject pool from which the participants were recruited, the topic of the headlines about which the participants made sharing decisions, the specific set of headlines shown, and the particular set of accuracy prompts employed. The meta-analysis examined how the effect of the accuracy prompts varied across these dimensions; it also examined how various individual-level variables moderated the effect of the accuracy prompts.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"False (source: snopes.com, factcheck.org) and true (mainstream news sources) news headlines in the format of a Facebook post",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Political orientation, preference for the Democratic versus Republican party, voting for Donald Trump in the 2016 U.S. Presidential Election, CRT, importance that participants self-reported placing on only sharing accurate news on social media",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Internal meta-analysis of 20 studies conducted between 2017 and 2020 aimed to gauge an effect of the various accuracy prompts on sharing discernement across all experiments."],"N":[26863],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Accuracy prompts significantly increased sharing discernment (interaction between headline veracity and treatment dummies: b = 0.038, z = 7.102, p < 0.001), which translates into a 71.7% increase over the meta-analytic estimate of baseline sharing discernment in the control condition (headline veracity dummy: b = 0.053, z = 6.636, p < 0.001). This increase in discernment was driven by accuracy prompts significantly decreasing sharing intentions for false news (treatment dummy: b = −0.034, z = 7.851, p < 0.001; Fig. 2), which translates into a 10% decrease relative to the meta-analytic estimate of baseline sharing intentions for false news in the control condition (intercept: b = 0.341, z = 15.695, p < 0.001)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"0eeb7ce0560918fe2ee9dfe9d1b390e6","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Multiple interventions: Accuracy prompts & Media-literacy tips"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Arechar, A. A., Allen, J. N. L., Berinsky, A. J., Cole, R., Epstein, Z., Garimella, K., Gully, A., Lu, J. G., Moss, R. M., Stagnaro, M. N., Zhang, Y., Pennycook, G., & Rand, D. G. (2023). Understanding and combating misinformation across 16 countries on six continents. Nature Human Behaviour."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1038/s41562-023-01641-6"},"children":["https://doi.org/10.1038/s41562-023-01641-6"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/g65qu"},"children":["https://osf.io/g65qu"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The spread of misinformation online is a global problem that requires global solutions. To that end, we conducted an experiment in 16 countries across 6 continents (N = 34,286; 676,605 observations) to investigate predictors of susceptibility to misinformation about COVID-19, and interventions to combat the spread of this misinformation. In every country, participants with a more analytic cognitive style and stronger accuracy-related motivations were better at discerning truth from falsehood; valuing democracy was also associated with greater truth discernment, whereas endorsement of individual responsibility over government support was negatively associated with truth discernment in most countries. Subtly prompting people to think about accuracy had a generally positive effect on the veracity of news that people were willing to share across countries, as did minimal digital literacy tips. Finally, aggregating the ratings of our non-expert participants was able to differentiate true from false headlines with high accuracy in all countries via the ‘wisdom of crowds’. The consistent patterns we observe suggest that the psychological factors underlying the misinformation challenge are similar across different regional settings, and that similar solutions may be broadly effective.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Presented 20 COVID-related headlines (10 true, 10 false) and asked participants whether they would be willing to share the headline on social media. Participants were randomly assigned to 1 of 4 experimental conditions: accuracy, sharing, prompt, or tips. Sharing, prompt, and tips: Participants were asked about their likelihood of sharing such content on social media. Accuracy: Participants rated the level of accuracy.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"10 true and 10 false news headlines, randomly sampled from a larger set of 45 headlines (of which 30 were false).",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"self-reported preference for analytic thinking, CRT",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Participants were randomly assigned to one of the four conditions: Accuracy, Sharing, Prompt, and Tips. Here we include only results for the Prompt condition. But sharing discernment was also higher in the Tips condition compared to the baseline Sharing condition (meta-analytic estimate, b=0.076, z=4.30, p<0.001), and the magnitude of this effect did not significantly vary across countries (χ2=14.54, p=0.485).",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1 across 16 countries"],"Description":["Study conducted in 16 countries across 6 continents experimentally tested whether subtly shifting attention to accuracy increases the veracity of the news people are willing to share."],"N":[33480],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Prompt condition increased sharing discernment relative to the baseline Sharing condition (meta-analytic estimate, b=0.171, z=4.61, p<0.001). There was significant variation across countries in the magnitude of this effect (χ2=58.57, p<0.001). sharing discernment was also higher in the Tips condition compared to the baseline Sharing condition (meta-analytic estimate, b=0.076, z=4.30, p<0.001)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"2d3e3e6b6a5958b1e5e6916c915e5ab5","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Multiple interventions: Accuracy prompts & Media-literacy tips & Fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Offer-Westort, M., Rosenzweig, L. R., & Athey, S. (2022). Battling the Coronavirus Infodemic Among Social Media Users in Africa. ArXiv. (Preprint, not peer-reviewed)"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.48550/arXiv.2212.13638"},"children":["https://doi.org/10.48550/arXiv.2212.13638"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://github.com/gsbDBI/infodemic-replication"},"children":["https://github.com/gsbDBI/infodemic-replication"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"How can we induce social media users to be discerning when sharing information during a pandemic? An experiment on Facebook Messenger with users from Kenya and Nigeria tested interventions designed to decrease intentions to share COVID-19 misinformation without decreasing intentions to share factual posts. The initial stage of the study incorporated: (i) a factorial design with 40 intervention combinations; and (ii) a contextual adaptive design, increasing the probability of assignment to treatments that worked better for previous subjects with similar characteristics. The second stage evaluated the best-performing treatments and a targeted treatment assignment policy estimated from the data. We precisely estimate null effects from warning flags and related article suggestions, tactics used by social media platforms. However, tips to identify misinformation and nudges to consider information's accuracy reduced misinformation sharing by 4.2% and 4.9% respectively. Such low-cost scalable interventions may improve the quality of information circulating online.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Recruited individuals interacted with a Facebook Messenger chatbot to answer survey questions and receive randomized treatments. The experiment had two stages: the learning stage and the evaluation stage. The first (learning) stage implemented a design to learn the best-performing interventions from 40 factorial combinations of treatments. The second (evaluation) stage estimated treatment effects for the most effective interventions identified during the learning stage, comparing them against each other and to control. Each user’s treatment consisted of two independently randomized factors: (i) (headline- level) treatments delivered on specific posts shown to treated users, such as flags or warning labels pinned on the article of interest; (ii) (respondent-level) messaging treatments delivered to treated users, such as tips for spotting fake news, training videos, and nudges.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Participants were shown four media posts (two true and two false in random order) about COVID-19 prevention and treatment drawn from a stimuli set. After viewing each stimulus, users were first asked whether they wanted to share the post (publicly) on their Facebook Timeline, and then asked whether they wanted to share it (privately) through Messenger.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, digital literacy, desire to share posts.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["Evaluation stage: Fact-checks and related articles","Evaluation stage: Accuracy nudge","Evaluation stage: Facebook tips"],"Description":["Headline-level treatment: Fact-checking labels and related corrective articles","Respondent-level treatment: Accuracy nudge","Respondents-level treatment: 5 Facebook tips"],"N":[10531,10531,10531],"Effect size":["generalized augmented inverse probability weighted estimator (expressed in percentage points): -0,031 (fact-checks); -0,052 (related articles)","generalized augmented inverse probability weighted estimator (expressed in percentage points): 0.07 (95%-CI: 0-0.13)","generalized augmented inverse probability weighted estimator (expressed in percentage points): 0.05 (95%-CI: -0.02-0.12)"],"Comments":["The two headline-level treatments are not effective at improving sharing discernment. The fact check treatment is associated with a decrease of 0.6 pp (s.e. = 1.1, Z = −0.51, p = 0.61, 95% CI = [−2.78, 1.63]) in false sharing intentions as compared to control. The related articles treatment increases intention to share false stimuli as compared to control, although this estimate is not statistically distinguishable from zero at conventional significance levels (estimate = 0.8 pp, s.e. = 1.1, Z = 0.71, p = 0.48, 95% CI = [−1.39, 2.98]).","The respondent-level treatments are effective. The accuracy nudge and Facebook tips increase sharing discernment by 0.066 (s.e. = 0.032, Z = 2.074, p = 0.04, 95% CI = [0.004, 0.129]) and 0.054 (s.e. = 0.036, Z = 1.501, p = 0.13, 95% CI = [−0.016, 0.123]) relative to control, respectively. These effects are driven by decreases in false sharing of 2.3 pp (s.e. = 1.0, Z = −2.31, p = 0.02, 95% CI = [−4.2, −0.35]) for the accuracy nudge and 2.0 pp (s.e. = 1.1, Z = −1.82, p = 0.07, 95% CI = [−4.11, 0.16]) for Facebook tips, equivalent to 4.9% and 4.2% reductions in false sharing relative to control. Effects on true sharing are not distinguishable from zero at conventional significance levels for either treatment.","The respondent-level treatments are effective. The accuracy nudge and Facebook tips increase sharing discernment by 0.066 (s.e. = 0.032, Z = 2.074, p = 0.04, 95% CI = [0.004, 0.129]) and 0.054 (s.e. = 0.036, Z = 1.501, p = 0.13, 95% CI = [−0.016, 0.123]) relative to control, respectively. These effects are driven by decreases in false sharing of 2.3 pp (s.e. = 1.0, Z = −2.31, p = 0.02, 95% CI = [−4.2, −0.35]) for the accuracy nudge and 2.0 pp (s.e. = 1.1, Z = −1.82, p = 0.07, 95% CI = [−4.11, 0.16]) for Facebook tips, equivalent to 4.9% and 4.2% reductions in false sharing relative to control. Effects on true sharing are not distinguishable from zero at conventional significance levels for either treatment."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"3d687fc5471d8609e48b755db3ed9a40","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Swire, B., Ecker, U. K. H., & Lewandowsky, S. (2017). The role of familiarity in correcting inaccurate information. Journal of Experimental Psychology: Learning, Memory, and Cognition, 43(12), 1948–1961."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1037/xlm0000422"},"children":["https://doi.org/10.1037/xlm0000422"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"People frequently continue to use inaccurate information in their reasoning even after a credible retraction has been presented. This phenomenon is often referred to as the continued influence effect of misinformation. The repetition of the original misconception within a retraction could contribute to this phenomenon, as it could inadvertently make the “myth” more familiar—and familiar information is more likely to be accepted as true. From a dual-process perspective, familiarity-based acceptance of myths is most likely to occur in the absence of strategic memory processes. Thus, we examined factors known to affect whether strategic memory processes can be utilized: age, detail, and time. Participants rated their belief in various statements of unclear veracity, and facts were subsequently affirmed and myths were retracted. Participants then rerated their belief either immediately or after a delay. We compared groups of young and older participants, and we manipulated the amount of detail presented in the affirmative or corrective explanations, as well as the retention interval between encoding and a retrieval attempt. We found that (a) older adults over the age of 65 were worse at sustaining their postcorrection belief that myths were inaccurate, (b) a greater level of explanatory detail promoted more sustained belief change, and (c) fact affirmations promoted more sustained belief change in comparison with myth retractions over the course of 1 week (but not over 3 weeks), This supports the notion that familiarity is indeed a driver of continued influence effects.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants (in one study, aged 18–30; in a second study, aged 50+) rated statements both before and after misinformation corrections and factual affirmations. Corrections and affirmations were presented briefly (with no explanation) or in some detail (with an explanation of why the claims were true or false). Participants re-rated their belief immediately afterward, 30 minutes later, one week later, or three weeks later. Experiments used a 2 × 2 × 3 within–between design, with within-subjects factors type of item (myth, fact) and type of explanation (the veracity of each statement was explained either briefly or in some detail), and the between-subjects factor retention interval (immediate, 30 minutes, 1 week).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"40 misinformation and factual statements about different topics",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Direct belief ratings and inference questions",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study 1 experimentally tested effects of corrections on people’s belief in true and false statements. The experiment used a 2 × 2 × 3 within-between design, with within-subjects factors type of item (myth vs. fact) and type of explanation (the veracity of each statement was explained either briefly or in some detail), and the between-subjects factor retention interval (immediate, 30-min, or 1-week).","Experiment 2 was a conceptual replication of Experiment 1 but tested older adults. An additional 3-week retention interval condition was also added."],"N":[93,109],"Effect size":["Partial-Eta squared (η2p): 0.15 (Brief/detailed explanation)","Partial-Eta squared (η2p): 0.12 (Brief/detailed explanation)"],"Comments":["Belief rating η2p = .15 (Brief/detailed explanation); .10 (retention interval); Inference question η2p = .16 (Brief/detailed explanation); .12 (retention interval)","Belief rating η2p = .12 (Brief/detailed explanation); .25 (retention interval); Inference question η2p = .12 (Brief/detailed explanation); .16 (retention interval)"]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"679b983b48bc0d06cc3e03cde99cd4f9","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Huang, H. (2017). A war of (mis)information: The political effects of rumors and rumor rebuttals in an authoritarian country. British Journal of Political Science, 47(2), 283-312."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1017/S0007123415000253"},"children":["https://doi.org/10.1017/S0007123415000253"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Despite the prevalence of anti-government rumors in authoritarian countries, little is currently known about their effects on citizens’ attitudes toward the government, and whether the authorities can effectively combat rumors. With an experimental procedure embedded in two surveys about Chinese internet users’ information exposure, this study finds that rumors decrease citizens’ trust in the government and support of the regime. Moreover, individuals from diverse socio-economic and political backgrounds are similarly susceptible to thinly evidenced rumors. Rebuttals generally reduce people’s belief in the specific content of rumors, but often do not recover political trust unless the government brings forth solid and vivid evidence to back its refutation or win the endorsement of public figures broadly perceived to be independent. But because such high-quality and strong rebuttals are hard to come by, rumors will erode political support in an authoritarian state. These findings have rich implications for studies of rumors and misinformation in general, and authoritarian information politics in particular.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Each study had a pure control group, a rumors group that was exposed to two rumors (A and B), and three rebuttal groups that were respectively exposed to the rebuttals of rumor A, rumor B, or both rumors. Each group's political trust was measured after the treatment. Belief in rumor was also measured except for the control group that was not exposed to the rumors.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Both rumors and rebuttals were tweets from the Chinese social media platform Weibo.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, predispositions, levels of belief in the rumors, political trust on related issues",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["The study experimentally tested effects of rebuttals on belief in rumors.","The study experimentally tested effects of rebuttals on political trust."],"N":[631,799],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["Some rebuttals did not significantly reduce belief in the rumors. When significant, the effects ranged from 0.29 to 0.86 on a 7-point scale.","Rebuttals improved political trust in some areas and the effects ranged from 0.28 to 0.38 on a 7-point scale."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"8fcf5611f3c35182a4df8439f9dfb16a","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Gesser-Edelsburg, A., Diamant, A., Hijazi, R., & Mesch, G. S. (2018). Correcting misinformation by health organizations during measles outbreaks: A controlled experiment. PloS one, 13(12), e0209505."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0209505"},"children":["https://doi.org/10.1371/journal.pone.0209505"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Objectives\n\n(1) To examine ways for health organizations to correct misinformation concerning the measles vaccination on social networks for two groups: pro-vaccination and hesitant; (2) To examine the types of reactions of two subgroups (pro-vaccination, hesitant) to misinformation correction; and (3) To examine the effect of misinformation correction on these two subgroups regarding reliability, satisfaction, self-efficacy and intentions.\n\nMethods\n\nA controlled experiment with participants divided randomly into two conditions. In both experiment conditions a dilemma was presented as to sending a child to kindergarten, followed by an identical Facebook post voicing the children mothers’ concerns. In the third stage the correction by the health organization is presented differently in two conditions: Condition 1 –common information correction, and Condition 2 –recommended (theory-based) information correction, mainly communicating information transparently and addressing the public’s concerns. The study included (n = 243) graduate students from the Faculty of Social Welfare and Health Sciences at Haifa University.\n\nResults\n\nA statistically significant difference was found in the reliability level attributed to information correction by the Health Ministry between the Control condition and Experimental condition (sig<0.001), with the average reliability level of the subjects in Condition 2 (M = 5.68) being considerably higher than the average reliability level of subjects in Condition 1 (4.64). A significant difference was found between Condition 1 and Condition 2 (sig<0.001), with the average satisfaction from the Health Ministry’s response of Condition 2 subjects (M = 5.75) being significantly higher than the average satisfaction level of Condition 1 subjects (4.66). Similarly, when we tested the pro and hesitant groups separately, we found that both preferred the response presented in Condition 2.\n\nConclusion\n\nIt is very important for the organizations to correct misinformation transparently, and to address the emotional aspects for both the pro-vaccination and the hesitant groups. The pro-vaccination group is not a captive audience, and it too requires a full response that addresses the public's fears and concerns.\n\n",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants were randomly divided into two groups: Condition A (Common Information Correction) and Condition B (Recommended Information Correction). Both groups underwent identical simulations at first. The initial simulation was about a dilemma concerning sending children to a kindergarten during a measles outbreak, especially when some kids weren't vaccinated. The next showed a mother's misleading post about measles. In subsequent simulations, Condition A received a straightforward correction from a health authority without addressing emotional concerns. Condition B received a detailed correction that also acknowledged the emotional concerns of the mother and other parents.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"A dilemma was presented as to sending a child to kindergarten, followed by an identical Facebook post voicing the children mothers’ concerns. Lastly, the correction by the Health Ministry was presented.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Socio-demographic characteristics (sex, age, education, ethnicity, profession, marital status), parental variables characteristics (number of children, children’s age), whether pro-vaccination or vaccine hesitant, and outcome variables.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"N":[243],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["A significant difference was found in the reliability level attributed to the Health Ministry's response between Condition 1 and Condition 2 (sig<0.001), with the average reliability level of the participants in Condition 2 (M = 5.68) being considerably higher than the average reliability level of the participants in Condition 1 (M = 4.64). A significant difference was found between Condition 1 and Condition 2 (sig<0.001), with the average satisfaction with the Health Ministry’s response of Condition 2 participants (M = 5.75) being significantly higher than the average satisfaction of Condition 1 subjects (M = 4.66).  No significant difference was found between Condition 1 and Condition 2 as to further information searching after the Health Ministry’s response. In Condition 1 a significant association was found between satisfaction and this behavioral intention. The higher the participants' satisfaction with the Health Ministry’s response, the more they tended to vaccinate their children (sig<0.01). In Condition 2, no association was not found and this might be attributed to the fact that Condition 2 lacked statistical power for a regression because only seven of the Condition 2 participants said they would not vaccinate their children versus 80 who said they would.."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"d2e602b35000289555c85672fc0cf76b","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Paynter, J., Luskin-Saxby, S., Keen, D., Fordyce, K., Frost, G., Imms, C., Miller, S., Trembath, D., Tucker, M., & Ecker, U. K. H. (2019). Evaluation of a template for countering misinformation—Real-world autism treatment myth debunking. PLOS ONE, 14(1), Article e0210746."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0210746"},"children":["https://doi.org/10.1371/journal.pone.0210746"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0210746"},"children":["https://doi.org/10.1371/journal.pone.0210746"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation poses significant challenges to evidence-based practice. In the public health domain specifically, treatment misinformation can lead to opportunity costs or direct harm. Alas, attempts to debunk misinformation have proven sub-optimal, and have even been shown to “backfire”, including increasing misperceptions. Thus, optimized debunking strategies have been developed to more effectively combat misinformation. The aim of this study was to test these strategies in a real-world setting, targeting misinformation about autism interventions. In the context of professional development training, we randomly assigned participants to an “optimized-debunking” or a “treatment-as-usual” training condition and compared support for non-empirically-supported treatments before, after, and six weeks following completion of online training. Results demonstrated greater benefits of optimized debunking immediately after training; thus, the implemented strategies can serve as a general and flexible debunking template. However, the effect was not sustained at follow-up, highlighting the need for further research into strategies for sustained change.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Compared existing training materials aiming to reduce support for (and ultimately use of) non-empirically supported autism treatments with an optimized debunking treatment in early-childhood intervention staff immediately and after a 6-week delay.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Extensive training materials refuting fad autism treatments in the form of text, charts, images and aimed to build trust, convey social norms, affirm participants' identity, and communicate the evidence clearly.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Attitudes towards evidence-based practice; deference to scientific authority; perceived social validity of intervention",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Study experimentally compared existing training materials aiming to reduce support for (and ultimately use of) non-empirically-supported autism treatments with an optimized debunking treatment in early-childhood intervention staff immediately and after a 6-week delay."],"N":[856],"Effect size":["Partial-Eta squared (η2p): 0.66 (immediate)"],"Comments":["η2p = .66 (immediate) [effect size of interaction, i.e. comparison to treatment-as-usual  intervention: η2p = .20] / .45 (delayed) [note the latter is not reported in the paper as there was no longer a significant difference to the treatment-as-usual condition]."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"1d4c406a80e75918b6b53119f17b3a88","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Schmid, P., & Betsch, C. (2019). Effective strategies for rebutting science denialism in public discussions. Nature Human Behaviour, 3(9), 931–939."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1038/s41562-019-0632-4"},"children":["https://doi.org/10.1038/s41562-019-0632-4"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/xx2kt/"},"children":["https://osf.io/xx2kt/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Science deniers question scientific milestones and spread misinformation, contradicting decades of scientific endeavour. Advocates for science need effective rebuttal strategies and are concerned about backfire effects in public debates. We conducted six experiments to assess how to mitigate the influence of a denier on the audience. An internal meta-analysis across all the experiments revealed that not responding to science deniers has a negative effect on attitudes towards behaviours favoured by science (for example, vaccination) and intentions to perform these behaviours. Providing the facts about the topic or uncovering the rhetorical techniques typical for denialism had positive effects. We found no evidence that complex combinations of topic and technique rebuttals are more effective than single strategies, nor that rebutting science denialism in public discussions backfires, not even in vulnerable groups (for example, US conservatives). As science deniers use the same rhetoric across domains, uncovering their rhetorical techniques is an effective and economic addition to the advocates’ toolbox.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Experiments with a 2 (pre–post measure: within) × 2 (technique rebuttal absent vs. present: between) × 2 (topic rebuttal absent vs. present: between) mixed design. Participants were randomly assigned to 1 of 4 rebuttal conditions (advocate absent, topic only, technique only, combination of topic and technique).\n\n                                \n                        \n                \n",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"2 vignettes from an audiotaped or written radio discussion. The discussion included statements from a science denier (Study 1,2,3,4, 6: Vaccination Study 5: Climate Change) using misleading techniques: impossible expectation, conspiracy theories. In the advocate for science condition topic rebuttal, technique rebuttal or the combination of both were used. Depending on condition, an advocate for science was either absent, used topic rebuttal, technique rebuttal or a combination of both. \n\n                                \n                        \n                \n",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Other relevant moderators: political ideology (Experiment 4, Experiment 6), confidence in vaccination (Experiment 2,3,4 & 6)\nOther relevant mediators: perceived persuasiveness of the denier and advocate\n(Experiment 1), the perceived argument strength of the denier\nand advocate (Experiments 2 and 5) and participants’ persuasion\nknowledge (Experiment 3). \nRelevant control variables: Knowledge about vaccination, climate change (a full list of variables is provided in the supplement of the paper)\n                                \n                        \n                \n",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["Across all 6 studies","Across all 6 studies","Across all 6 studies"],"Description":["Internal meta-analysis: Rebuttal vs no rebuttal","Internal meta-analysis: Technique rebuttal vs no technique rebuttal","Internal meta-analysis: Topic rebuttal vs no topic rebuttal"],"N":[1773,1773,1773],"Effect size":["Hedges' g: 0.49 (95%-CI: 0.37-0.6)","Hedges' g: 0.31 (95%-CI: 0.22-0.41)","Hedges' g: 0.21 (95%-CI: 0.04-0.38)"],"Comments":["Attitude: g=0.49, 95% CI: 0.37, 0.60; intention: g=0.57, 95% CI: 0.46, 0.68","Attitude: g = 0.31, 95% CI: 0.22, 0.41; intention: g = 0.31, 95% CI: 0.20, 0.42","Attitude: g = 0.21, 95% CI: 0.04, 0.38; intention: g = 0.33, 95% CI: 0.24, 0.43."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"f2880f2a8f4dbbdeff18f589152f09b5","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Schmid, P., Schwarzer, M., & Betsch, C. (2020). Weight-of-evidence strategies to mitigate the influence of messages of science denialism in public discussions. Journal of Cognition, 3(1), Article 36."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"http://doi.org/10.5334/joc.125"},"children":["http://doi.org/10.5334/joc.125"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.17605/OSF.IO/SEFQU"},"children":["https://doi.org/10.17605/OSF.IO/SEFQU"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"In mass media, the positions of science deniers and scientific-consensus advocates are repeatedly presented in a balanced manner. This false balance increases the spread of misinformation under the guise of objectivity. Weight-of-evidence strategies are an alternative, in which journalists lend weight to each position that is equivalent to the amount of evidence that supports the position. In public discussions, journalists can invite more advocates of scientific consensuses than science deniers (outnumbering) or they can employ warnings about the false-balance effect prior to the discussions (forewarning). In three pre-registered laboratory experiments, we tested the efficacy of outnumbering and forewarning as weight-of-evidence strategies to mitigate science deniers’ influence on individuals’ attitudes towards vaccination and their intention to vaccinate. We explored whether advocates’ responses to science deniers (rebuttal) and audiences’ issue involvement moderate the efficacy of these strategies. A total of N = 887 individuals indicated their attitudes towards vaccination and their intention to vaccinate before and after watching a television (TV) discussion. The presence and absence of forewarning, outnumbering and rebuttal were manipulated between subjects; participants also indicated their individual issue involvement. We obtained no evidence that outnumbering mitigates damage from denialism, even when advocates served as multiple sources. However, forewarning about the false-balance effect mitigated deniers’ negative effects. Moreover, the protective effect was independent of rebuttal and issue involvement. Thus, forewarnings can serve as an effective, economic and theory-driven strategy to counter science denialism in public discussions, at least for highly educated individuals such as university students.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Experiment 1: A 2 (pre–post measure: within) × 2 (rebuttal absent vs. present: between) × 2 (outnumbering absent vs. present: between) mixed design. Rebuttal was either absent or present.\nExperiment 2: A 2 (pre–post measure: within) × 2 (rebuttal absent vs. present: between) × 2 (outnumbering absent vs. present: between) × 2 (forewarning absent vs. present: between) mixed design. Rebuttal was either absent or present.\nExperiment 3: A 2 (pre-post measure: within) × 2 (rebuttal absent vs. present: between) × 2 (outnumbering absent vs. present: between) × 2 (forewarning absent vs. present: between) mixed design. Rebuttal was either absent or present.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Mock TV discussion as part of a fictitious scenario. Discussion statements taken from a science denier using misleading techniques:  impossible expectation, conspiracy theories. In the advocate for science condition topic and technique rebuttal were used",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Other relevant variables: issue involvement, speakers’ perceived credibility (a full list of variables is provided in the supplement of the paper).\n\n                                \n                        \n                \n",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The study focused on several conditions for rebutting science denialism in public discussions (i.e., outnumbering, forewarning). All conditions included rebuttals - and we focus only on effectiveness of rebuttal conditions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2","3"],"Description":["Study 1: Outnumbering and rebuttal, results for rebuttal vs no rebuttal.","Study 2: Outnumbering, forewarning and rebuttal, results for rebuttal vs no rebuttal.","Study 3: outnumbering by delivering multiple rebuttal sources, forewarning and rebuttal, results for rebuttal vs no rebuttal."],"N":[101,390,390],"Effect size":["Partial-Eta squared (η2p): Attitude η²p = .094; Intention η²p = .036","Partial-Eta squared (η2p): Attitude η²p = .116; Intention η²p = .124 ; confidence in vaccination η²p = .154","Partial-Eta squared (η2p): Attitude η²p = .059; Intention η²p = .078 ; confidence in vaccination η²p = .108"],"Comments":["Watching the public debate significantly damaged individuals’ attitudes towards vaccination and their intention to get vaccinated, but the rebuttal successfully mitigated this damage. The mitigating effect was only marginally significant on intention to get vaccinated.","Replicating the results from Experiment 1, watching the public debate significantly damaged individuals’ attitudes towards vaccination, including intention to get vaccinated and confidence in vaccination. However, the rebuttal mitigated this damage on all outcome measures, again indicating successful manipulation.",null]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"016a76de8e9f85972e08bd021daa2ad7","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Ecker, U. K. H., Butler, L. H., & Hamby, A. (2020). You don't have to tell a story! A registered report testing the effectiveness of narrative versus non-narrative misinformation corrections. Cognitive Research: Principles and Implications, 5(1), Article 64."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1186/s41235-020-00266-x"},"children":["https://doi.org/10.1186/s41235-020-00266-x"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/gtm9z/"},"children":["https://osf.io/gtm9z/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation often has an ongoing effect on people’s memory and inferential reasoning even after clear corrections are provided; this is known as the continued influence effect. In pursuit of more effective corrections, one factor that has not yet been investigated systematically is the narrative versus non-narrative format of the correction. Some scholars have suggested that a narrative format facilitates comprehension and retention of complex information and may serve to overcome resistance to worldview-dissonant corrections. It is, therefore, a possibility that misinformation corrections are more effective if they are presented in a narrative format versus a non-narrative format. The present study tests this possibility. We designed corrections that are either narrative or non-narrative, while minimizing differences in informativeness. We compared narrative and non-narrative corrections in three preregistered experiments (total N = 2279). Experiment 1 targeted misinformation contained in fictional event reports; Experiment 2 used false claims commonly encountered in the real world; Experiment 3 used real-world false claims that are controversial, in order to test the notion that a narrative format may facilitate corrective updating primarily when it serves to reduce resistance to correction. In all experiments, we also manipulated test delay (immediate vs. 2 days), as any potential benefit of the narrative format may only arise in the short term (if the story format aids primarily with initial comprehension and updating of the relevant mental model) or after a delay (if the story format aids primarily with later correction retrieval). In all three experiments, it was found that narrative corrections are no more effective than non-narrative corrections. Therefore, while stories and anecdotes can be powerful, there is no fundamental benefit of using a narrative format when debunking misinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In 3 experiments, narrative (N) and non-narrative (NN, fact-based) corrections were contrasted. Experiment 1: RCT (2 control conditions and the 2 treatment conditions, N and NN, mixed within-between design, with the within-subjects factor of condition, and the between-subjects factor of test delay). Experiments 2 and 3: 2 × 2 mixed within–between design, with the within-subjects factor of correction type (NN; N) and the between-subjects factor of test delay (immediate; delayed).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 1: fictitious event reports,\nStudy 2: false claims encountered in the real world,\nStudy 3: controversial real-world false claims selected to be congruent with a conservative worldview",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Memory test for the purposes of adequate encoding of test items",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2","3"],"Description":["Experiment 1 investigated whether corrections of event-related misinformation are more effective if presented in a narrative format. Experiment 1 targeted misinformation contained in fictional event reports in four conditions. There were two control conditions: One featured no misinformation (noMI condition), another featured a piece of misinformation that was not corrected (noC condition). The two experimental conditions corrected the initially-provided misinformation using either a non-narrative (NN) or narrative (N) correction. The test phase followed the study phase either immediately or after a 2-day delay.","Experiment 2 tested whether corrections targeting real-world misconceptions are more effective if they are provided in a narrative versus non-narrative format. Experiment 2 used false claims commonly encountered in the real world, including both true facts and common misconceptions (myths). Claims were followed by explanations that affirmed the facts and corrected the myths. Corrections were either in a non-narrative (NN) or narrative (N) form, and the test was again either immediate or delayed.","Experiment 3 tested whether narrative corrections would be more effective than non-narrative corrections when debunking worldview-consistent misconceptions. Experiment 3 used real-world false claims that are controversial, including both facts and myths, which were followed by affirmations and corrections. Corrections were again either non-narrative (NN) or narrative (N), and the test was immediate or delayed."],"N":[770,776,733],"Effect size":["Partial-Eta squared (η2p): 0.237 (immediate, non-narrative)","Partial-Eta squared (η2p): 0.604 (immediate, non-narrative)","Partial-Eta squared (η2p): 0.386 (immediate, non-narrative)"],"Comments":["Corrections reduced misinformation reliance in inferences regardless of format, both immediately and after a 2-day delay. η2p = .237 (immediate, non-narrative) / .245 (immediate, narrative) / .134 (delayed, non-narrative) / .127 (delayed, narrative) [note these are from contrasts that take full ANOVA model into account; effect sizes from isolated contrasts are larger, .211 - .415].","Corrections reduced misinformation beliefs and misinformation reliance in inferences regardless of format, both immediately and after a 2-day delay.  η2p = .604 (immediate, non-narrative) / .578 (immediate, narrative) / .506 (delayed, non-narrative) / .480 (delayed, narrative) [not reported in paper; calculated from one-sample t-test of belief change against zero; note zero may not be most appropriate baseline as it does not take demand characteristics into account].","Corrections slightly reduced misinformation beliefs and misinformation reliance in inferences regardless of format, both immediately and after a 2-day delay. η2p = .386 (immediate, non-narrative) / .334 (immediate, narrative) / .196 (delayed, non-narrative) / .185 (delayed, narrative) [not reported in paper; calculated from one-sample t-test of belief change against zero; note zero may not be most appropriate baseline as it does not take demand characteristics into account]."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"e6587f08b9099ed6298cc4fec237cc7f","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Bowles, J., Larreguy, H., & Liu, S. (2020). Countering misinformation via WhatsApp: Preliminary evidence from the COVID-19 pandemic in Zimbabwe. PLOS One, 15(10): e0240005."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0240005"},"children":["https://doi.org/10.1371/journal.pone.0240005"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"We examine how information from trusted social media sources can shape knowledge and behavior when misinformation and mistrust are widespread. In the context of the COVID-19 pandemic in Zimbabwe, we partnered with a trusted civil society organization to randomize the timing of the dissemination of messages aimed at targeting misinformation about the virus to 27,000 newsletter WhatsApp subscribers. We examine how exposure to these messages affects individuals’ beliefs about how to deal with the virus and preventative behavior. In a survey of 864 survey respondents, we find a 0.26σ increase in knowledge about\nCOVID-19 as measured by responses to factual questions. Through a list experiment embedded in the survey, we further find that potentially harmful behavior—not abiding by lockdown guidelines—decreased by 30 percentage points. The results show that social media messaging from trusted sources may have substantively large effects not only on individuals’ knowledge but also ultimately on related behavior.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"COVID-19 messages provided to the treated group on Monday and to the control group on Saturday. Survey conducted on Thursday.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"WhatsApp messages targeting COVID-19 misinformation crafted by a trusted civil society organization.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This study experimentally tested the effects of staggered provision of WhatsApp messages targeting COVID-19 misinformation"],"N":[868],"Effect size":["Standard deviation units: Knowledge: 0.07; Behavior: 0.26"],"Comments":["The study found that WhatsApp messages had substantial effects on knowledge, with treated respondents reporting 0.26σ greater factual knowledge (p < 0.001). Additionally, the treatment resulted in a significant reduction in non-compliance with social distancing, from 37% in the control group to 7% in the treatment group, implying a change in behavior (p < 0.05). These effects were robust across different specifications and not explained by demand effects or social desirability bias."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"ae7c13f0df0a9cfd6613941d2994c141","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Porter, E. & Wood, T. J. (2021). The Global effectiveness of fact-checking: Evidence from simultaneous experiments in Argentina, Nigeria, South Africa, and the United Kingdom. Proceedings of National Academy of Sciences 118(37), e2104235118."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1073/pnas.2104235118"},"children":["https://doi.org/10.1073/pnas.2104235118"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/a3p49"},"children":["https://osf.io/a3p49"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The spread of misinformation is a global phenomenon, with implications for elections, state-sanctioned violence, and health outcomes. Yet, even though scholars have investigated the capacity of fact-checking to reduce belief in misinformation, little evidence exists on the global effectiveness of this approach. We describe fact-checking experiments conducted simultaneously in Argentina, Nigeria, South Africa, and the United Kingdom, in which we studied whether fact-checking can durably reduce belief in misinformation. In total, we evaluated 22 fact-checks, including two that were tested in all four countries. Fact-checking reduced belief in misinformation, with most effects still apparent more than 2 weeks later. A meta-analytic procedure indicates that fact-checks reduced belief in misinformation by at least 0.59 points on a 5-point scale. Exposure to misinformation, however, only increased false beliefs by less than 0.07 points on the same scale. Across continents, fact-checks reduce belief in misinformation, often durably so.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In each country, participants were randomly exposed to between zero and seven misinformation items and between zero and seven fact-checks. Randomization occurred at the item level. All participants then immediately answered outcome questions about their belief in the misinformation. In this second wave, participants were asked outcome questions only, receiving no reminders of their earlier treatment.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Fact-checks produced by fact-checking organizations in each country, while misinformation stimuli consisted of brief summaries of the false claims that led to the corresponding fact-checks.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Ideology",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["To understand the effects of corrections in aggregate, a meta-analysis with random effects of the 28 experiments was performed"],"N":[8000],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Corrections reduced belief in falsehoods by 0.59 point on the 5-point scale (P < 0.01). On the same scale, misinformation only increased belief in falsehoods by 0.07 (P > 0.05). Fact-checks thus increase factual accuracy by more than eight times the amount that misinformation degrades factual accuracy."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"3a36f4a5227a289e105a8447dcec2c39","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Winters, M., Oppenheim, B., Sengeh, P., Jalloh, M. B., Webber, N., Pratt, S. A., ... & Nordenstedt, H. (2021). Debunking highly prevalent health misinformation using audio dramas delivered by WhatsApp: evidence from a randomised controlled trial in Sierra Leone. BMJ global health, 6(11), e006954."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"http://dx.doi.org/10.1136/bmjgh-2021-006954"},"children":["http://dx.doi.org/10.1136/bmjgh-2021-006954"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"N/A",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"This study focused on countering infectious disease misinformation, which is a significant challenge for disease control efforts. The research was conducted in Sierra Leone, aiming to address two prevalent misconceptions: the belief that mosquitoes cause typhoid and the notion that typhoid always co-occurs with malaria. The study explored whether directly debunking misinformation or simply providing correct information is more effective in combating these misconceptions. Two intervention approaches were tested, involving a total of 491 participants. Group A (n=246) received an information intervention that explicitly discussed the misinformation, explained its inaccuracies, and then provided scientifically accurate information. Group B (n=245) received an intervention that solely focused on delivering accurate information without directly addressing the misinformation. Both interventions were delivered via locally relevant audio dramas on WhatsApp. A control group (n=245) received unrelated content about breastfeeding.\nAt the start of the study, 51% of participants believed that mosquitoes cause typhoid, and 59% believed that typhoid always co-occurs with malaria. The study's endpoint survey, completed by 91% of participants, revealed that both interventions effectively reduced belief in the misinformation compared to the control group. Results from different analytical approaches indicated that directly debunking misinformation may be more effective. Both interventions also improved participants' knowledge and self-reported behaviors related to reducing typhoid risk, including an increase in the use of a key preventive measure: drinking treated water. In conclusion, this community-based field experiment demonstrated the potential to counter prevalent health misinformation. The study suggests that directly addressing and debunking misinformation in detail can be particularly effective in changing beliefs and behaviors.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Eligible participants were adults (18 years and older), living in Freetown, fluent in Krio, in possession of a phone with WhatsApp and with no hearing impairments.The information intervention for group A (n=246) explicitly discussed misinformation and explained why it was incorrect and then provided the scientifically correct information. The intervention for group B (n=245) only focused on providing correct information, without directly discussing related misinformation. Both interventions were delivered via audio dramas on WhatsApp that incorporated local cultural understandings of typhoid. Participants were randomised 1:1:1 to the intervention groups or the control group (n=245), who received two episodes about breastfeeding.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Belief that (1) typhoid is caused by mosquitoes and (2) can only co-occur with malaria were captured with yes/ no questions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics including sex, education, religion,\r\nmonthly income and age, and main outcomes.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","1"],"Description":["The study explored whether directly debunking misinformation or simply providing correct information is more effective in combating these misconception that typhoid is caused by mosquitoes.","The study explored whether directly debunking misinformation or simply providing correct information is more effective in combating these misconception that typhoid can only co-occur with malaria."],"N":[736,736],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["The belief that typhoid is caused by mosquitoes was significantly reduced in intervention group A compared with the control group in the ITT analysis (group A: adjusted OR (AOR) 0.29, 95% CI 0.18 to 0.47, see table 3 and online supplemental figure S1). In intervention group B, the reduction was not significant (AOR 0.61, 95% CI 0.39 to 0.95, p=0.029).","The belief that typhoid co-occurs with malaria was significantly reduced in both intervention groups in the ITT analysis (group A: AOR 0.29, 95% CI 0.19 to 0.45; group B: AOR 0.55, 95% CI 0.36 to 0.83)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"d68f1911c3d8c3c10910562fb821f854","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Tay, L. Q., Hurlstone, M. J., Kurz, T., & Ecker, U. K. H. (2022). A comparison of prebunking and debunking interventions for implied versus explicit misinformation. British Journal of Psychology, 113(3), 591–607."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1111/bjop.12551"},"children":["https://doi.org/10.1111/bjop.12551"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/dktnu/"},"children":["https://osf.io/dktnu/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Psychological research has offered valuable insights into how to combat misinformation. The studies conducted to date, however, have three limitations. First, pre-emptive (“prebunking”) and retroactive (“debunking”) interventions have mostly been examined in parallel, and thus it is unclear which of these two predominant approaches is more effective. Second, there has been a focus on misinformation that is explicitly false, but implied misinformation that uses literally true information to mislead is common in the real world. Finally, studies have relied mainly on questionnaire measures of reasoning, neglecting behavioural impacts of misinformation and interventions. To offer incremental progress towards addressing these three issues, we conducted an experiment (N = 735) involving misinformation on fair trade. We contrasted the effectiveness of prebunking versus debunking and the impacts of implied versus explicit misinformation, and incorporated novel measures assessing consumer behaviours (i.e., willingness-to-pay; information seeking; online misinformation promotion) in addition to standard questionnaire measures. In general, both prebunking and debunking reduced misinformation reliance. We also found that individuals tended to rely more on explicit than implied misinformation both with and without interventions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Tested effectiveness of (identical) prebunking and debunking interventions with implied and explicit misinformation.The experiment adopted a 2 (misinformation type: implied, explicit) × 3 (intervention type: no intervention, prebunking, debunking) plus control between-subjects design.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Fictional articles for control, implied misinformation, and explicit misinformation on a real-world topic (fair trade).",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Social media behavior, tweet-sentiment",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","1"],"Description":["Study experimentally tested effects of prebunking and debunking interventions on reliance on misinformation, willingness-to-purchase, and tweet sentiment","Study experimentally tested effects of prebunking and debunking interventions on reliance on misinformation, willingness-to-purchase, and tweet sentiment"],"N":[856,856],"Effect size":["Cohen’s d: 0.89","Cohen’s d: 0.46"],"Comments":["Both debunking and prebunking significantly reduced participants’ number of references to misinformation, with t(625) = −9.60, p = <.001, d = .89, and t(625) = −8.08, p = <.001, d = .73, respectively. The difference between debunking and prebunking conditions was not significant, t(625) = −1.61, p = .108, d = .28.","Debunking resulted in more positive sentiments compared to the no-intervention condition, t(625) = 4.65, p = <.001, d = .46, as well as compared to the prebunking condition, t(625) = 2.44, p = .030, d = .24. There was also a significant difference between prebunking and no-intervention conditions, with prebunking resulting in more positive sentiments, t(625) = 2.23, p = .030, d = .22."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"69222b64d3ba648b3de4f900eb5a28ef","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Schmid, P., & Betsch, C. (2022). Benefits and Pitfalls of Debunking Interventions to Counter mRNA Vaccination Misinformation During the COVID-19 Pandemic. Science Communication. 44(5):531–58."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/10755470221129608"},"children":["https://doi.org/10.1177/10755470221129608"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/dk52c/"},"children":["https://osf.io/dk52c/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation about mRNA vaccination is a barrier in the global fight against the COVID-19 pandemic. Thus, authorities often rely on text-based refutations as a countermeasure. In two experiments (N = 2,444), text-based refutations effectively reduced the belief in misinformation and immunized participants against the impact of a misleading social media post. However, a follow-up (N = 817) questions the longevity of these debunking and prebunking effects. Moreover, the studies reveal potential pitfalls by showing a row of unintended effects of the refutations (lacking effect on intentions, backfire-effects among religious groups, and biased judgments when omitting information about vaccine side effects).",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Each participant was assigned randomly to one of two conditions, resulting from the 2 (debunking vs. control; between subjects) × 3 (measurement before vs. immediately after the debunking versus 2 months after the debunking; within subjects mixed design. Credibility of (mis)information was measured at all time points. The control group received a similar information text without the crucial parts of debunking (warning about a misinformation and explanation of why the misinformation is wrong).\nDebunking text design was informed by the Debunking Handbook.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Texts were created specifically for this study following the Debunking Handbook and together with medical experts. The misinformation was the myth that mRNA vaccines alter human DNA.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, Credibility judgments of (mis)information, religiosity,",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Results are from Study 1",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Study experimentally tested effects of text-based refutations."],"N":[1387],"Effect size":["Regression coefficient B: Debunking effect short term: −7.06, Debunking effect long term: −1.01 (95%-CI: Debunking effect short term: −9.81; Debunking effect long term: −4.48-Debunking effect short term: −4.31; Debunking effect long term: 2.47)"],"Comments":["Participants who received the debunking text rated the false antivaccine news headline that mRNA vaccines alter the human genome as less credible compared to the control condition."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"bfa80da431a856fa5cb0d1f1127e56e4","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Bauer, F, & Wilson, K. L. (2022). Reactions to China-linked fake news: Experimental evidence from Taiwan. The China Quarterly 249, 21-46."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1017/S030574102100134X"},"children":["https://doi.org/10.1017/S030574102100134X"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"China is accused of conducting disinformation campaigns on Taiwan’s social media. Existing studies on foreign interventions in democratic societies predict that such disinformation campaigns should lead to increasing partisan polarization within Taiwan. We argue that a backlash effect, making Taiwan’s citizens more united against China, is equally plausible. We conduct a survey experiment exposing participants to a real-life rumour and rebuttal to test these competing hypotheses. We find, at best, mixed evidence for polarization. Although neither rumour nor rebuttal mention China, there is consistent evidence of backlash against China. Most notably, participants across the political spectrum are more inclined to support Taiwanese independence after viewing the rumour rebuttal. These findings indicate that citizens may put aside partisanship when confronted with false news that is plausibly linked to an external actor. We conclude by discussing the broader applicability of our theory and implications for cross-state relations.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Compared the credibility ratings and political attitudes of the control group, the rumor group, and the rebuttal group.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Rumor initiated by an opposition politician and rebuttal from the government.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, political affiliation, rumor credibility, political attitudes",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study compared the credibility ratings and political attitudes of the control group, the rumor group, and the rebuttal group."],"N":[561],"Effect size":["Average treatment effects using difference-in-means tests: 0.36"],"Comments":["The rebuttal in the study increased support for independence by approximately 0.364 on a 6-point scale."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"5f9b1eac31ad45c8db7381b0eab64754","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Schroeder, N. L., & Kucera, A. C. (2022). Refutation text facilitates learning: A meta-analysis of between-subjects experiments. Educational Psychology Review, 34(2), 957-987."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1007/s10648-021-09656-z"},"children":["https://doi.org/10.1007/s10648-021-09656-z"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Scientific misconceptions are ubiquitous, and in our era of near-instant information exchange, this can be problematic for both public health and the public understanding of scientific topics. Refutation text is one instructional tool for addressing misconceptions and is simple to implement at little cost. We conducted a random-effects meta-analysis to examine the effectiveness of the refutation text structure on learning. Analysis of 44 independent comparisons (n = 3,869) showed that refutation text is associated with a positive, moderate effect (g = 0.41, p < .001) compared to other learning conditions. This effect was consistent and robust across a wide variety of contexts. Our results support the implementation of refutation text to help facilitate scientific understanding in many fields.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Databases were searched for the term refutation text (i.e. texts that explicitly state a misconception, refute it, and provide an explanation of the correct idea), inclusion criteria was a between subject comparison of a refutation text and a non-refutation text condition.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Multiple topics regarding misconceptions",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Varied",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["A meta-analysis of 473 studies aimed to gauge an effect of refutation texts on learning facilitation."],"N":[3869],"Effect size":["Hedges g: 0.41 (95%-CI: 0.3-0.51)"],"Comments":["Refutation text is associated with a positive moderate effect compared to other learning conditions."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"24ad03ba655e2e351db153d7460b09be","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Yu, W., Shen, F., & Min, C. (2022). Correcting science misinformation in an authoritarian country: An experiment from China. Telematics and Informatics 66, 101749."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1016/j.tele.2021.101749"},"children":["https://doi.org/10.1016/j.tele.2021.101749"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"People rely on heuristic cues to evaluate messages. An increasing number of studies found corrective messages useful in correcting misinformation, and the correction effect varies on heuristic cues. Existing studies, however, mostly focus on correction effects in the Western context. This study aims to compare the effects of corrective messages with different heuristic cues in an authoritarian society. We focused on the cues that suggest government authoritativeness. Using an online experiment, we compared the impacts of correction sources (official vs. professional vs. layperson) and tones (formal vs. conversational) on the believability of the correction. The results indicated corrections from a government source and delivered in a formal tone were more believable in China. In addition, we examined the moderating role of attitude congruence.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"An online experiment with a 2 × 6 × 2 between-subjects design (topic × source × tone). The examined topics were GMO and 5G. The three types of sources were official sources (government and mass media), professional sources (popular science media and scientist), and layperson sources (celebrity and ordinary person). The message tone was formal vs. conversational tone. First, participants were randomly assigned to one of the 24 groups. Then they were asked to report their topic attitude. They read a message containing misinformation and immediately after read a correction message. Participants were asked to rate their perceived believability of the correction message.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"The material was presented as posts from Sina Weibo, one of the most popular social media platforms in China. The misinformation messages were adapted from authentic misinformation about GMOs and 5G on social media. The correction posts started with a hashtag signaling that the news is fake news and then explained why the claims were wrong.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"The believability of correction messages was measured using a 5-point semantic differential scale. Participants rated how they perceived the correction message along with five adjective pairs: unbelievable/believable, unconvincing/convincing, untrustworthy/trustworthy, inaccurate/ accurate, and uninformative/informative. The average scores were used to form an index of correction.\nTopic attitudes were measured by asking participants to rate the extent to which they support adoption of GMO or 5G technology.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","1","1","1","5"],"Description":["Comparison of believability across different sources.","Effect of message tone on believability.","Interaction of message tone with topic on correction believability.","Effect of attitude congruence on correction believability.","Moderation effect of tone on believability based on attitude congruence."],"N":[2965,2965,2965,2713,2713],"Effect size":["F statistic: 2.92","F statistic: 5.85","F statistic: 9","F statistic: 218.21","F statistic: 5.18"],"Comments":["The believability of government source was significantly higher than the professional sources, the lay sources, and the mass media source.","The believability of the correction message in a formal tone was higher than in a conversational tone.","Participants tend to believe the 5G correction more when it was written in a formal tone.","Participants with congruent attitudes perceived the correction as more believable.","Participants with an incongruent attitude perceived formal corrections as more believable than conversational ones."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"6bf0c1ac9519b3952ce0870203bfcb2d","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Batista Pereira, F., Bueno, N. S., Nunes, F., & Pavão, N. (2022). Fake news, fact checking, and partisanship: the resilience of rumors in the 2018 Brazilian elections. The Journal of Politics, 84(4), 2188-2201."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1086/719419"},"children":["https://doi.org/10.1086/719419"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"N/A",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Studies about fake news in developed democracies suggest that fact checking reduces misinformation. They also identify partisan-motivated reasoning as the driving force behind beliefs in false information and the resistance to corrections. But how effective are corrections in developing democracies? Does the dominant explanation for misinformation hold in settings with different partisan configurations? Drawing on a survey experiment during the 2018 elections in Brazil, we find that fact- checking corrections in Brazil are ineffective at reducing misinformation. They fail even when they are most likely to work: among nonpartisans and when they confirm individuals’ political predispositions. Although partisan-motivated reasoning predicts beliefs in false information, it is not the main driving force behind the (in)effectiveness of corrections. This study calls attention to the challenges of curbing political misinformation in developing democracies and urges future research to foster a better understanding of the dynamics of fake news across different contexts.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"The study used tablet questionnaires during face-to-face interviews.The main task of the study was to investigate the effectiveness of fact-checking corrections in reducing beliefs in fake news during the 2018 Brazilian elections. The study presented existing and widespread false rumors to respondents and then experimentally manipulated whether respondents received third-party fact-checking information correcting the rumor or no correction at all. The study used a total of six false rumors, four of which were political and two of which were nonpolitical. The four political rumors included information about the PT and its politicians.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Six false rumors (four political and two nonpolitical) that had been fact-checked by various agencies in the country",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Dogmatism, disengagement, political interest, education, and other socioeconomic variables.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study exposed participants to common false rumors and tested reactions provided with or without third-party fact-checking information. Corrective information was provided. The study assessed participants' beliefs regarding the story."],"N":[2236],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Fact-checking corrections did not lead to a significant reduction in misinformation beliefs, regardless of whether they corresponded to political or nonpolitical rumors."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"34c85ead7e9b8215a9a6c551e819a07c","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Badrinathan, S., & Chauchard, S. (2023). I Don't Think That’s True, Bro!” Social Corrections of Misinformation in India. The International Journal of Press/Politics, 0(0)."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/19401612231158770"},"children":["https://doi.org/10.1177/19401612231158770"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.17605/OSF.IO/CKJBE"},"children":["https://doi.org/10.17605/OSF.IO/CKJBE"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Fact-checks and corrections of falsehoods have emerged as effective ways to counter misinformation online. But in contexts with encrypted messaging applications (EMAs), corrections must necessarily emanate from peers. Are such social corrections effective? If so, how substantiated do corrective messages need to be? To answer these questions, we evaluate the effect of different types of social corrections on the persistence of misinformation in India (𝑁≈5,100). Using an online experiment, we show that social corrections substantially reduce beliefs in misinformation, including in beliefs deeply anchored in salient group identities. Importantly, these positive effects are not systematically attenuated by partisan motivated reasoning, highlighting a striking difference from Western contexts. We also find that the presence of a correction matters more relative to how sophisticated this correction is: substantiating a correction with a source only improves its effect in a minority of cases; besides, when social corrections are effective, citing a source does not drastically improve the size of their effect. These results have implications for both users and platforms and speak to countering misinformation in developing countries that rely on private messaging apps.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants complete baseline survey, following which they are shown screenshots of a WhatsApp thread featuring a controversial claim followed by corrective messages. The features of the corrective message are varied in line with treatment conditions. There were four types of corrective messages: (1) no correction (2) random-guy correction (3) authority/domain expert correction (4) fact-checked correction.\nRespondents were randomized into one of these four groups with equal probability. The between-subject randomization of participants into Control, Domain Expert, Fact Checker, and Unsubstantiated Correction re-occurs prior to each of the nine successive screenshots respondents are exposed to.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"WhatsApp chat screenshots about political, health, and social claims. Respondents evaluate 9 (7 false and 2 true) claims. Screenshots  varied along three factors: (1) source labels from congenial/dissonant/neutral media, (2) congenial/dissonant politician or passive tone making the claim (3) presence/source/sophistication of corrective message.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Baseline demographics, political attitudes, conspiratorial predispositions, and trust in various political and health institutions, accuracy judgments",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study presents a pooled model that averages across all rumors to calculate an overall correction effect."],"N":[5104],"Effect size":["Bivariate OLS Coefficient: -0.2"],"Comments":["Corrections from various sources, such as experts and fact-checkers, significantly reduce overall rates of beliefs in false rumors."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"f31d44c055ebdcfb05c08e627e89c01b","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Porter, E., Velez, Y., & Wood, T. J. (2023). Correcting COVID-19 vaccine misinformation in 10 countries. Royal Society Open Science, 10(3), 221097."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1098/rsos.221097"},"children":["https://doi.org/10.1098/rsos.221097"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FOFPUAD."},"children":["https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FOFPUAD."]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"What can be done to reduce misperceptions about COVID-19 vaccines? We present results from experiments conducted simultaneously on YouGov samples in 10 countries (N = 10 600), which reveal that factual corrections consistently reduce false beliefs about vaccines. With results from these 10 countries, we find that exposure to corrections increases belief accuracy by 0.16 on a 4-point scale, while exposure to misinformation decreases belief accuracy by 0.09 on the same scale. We are unable to find evidence that either misinformation or factual corrections affect intent to vaccinate or vaccine attitudes. Our findings on effect duration are less conclusive; when we recontacted participants two weeks later, we observed 39% of the initial accuracy increase, yet this result narrowly misses conventional thresholds of statistical significance (p = 0.06). Taken together, our results illustrate both the possibilities and limitations of factual corrections. Evidence from 10 highly diverse populations shows that exposure to factual information reduces belief in falsehoods about vaccines, but has minimal influence on subsequent behaviours and attitudes.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"The experiments were conducted simultaneously on YouGov samples in 10 countries, where participants were then enrolled in three within-subjects trials to measure how exposure to corrections or misinformation increases belief accuracy. In each trial, participants could be assigned to a control condition, in which they answered only outcome questions; a misinformation condition, in which they were exposed to misinformation before answering outcome questions; or a misinformation and factual correction condition, in which they were exposed to misinformation and a factual correction before answering outcome questions. At the end of each trial, participants assessed the accuracy of the corresponding false claim. In a follow-up survey, participants answered the identical set of outcome questions they had answered in the prior wave.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Two of the three trials concerned identical global false claims (that Gates Foundation was set to make enormous profits from the COVID-19 vaccines, and that mRNA vaccines permanently alter DNA), while the remaining trial concerned a country-specific false claim.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographic, ideology, vaccination status, general attitudes toward vaccines and conspiracies, and main outcomes.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["Wave 1","Wave 2"],"Description":["Study experimentally tested whether exposure to corrections affects participants' belief accuracy.","Study experimentally tested whether exposure to corrections affects participants' belief accuracy."],"N":[10600,2460],"Effect size":["Meta-analytic estimate: 0.16","Meta-analytic estimate: 0.03"],"Comments":["Exposure to misinformation corrections increases belief accuracy by 0.16 on a 4-point scale, while exposure to misinformation decreases belief accuracy by 0.09 on the same scale.There is no significant evidence that misinformation and factual corrections affect the intent to vaccinate or views about vaccines.","The effect is reduced to 0.03 points in Wave 2, very narrowly missing the threshold for statistical significance (p= 0.06, two-tailed)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"8503c43327b3cda8fb81b57d1a5cf6e9","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Armand, A., Augsburg, B., Bancalari, A., & Kameshwara, K. K. (2021). Social proximity and misinformation: Experimental evidence from a mobile phone-based campaign in India.  CEPR Discussion Paper No. DP16492 (Preprint, not peer-reviewed)."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://repec.cepr.org/repec/cpr/ceprdp/DP16492.pdf"},"children":["https://repec.cepr.org/repec/cpr/ceprdp/DP16492.pdf"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"We study how social proximity between the sender and the receiver of information shapes the effectiveness of preventive health behaviour campaigns and the persistence of misinformation. We implement a field experiment among a representative sample of slum residents in two major Indian cities characterized by Hindu Muslim tensions. We show that informative messages are effective at improving evidence-based behavior, but not non-evidence-based behavior. These findings do not differ by social proximity, signalled by religion. However, when sender and receiver share the same religion, the intervention significantly reduces misinformation carrying in-group salience, highlighting the role of social proximity in fighting misinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Researchers draw a country-representative sample from a census of slum residents conducted in the second half of 2018 in the two largest urban agglomerations in the Indian State of Uttar Pradesh (UP), Lucknow and Kanpur. A baseline survey was conducted in June-July 2020 by reaching 3,991 households. Two waves of follow-up panel data were collected in October-November 2020, and December 2020-January 2021 (3.5 and 5.5 months after the baseline survey). Researchers randomly allocated residents to either a doctor message group or a control group. In the doctor message group, participants received pre-recorded voice messages containing an introduction by a local citizen, the sender, followed by statements from doctors of locally-renowned hospitals debunking common misconceptions about the virus and reminding about evidence-based policy recommendations.\nIn the control group, participants received instead a message with the same introduction, but followed by a Bollywood gossip.To vary social proximity, they randomized in both the doctor message and the control message the initial greeting of the sender to be either a Muslim or a Hindu greeting. This design creates exogenous variation in the religion concordance between the sender and the receiver of the message.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"They elicit the level of misinformation by measuring agreement with a sequence of randomly-ordered statements.\r\nThey distinguish between views that are not necessarily based on facts or knowledge (labeled as opinions), or incorrect views based on faulty knowledge or understanding (labeled as misconceptions).",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Survey outcomes: knowledge on how to prevent the virus, compliance with evidence-based recommendations, trust in information, fact-checking and beliefs. \n\nBehavioral outcomes: agreement with preventive practices against COVID-19, and comply with a randomly-asked World Health Organization's recommendation to protect from infection.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["Baseline survey + Wave 1 and 2 follow-up panel data"],"Description":["The study experimentally tested whether statements from doctors of locally-renowned hospitals debunking common misconceptions about the virus and reminding about evidence-based policy recommendations improve evidence-based behavior."],"N":[3991],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Agreement with face mask and hand-washing increased by 0.6 percentage points (0.75% above control). Compliance with policy recommendations rose significantly by 4 percentage points (7.09% above control). Need to verify information with others decreased by 2.3 percentage points (3.54% above control). The doctor's message reduced agreement with misconceptions by 0.8 percentage points."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"8113269eb979f3f1fa6b0cee1d3a40a7","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Hirshleifer, S.,  Naseem, M., Raza, A.A., & Rezaee, A. (2022). The spread of (mis)information: A social media experiment in Pakistan. CEGA Working Paper No. 213. (Preprint, not peer-reviewed)"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.26085/C3RW2T"},"children":["https://doi.org/10.26085/C3RW2T"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://www.socialscienceregistry.org/trials/9954"},"children":["https://www.socialscienceregistry.org/trials/9954"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"This randomized experiment on a social media platform in Pakistan measures the impact of two treatments that fully control access to misinformation relative to a more standard ex-post approach to controlling misinformation. In the first treatment, no misinformation was allowed on the platform, while in the second, it was allowed with an official rebuttal. The experiment focuses on misinformation regarding COVID-19 and is combined with an intervention to disseminate official information about the pandemic on the platform. Controlling misinformation through either treatment reduces platform usage. It also reduces exposure to official information by more than it reduces exposure to misinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"The study is implemented on Baang, a voice based social media platform where all users are anonymous and all posts are public. During the experiment period of 2 months, when users call in, they have the option to listen to most recent posts or most popular posts or official (accurate) posts about COVID-19. In the remove treatment, all posts containing COVID-19 misinformation were removed and never posted. In the sunshine treatment, all posts containing COVID-19 misinformation were allowed with a rebuttal played automatically right after the misinformation content, as an official responses from the platform. These two treatments are compared against a control condition that relies on ex-post community-based moderation. In the control, all user-created content was available immediately as it was posted, but users could tag messages as potential COVID-19 misinformation. These tagged posts were then sent to moderators to remove from the platform if identified as false.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"User generated COVID-19 misinformation posts, useful posts and official posts on a local voice based social media platform.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Exposure measured by number of minutes spent listening to a post, Engagement measured by number of times it is shared and a combined index of likes, comments, dislikes. Post- treatment survey on a subsample to elicit attitudes towards platform and other information sources.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["Study 1: Impact on exposure to and engagement with official information posts","Study 1: Impact on exposure to user-generated useful information","Study 1: Impact on exposure to misinformation"],"Description":["Examination of the effect of treatments on exposure to official information and engagement.","Effect of treatments on exposure to useful user-generated posts.","Effect of each treatment on exposure to misinformation."],"N":["Not Specified","Not Specified","Not Specified"],"Effect size":["Difference in means: -0.33","Difference in means: -0.14","Difference in means: -0.12"],"Comments":["Users assigned to the treatments listen to 0.33 (25%) fewer minutes of the official posts relative to users assigned the control.","Users assigned to either of the two treatments spend 0.14 (38%) less minutes listening to useful posts than in the control.","Users in the remove treatment are exposed to effectively zero misinformation, a decrease of 97% relative to the control."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"character","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"ff88c41bad12927dc536afe8ee8f6a08","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Debunking and Rebuttals"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Bruns, H., Dessart, F. J., Krawczyk, M. W., Lewandowsky, S., Pantazi, M., Pennycook, G., Schmid, P. & Smillie, L. (2023). The role of (trust in) the source of prebunks and debunks of misinformation. Evidence from online experiments in four EU countries. OSF.  (Preprint, not peer-reviewed)"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.31219/osf.io/vd5qt"},"children":["https://doi.org/10.31219/osf.io/vd5qt"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/vd5qt"},"children":["https://osf.io/vd5qt"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation surrounding crises poses a significant challenge for public institutions. Understanding the relative effectiveness of different types of interventions to counter misinformation and understanding which segments of the population are most or least receptive to them, is crucial. We conduct a preregistered online experiment involving 5,228 participants from Germany, Greece, Ireland, and Poland. Participants were exposed to misinformation on climate change or Covid - 19. In addition, they were preemptively exposed to a prebunk, warning them of commonly used misleading strategies, before encountering the misinformation, or a debunking intervention afterward. The source of the intervention (i.e. the European Commission ) was either revealed or not. Findings show that both interventions effectively change the four outcome variables in the desired direction in almost all cases, with debunks sometimes being more effective than prebunks. Moreover, revealing the source of the interventions does not significantly impact their overall effectiveness. Although one case of undesirable effect heterogeneity – debunks with revealed source were less effective in decreasing credibility of misinformation for people with low trust in the European Union – was observed, the results mostly suggest that the European Commission, and possibly other institutions, can confidently debunk and prebunk misinformation regardless of the trust level of its recipients.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"2 (intervention: prebunks vs. debunk) x 2 (intervention source: no source vs. European \nCommission) + 1  (control) between - subjects design.\nDebunking text design was informed by the Debunking Handbook.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Three Covid-19 misinformation and three climate change misinformation sourced from real claims found online; Debunking texts were designed in line with Debunking Handbook.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics; agreement with the misleading article's claim; credibility assessment of the misleading article; intentions to share the misleading article; trust in EU; individual perceptions of debunks and prebunks",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","1","1","1"],"Description":["Study experimentally tested effects of prebunking and debunking interventions on the agreement, credibility assessment and sharing intentions in regards to the misleading article’s claim","Study experimentally tested effects of prebunking and debunking interventions on the agreement, credibility assessment and sharing intentions in regards to the misleading article’s claim","Study experimentally tested effects of prebunking and debunking interventions on the agreement, credibility assessment and sharing intentions in regards to the misleading article’s claim","Study experimentally tested effects of prebunking and debunking interventions on the agreement, credibility assessment and sharing intentions in regards to the misleading article’s claim"],"N":[5228,5228,5228,5228],"Effect size":["Odds ratio: Neutral debunk: 0.51,  European Commission debunk: 0.48 (NA%-CI: Neutral debunk: 0.44, European Commission debunk: 0.41-Neutral debunk: 0.60, European Commission debunk: 0.56)","Linear OLS regression as linear estimates: Neutral debunk: -1.20, European Commission debunk: -1.22 (NA%-CI: Neutral debunk: -1.60,  European Commission debunk: -1.62-Neutral debunk:  -0.81,  European Commission debunk: -0.83)","Odds ratio: Neutral debunk: 0.54, European Commission debunk: 0.64 (NA%-CI: Neutral debunk: 0.43,  European Commission debunk: 0.51-Neutral debunk: -0.68,  European Commission debunk: -0.79)","Odds ratio: Neutral debunk: 1.19, European Commission debunk: 1.24 (NA%-CI: Neutral debunk: 0.98,  European Commission debunk: 1.02-Neutral debunk: 1.45,  European Commission debunk: 1.51)"]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"d91df5cea84dc604ec4ec38034885c63","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Friction"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Fazio, L. K. (2020). Pausing to consider why a headline is true or false can help reduce the sharing of false news.Harvard Kennedy School (HKS) Misinformation Review."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.37016/mr-2020-009"},"children":["https://doi.org/10.37016/mr-2020-009"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/mu7n8/"},"children":["https://osf.io/mu7n8/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"In an online experiment, participants who paused to explain why a headline was true or false indicated that they were less likely to share false information compared to control participants. Their intention to share accurate news stories was unchanged. These results indicate that adding “friction” (i.e., pausing to think) before sharing can improve the quality of information shared on social media.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants indicated how likely they would be to share true and false news headlines. Half of the participants were asked to explain how they knew that the headline was true or false before providing each rating. The experiment had a 2 (repetition: repeated, new) × 2 (task: control, explain) mixed design. Repetition was manipulated within-subjects; the participants’ task during the share phase was manipulated between-subjects.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"24 real political headlines from Pennycook, Cannon, and Rand (2018), available at  at https://osf.io/txf46/. Stimuli included an equal number of true/false and pro-democrat/pro-republican headlines. Across participants, researchers counterbalanced which set of 12 was repeated (presented during the exposure and share phase) and new (presented only during the share phase).",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Study experimentally tested whether asking participants to explain why a headline was true or false would affect their intentions to share true and false headlines."],"N":[501],"Effect size":["Cohen’s d: 0.27"],"Comments":["In the control condition, over half of the participants (57%) indicated that they would be “likely”, “somewhat likely” or “extremely likely” to share at least one false headline. However, in the explanation condition, only 39% indicated that they would be at least “likely” to share one or more false headlines. A similar decrease occurred in the number of people who indicated that they would be “extremely likely” to share at least one false headline (24% in control condition; 17% in explanation condition)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"9ae4ae9cc455bde527cb5db5fa6b8aba","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Friction"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Pillai, R. M. & Fazio, L. K. (2023). Explaining why headlines are true or false reduces intentions to share false information. Collabra: Psychology."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1525/collabra.87617"},"children":["https://doi.org/10.1525/collabra.87617"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/cns75/"},"children":["https://osf.io/cns75/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Recent years have seen a growing interest among academics and the public in ways to curb the spread of misinformation on social media. A recent experiment demonstrated that explanation prompts—simply asking people to explain why they think information is true or false—can reduce intentions to share false, but not true, political headlines on social media (Fazio, 2020). However, there is currently only one experiment demonstrating the benefits of this intervention, and this experiment manipulated the treatment between-subjects, raising concerns about differential attrition across the treatment and control groups over the course of the experiment. Thus, the present experiment (N = 499 US MTurkers) replicates Fazio (2020) in a within-subjects design, with all participants taking part in both the treatment and control conditions in two successive blocks. We replicate the effect of the intervention—explaining why headlines were true or false selectively reduced intentions to share false headlines. Our results also reveal that the longevity of the impact of these prompts is limited—encountering the explanation prompts did not reduce subsequent intentions to share false information when the explanation prompts were removed. Overall, our results suggest that encouraging people to pause and think about the truth of information can improve the quality of user-shared information on social media.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants indicated how likely they would be to share true and false news headlines. The experiment had a 2 (repetition: repeated, new) × 2 (task: control, explain) within-subjects design. Participants did two blocks of trials. For one of the two blocks participants were asked to explain how they knew that the headline was true or false before providing each rating.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"48 real political headlines available at https://osf.io/cns75/. Stimuli included an equal number of true/false and pro-democrat/pro-republican headlines. Across participants, researchers counterbalanced which set of 24 was repeated (presented during the exposure and share phase) and new (presented only during the share phase).",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Study experimentally tested whether asking participants to explain why a headline was true or false would affect their intentions to share true and false headlines."],"N":[499],"Effect size":["Cohen's d: 0.3 (95%-CI: -0.21-0.12)"],"Comments":["As predicted, asking participants to explain the accuracy of headlines reduced sharing intentions for false headlines, t(498) = -6.62, p < .001, d = 0.30, 95% CI of the difference [-0.21, -0.12], but this effect was not significant for true headlines, t(498) = 0.06, p = .952, d < 0.01, 95% CI of the difference [-0.06, 0.06]."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"c30a43d5dbb65691334f3bca9bba45fd","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["van der Linden, S., Leiserowitz, A., Rosenthal, S., & Maibach, E. (2017). Inoculating the public against misinformation about climate change. Global Challenges, 1(2), Article 1600008."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1002/gch2.201600008"},"children":["https://doi.org/10.1002/gch2.201600008"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Effectively addressing climate change requires significant changes in individual and collective human behavior and decision-making. Yet, in light of the increasing politicization of (climate) science, and the attempts of vested-interest groups to undermine the scientific consensus on climate change through organized “disinformation campaigns,” identifying ways to effectively engage with the public about the issue across the political spectrum has proven difficult. A growing body of research suggests that one promising way to counteract the politicization of science is to convey the high level of normative agreement (“consensus”) among experts about the reality of human-caused climate change. Yet, much prior research examining public opinion dynamics in the context of climate change has done so under conditions with limited external validity. Moreover, no research to date has examined how to protect the public from the spread of influential misinformation about climate change. The current research bridges this divide by exploring how people evaluate and process consensus cues in a polarized information environment. Furthermore, evidence is provided that it is possible to pre-emptively protect (“inoculate”) public attitudes about climate change against real-world misinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In Study 1 (N = 1,000), a persuasive myth about climate change was identified. In Study 2 (N = 2,167) Americans were randomly assigned to 1 of 6 experimental conditions—a pure control group, a facts-only condition, a misinformation-only condition, a false balance condition, a forewarning-only condition, and a full inoculation condition (forewarning + preemptive refutation)—before they were exposed to misinformation about climate change.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"97% scientific consensus (facts) and Oregon Global Warming Petition (misinformation)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Climate attitudes",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"This paper features 2 studies. We only include intervention Study 2 here. The purpose of Study 1 was to identify the most influential and representative “countermessages” used by climate change opponents.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["2"],"Description":["Study 2 experimentally tested  whether it is possible to “inoculate” people against climate misinformation."],"N":[2167],"Effect size":["Cohen’s d: 0.75"],"Comments":["The consensus-treatment (CT) alone elicited a large increase in perceived scientific agreement (d = 1.23 relative to control). In contrast, the (misinformation) countermessage (CM) had a substantial negative influence (d = 0.48) when presented on its own. When participants viewed the messages sequentially (CT | CM), the informational value of the consensus-treatment was negated completely (d = 0.04). As hypothesized, the general (In1 | CM) and detailed (In2 | CM) inoculation interventions were each successful in preserving much of the positive effect of the consensus message in the presence of counterinformation (d= 0.33 and 0.75 or one-third and two-thirds of the initial consensus-treatment effect, respectively)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"0d9990dc77982cf1dd8a315ab1f81c23","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Cook J., Lewandowsky, S., & Ecker, U. K. H. (2017) Neutralizing misinformation through inoculation: Exposing misleading argumentation techniques reduces their influence. PLOS ONE, 12(5), Article e0175799."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0175799"},"children":["https://doi.org/10.1371/journal.pone.0175799"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://datadryad.org/stash/dataset/doi:10.5061/dryad.f17j3"},"children":["https://datadryad.org/stash/dataset/doi:10.5061/dryad.f17j3"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation can undermine a well-functioning democracy. For example, public misconceptions about climate change can lead to lowered acceptance of the reality of climate change and lowered support for mitigation policies. This study experimentally explored the impact of misinformation about climate change and tested several pre-emptive interventions designed to reduce the influence of misinformation. We found that false-balance media coverage (giving contrarian views equal voice with climate scientists) lowered perceived consensus overall, although the effect was greater among free-market supporters. Likewise, misinformation that confuses people about the level of scientific agreement regarding anthropogenic global warming (AGW) had a polarizing effect, with free-market supporters reducing their acceptance of AGW and those with low free-market support increasing their acceptance of AGW. However, we found that inoculating messages that (1) explain the flawed argumentation technique used in the misinformation or that (2) highlight the scientific consensus on climate change were effective in neutralizing those adverse effects of misinformation. We recommend that climate communication messages should take into account ways in which scientific content can be distorted, and include pre-emptive inoculation messages.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Study 1 tested the effect of inoculation against misinformation that takes the form of \"false balance\" media coverage of climate change. Participants were randomly assigned to 1 of 5 groups: a control group or one of 4 groups that saw misinformation. For the 4 misinformation groups, consensus information and inoculation information were fully crossed so that prior to the misinformation, participants read consensus information, inoculation information, a message combining both consensus and inoculation information, or no message. Study 2 tested the impact of misinformation that explicitly seeks to manufacture doubt about the scientific consensus on climate change. It had a 2 × 2 between-subjects design, fully crossing a misinformation intervention and an inoculation intervention such that participants were divided into a control group (no intervention text), an inoculation group (inoculation with no misinformation), a misinformation group (misinformation with no inoculation), and an inoculation/misinformation group (inoculation preceding misinformation).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"1 mock news article featuring scientists presenting research supporting AGW and rejecting AGW with alternative explanations",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"AGW acceptance, free-market support, trust in climate scientists, trust in contrarian scientists, attribution of long-term climate trends to human activity, perceived consensus, and mitigative climate policy support.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Experiment 1 tested the effect of inoculation against misinformation that takes the form of ‘false balance’ media coverage regarding climate change.","Experiment 2 tested the impact of misinformation that explicitly seeks to manufacture doubt about the scientific consensus on climate change (fake experts). Experiment 2 also tested whether inoculating participants prior to reading misinformation was effective in neutralizing the influence of the misinformation."],"N":[751,400],"Effect size":["Eta squared: 0.01","Eta squared: 0.01"],"Comments":["Experiment 1 found that pre-emptively explaining the potentially misleading effect of false-balance media coverage was effective in neutralizing the negative influence of that type of misleading media coverage. Relative to perceived consensus in the control group (70%), Misinformation only: 63,5%, Consensus + Misinformation: 86%; Inoculation + Misinformation = 70%, Inoculation + Consensus + Misinformation = 84%","Experiment 2 demonstrated that misinformation—in the form of fake experts casting doubt on a scientific consensus—had a polarizing effect on climate attitudes, such that people with low free-market support increased climate acceptance, while people with high free-market support decreased climate acceptance. However, an inoculating message that explains the misinforming technique without mentioning any specifics fully neutralized the polarizing effect of misinformation. Relative to perceived consensus in the control group (54,5%), Misinformation only: 44,5%, Inoculation only: 50,4%; Inoculation + Misinformation:51,6%"]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"91bd00b296142a12e6525facca17570d","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Roozenbeek, J., & van der Linden, S. (2019). Fake news game confers psychological resistance against online misinformation. Palgrave Communications, 5(1),Article 65."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1057/s41599-019-0279-9"},"children":["https://doi.org/10.1057/s41599-019-0279-9"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://figshare.com/articles/dataset/Bad_News_Dataset/8269763"},"children":["https://figshare.com/articles/dataset/Bad_News_Dataset/8269763"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The spread of online misinformation poses serious challenges to societies worldwide. In a novel attempt to address this issue, we designed a psychological intervention in the form of an online browser game. In the game, players take on the role of a fake news producer and learn to master six documented techniques commonly used in the production of misinformation: polarisation, invoking emotions, spreading conspiracy theories, trolling people online, deflecting blame, and impersonating fake accounts. The game draws on an inoculation metaphor, where preemptively exposing, warning, and familiarising people with the strategies used in the production of fake news helps confer cognitive immunity when exposed to real misinformation. We conducted a large-scale evaluation of the game with N = 15,000 participants in a pre-post gameplay design. We provide initial evidence that people’s ability to spot and resist misinformation improves after gameplay, irrespective of education, age, political ideology, and cognitive style.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"One study (n = 14,163), within-subjects pre–post design. Participants were Bad News game players who took part in an in-game survey at the start and end of the game. Participants rated 6 items (2 \"real news,\" 4 \"fake news\") on a 1–7 reliability scale.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"2 true (non-manipulative and factual) and 4 misleading simulated Twitter posts",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"CRT (ball & bat question), age, gender, education, political ideology",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The treatment questions reflected a random sample of the strategies included in the game: impersonation, conspiracy, and discrediting.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This study experimentally tested whether learning common misinformation techniques through an inoculation game would impact people's recognition of these techniques as reflected in the judgements of headlines reliability."],"N":[14163],"Effect size":["Cohen’s d: 0.33"],"Comments":["The process of active inoculation through playing the Bad News game significantly reduced the perceived reliability of tweets that embedded several common online misinformation strategies. The observed effect sizes range from small to moderate, and are in line with the average effect-size in the context of research on resistance to persuasion."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"dea30e2badf1a8d07b42c3862495db09","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Roozenbeek, J., & van der Linden, S. (2020). Breaking Harmony Square: A game that “inoculates” against political misinformation. Harvard Kennedy School (HKS) Misinformation Review."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.37016/mr-2020-47"},"children":["https://doi.org/10.37016/mr-2020-47"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/r89h3/"},"children":["https://osf.io/r89h3/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"We present Harmony Square, a short, free-to-play online game in which players learn how political misinformation is produced and spread. We find that the game confers psychological resistance against manipulation techniques commonly used in political misinformation: players from around the world find social media content making use of these techniques significantly less reliable after playing, are more confident in their ability to spot such content, and less likely to report sharing it with others in their network.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"To test whether playing the Harmony Square game (1) reduces the perceived reliability of misinformation, (2) increases people's confidence in their assessment of the reliability of misinformation, and (3) reduces intentions to share misinformation with others, the study used a 2 (treatment, control) × 2 (pre, post) mixed-design RCT.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"16 simulated misleading social media posts containing manipulation techniques learned in the game (8 real fake news found on social media and fake news sites and 8 fictional fake news items created for the study). Topic: a mix of politically partisan and politically neutral content, political items included an equal number of right-leaning and left-leaning items.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Age, gender, education, political ideology, social media use, political interest, how often people check the news, reliability judgments of social media posts containing misinformation",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The purpose of the Harmony Square game was not to learn how to distinguish high-quality and low-quality content, but rather to teach people how to spot common types of misinformation on social media. For that reason, the studies tests only included manipulative (fake) items, both real and fictional.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study experimentally tested whether playing the Harmony Square game 1) reduces the perceived reliability of misinformation, 2) increases people's confidence in their assessment of the reliability of misinformation, and 3) reduces intentions to share misinformation with others."],"N":[681],"Effect size":["Cohen’s d: 0.51"],"Comments":["Reliability of real fake news (d = 0.51), experimenter-designed fake news (d = 0.54), confidence (d = 0.30, d = 0.30), and less sharing (d = 0.28, d = 0.27)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"de45fe188a74a010f93bd803d8e2bfd9","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Basol, M., Roozenbeek, J., Berriche, M., Uenal, F., McClanahan, W. P., & van der Linden, S. (2021). Towards psychological herd immunity: Cross-cultural evidence for two prebunking interventions against COVID-19 misinformation. Big Data & Society, 8(1), Article 20539517211013868."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/20539517211013868"},"children":["https://doi.org/10.1177/20539517211013868"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/mbqwj/"},"children":["https://osf.io/mbqwj/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation about the novel coronavirus (COVID-19) is a pressing societal challenge. Across two studies, one preregistered (n1 = 1771 and n2 = 1777), we assess the efficacy of two ‘prebunking’ interventions aimed at improving people’s ability to spot manipulation techniques commonly used in COVID-19 misinformation across three different languages (English, French and German). We find that Go Viral!, a novel five-minute browser game, (a) increases the perceived manipulativeness of misinformation about COVID-19, (b) improves people’s attitudinal certainty (confidence) in their ability to spot misinformation and (c) reduces self-reported willingness to share misinformation with others. The first two effects remain significant for at least one week after gameplay. We also find that reading real-world infographics from UNESCO improves people’s ability and confidence in spotting COVID-19 misinformation (albeit with descriptively smaller effect sizes than the game). Limitations and implications for fake news interventions are discussed.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Two studies for the Go Viral game:Study 1 implemented voluntary in-game pre–post test and Study 2 was an RCT with 3 conditions (Go Viral, UNESCO infographics, control) across 3 languages (English, German, French).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 1: 3 true and 3 misleading social media posts, Study 2: 9 true and 9 misleading social media posts (source: real - BBC, Reuters, AP, misleading - fact-checking websites)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Study 1: age, gender, education, political ideology, geographic origin. Study 2: age, gender, education, political ideology, COVID vaccination intentions, counterarguing, (motivational) threat measures.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2.  Go Viral! treatment condition","2. Infographics treatment condition"],"Description":["Study 1 experimentally tested whether playing the Go Viral! game will impact people's recognition of manipulativeness of misinformation about COVID-19.","This part of study 2 experimentally tested whether playing the Go Viral! game will impact people's recognition of manipulativeness of misinformation about COVID-19, as well as their confidence in their judgements and willingness to share misinformation.","This part of study 2 experimentally tested whether exposing people to prebunking Infographics will impact people's recognition of manipulativeness of misinformation about COVID-19, as well as their confidence in their judgements and willingness to share misinformation"],"N":[1771,1777,1777],"Effect size":["Cohen’s d: 0.52","Cohen’s d: 0.56","Cohen’s d: 0.17"],"Comments":["Participants, who played Go Viral!, irrespective of their demographic background (aside from political ideology), found misinformation about COVID-19 significantly more manipulative after playing than before, whereas their assessment of real news did not change in a meaningful sense. The effect sizes are in line with previous studies that have used similar designs.","Go Viral game compared to control condition: d=0.56 (manipulativeness), d=0.44 (confidence), d=0.15 (sharing). The game 1) reduces the perceived reliability of misinformation, 2) increases people's confidence in their assessment of the reliability of misinformation, and 3) reduces intentions to share misinformation with others.","Prebunking infographics compared to control condition: d=0.17 (manipulativeness), d=0.15 (confidence), sharing n.s. This indicates that reading through the UNESCO infographics significantly increases the perceived manipulativeness of COVID-19 misinformation, as well as confidence in their assessment of misinformation manipulativeness. However, for willingness to share there is no significant difference between the Infographics condition and the control group nor the Go Viral! condition. These results are similar (and significant) in all three countries."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"1338d658635ad707464ff363cad2f7d2","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Maertens, R., Roozenbeek, J., Basol, M., & van der Linden, S. (2021). Long-term effectiveness of inoculation against misinformation: Three longitudinal experiments. Journal of Experimental Psychology: Applied, 27(1), 1–16."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1037/xap0000315"},"children":["https://doi.org/10.1037/xap0000315"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/2dtkb/"},"children":["https://osf.io/2dtkb/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"This study investigates the long-term effectiveness of active psychological inoculation as a means to build resistance against misinformation. Using 3 longitudinal experiments (2 preregistered), we tested the effectiveness of Bad News, a real-world intervention in which participants develop resistance against misinformation through exposure to weakened doses of misinformation techniques. In 3 experiments (NExp1 = 151, NExp2 = 194, NExp3 = 170), participants played either Bad News (inoculation group) or Tetris (gamified control group) and rated the reliability of news headlines that either used a misinformation technique or not. We found that participants rate fake news as significantly less reliable after the intervention. In Experiment 1, we assessed participants at regular intervals to explore the longevity of this effect and found that the inoculation effect remains stable for at least 3 months. In Experiment 2, we sought to replicate these findings without regular testing and found significant decay over a 2-month time period so that the long-term inoculation effect was no longer significant. In Experiment 3, we replicated the inoculation effect and investigated whether long-term effects could be due to item-response memorization or the fake-to-real ratio of items presented, but found that this is not the case. We discuss implications for inoculation theory and psychological research on misinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Three longitudinal experiments (two preregistered) investigated the long-term effectiveness of gamified inoculation (operationalized as the Bad News Game). Participants played either Bad News (inoculation group) or Tetris (gamified control group) and rated the reliability of news headlines that either used a misinformation technique or did not. Online pre–post and between-groups experiment with the Bad News Game embedded into a Qualtrics survey, run through Prolific. Study 1: Follow-up after 1 week, 5 weeks, and 13 weeks. Study 2: Follow-up after 9 weeks. Study 3: Follow-up after 1 week.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 1, 2: 18 misleading posts and 3 non-misleading Twitter-like posts, Study 3: 6 misleading and 1 non-misleading (pretest) Twitter-like posts, 6 misleading and 6 non-misleading (posttest) Twitter-like posts (source: expert team, based on DEPICT framework)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Age, gender, political ideology (from 1–7, very left-wing to very right-wing), country of residence, first language, social media usage (from 1–5, never to daily), and a single-item cognitive reflection test (the bat and the ball)",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2","3"],"Description":["In Experiment 1, researchers aimed to test the hypothesis that inoculation effects are subject to decay in repeated measures over time. T1 = pretest; T2 = posttest (0 weeks); T3 = posttest (1 week); T4 = posttest (5 weeks); T5 = posttest (13 weeks).","In Experiment 2, researchers eliminated the confound of repeated measurement by removing all follow-ups between the direct posttest and the posttest 2 months later.","In Experiment was identical to Experiment 1 (up to T3, the first follow-up) but changed both the item set and fake-to-real ratio for the follow-up measure. It also omitted the control group."],"N":[118,110,87],"Effect size":["Cohen’s d: 1","Cohen’s d: 0.69","Cohen’s d: 0.72 (95%-CI: 0.48-0.95)"],"Comments":["The inoculation effects were significant over all 4 testing points: from the immediate post-test (d = -1.0) to 13 weeks delayed test). Researchers hypothesized that the repeated tests might have confounded the result as they could function as booster sessions or simply testing effects.","The inoculation effect decays over the course of 9 weeks, rendering the effect no longer significant. The analyses also show that the decay is only partial. Treatment effect at T2 (immediate post-test): d = 0.69. Between T2 and T3 (in 2 months): non-significant inoculation effect retention of 36%, d = −0.35.","The inoculation retention over a 1-week period was similar between the two experimental setups (Exp 1 and 3), thereby finding no evidence for item ratio or item set specific retention effects."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"6e62afd6b6d415f5cd763571614a89ec","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Roozenbeek, J., van der Linden, S., Goldberg, B., Rathje, S., & Lewandowsky, S. (2022). Psychological inoculation improves resilience against misinformation on social media. Science Advances, 8(34), Article eabo6254."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1126/sciadv.abo6254"},"children":["https://doi.org/10.1126/sciadv.abo6254"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/3769y/"},"children":["https://osf.io/3769y/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Online misinformation continues to have adverse consequences for society. Inoculation theory has been put forward as a way to reduce susceptibility to misinformation by informing people about how they might be misinformed, but its scalability has been elusive both at a theoretical level and a practical level. We developed five short videos that inoculate people against manipulation techniques commonly used in misinformation: emotionally manipulative language, incoherence, false dichotomies, scapegoating, and ad hominem attacks. In seven preregistered studies, i.e., six randomized controlled studies (n = 6464) and an ecologically valid field study on YouTube (n = 22,632), we find that these videos improve manipulation technique recognition, boost confidence in spotting these techniques, increase people’s ability to discern trustworthy from untrustworthy content, and improve the quality of their sharing decisions. These effects are robust across the political spectrum and a wide variety of covariates. We show that psychological inoculation campaigns on social media are effective at improving misinformation resilience at scale.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Studies 1–6: Participants were randomly assigned to either a treatment (inoculation video) or control (unrelated video) condition, after which outcome variables were assessed + covariates. \n\nStudy 7: YouTube users were shown an inoculation video as a YouTube ad. Viewers were shown a single-item multiple-choice survey question assessing manipulation technique recognition within the YouTube environment. A control group answered a survey question but did not see an inoculation video ad.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Studies 1-6: A series of 10 social media posts (Twitter or Facebook, anonymized) randomly either manipulative (i.e., making use of a manipulation technique learned about in the inoculation video) or neutral (i.e., not making use of a manipulation technique. Items were taken from real-world examples of manipulative content but slightly adapted to fit within the social media post format. We created separate item sets for each video. \n\nStudy 7: 6 manipulative headlines taken from Studies 1-6 (3 headlines per inoculation video, with 2 videos being run as YouTube ads).",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Age, gender, education, political ideology, news consumption, social media use, populism, analytical thinking (CRT), numeracy skills, \"bullshit\" receptivity, conspiracy belief, misinformation susceptibility (MIST), 10-item personality inventory (TIPI), actively open-minded thinking (AOT)",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"To our knowledge, this was the first large-scale field study of inoculation interventions on a social media platform (in our case YouTube).",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2","3","4","5","6","7"],"Description":["Study 1 tested whether a +- 1.5 minute inoculation video about emotional manipulation conferred resistance against the use of this manipulation technique in social media content.","Study 2 tested whether a +- 1.5 minute inoculation video about incoherence (mutually exclusive arguments) conferred resistance against the use of this manipulation technique in social media content.","Study 1 tested whether a +- 1.5 minute inoculation video about false dichotomies conferred resistance against the use of this manipulation technique in social media content.","Study 2 tested whether a +- 1.5 minute inoculation video about scapegoating conferred resistance against the use of this manipulation technique in social media content.","Study 5 tested whether a +- 1.5 minute inoculation video about ad hominem attacks conferred resistance against the use of this manipulation technique in social media content.","Study 6 sought to replicate Study 1, i.e., we tested whether a +- 1.5 minute inoculation video about emotional manipulation conferred resistance against the use of this manipulation technique in social media content. We also tested whether the order of presentation of the outcome measures (technique recognition, trustworthiness and sharing) interacted with the main effect.","Study 6 was conducted on YouTube. We sought to assess whether watching an inoculation video as a YouTube ad subsequently improved people's ability to identify manipulation techniques. We ran 2 of the videos (emotional language, Studies 1 and 6; and false dichotomies, study 3) as YouTube ads and asked each a single survey question, each containing a manipulative headline and asking participants to identify which manipulation technique is used. We administered a total of 6 items, 3 per video. The control group did not watch an inoculation video as a YouTube ad but did answer a survey question."],"N":[1072,1086,1095,1080,1083,1072,22632],"Effect size":["Cohen's d: 0.49","Cohen's d: 0.62","Cohen's d: 0.68","Cohen's d: 0.28","Cohen's d: 0.45","Cohen's d: 0.67","Cohen's h: 0.09"],"Comments":["Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.49), confidence in identifying manipulative content, (p < 0.001, d = 0.50), trustworthiness discernment (p < 0.001, d = 0.25), and sharing discernment (p < 0.001, d = 0.21).","Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.62), no significant effect on confidence in identifying manipulative content, (p = 0.471, d = 0.04), a significant effect on trustworthiness discernment (p = 0.002, d = 0.19), and no effect on sharing discernment (p = 0.109, d = 0.10).","Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.68), confidence in identifying manipulative content, (p < 0.001, d = 0.48), trustworthiness discernment (p < 0.001, d = 0.32), and sharing discernment (p < 0.001, d = 0.22).","Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.28), confidence in identifying manipulative content, (p < 0.001, d = 0.35), no significant effect on trustworthiness discernment (p = 0.100, d = 0.10), and no effect on sharing discernment (p = 0.067, d = 0.11).","Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.45), confidence in identifying manipulative content, (p < 0.001, d = 0.24), trustworthiness discernment (p = 0.002, d = 0.19), and sharing discernment (p < 0.001, d = 0.19).","Results showed a significant effect of inoculation on participants' ability to discern manipulative from non-manipulative content (p < 0.001, d = 0.67), trustworthiness discernment (p < 0.001, d = 0.44), and sharing discernment (p < 0.001, d = 0.34). The order of presentation under each item did not interact significantly with the main effect (p > 0.351).","Results showed a significant effect of watching an inoculation video on participants' ability to correctly identify a manipulation technique (Cohen's h = 0.09, p < 0.001)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"565db582f895d4e6445f3d5ddecf3e4e","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Lu, C., Hu, B., Li, Q., Bi, C., & Ju, X. D. (2023). Psychological Inoculation for Credibility Assessment, Sharing Intention, and Discernment of Misinformation: Systematic Review and Meta-Analysis. Journal of Medical Internet Research, 25, e49255."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.2196/49255"},"children":["https://doi.org/10.2196/49255"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/v8pxq/"},"children":["https://osf.io/v8pxq/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Background:The prevalence of misinformation poses a substantial threat to individuals' daily lives, necessitating the deployment of effective remedial approaches. One promising strategy is psychological inoculation, which preemptively immunizes individuals against misinformation attacks. However, uncertainties remain regarding the extent to which psychological inoculation effectively enhances the capacity to differentiate between misinformation and real information.\nObjective: To reduce the potential risk of misinformation about digital health, this study aims to examine the effectiveness of psychological inoculation in countering misinformation with a focus on several factors, including misinformation credibility assessment, real information credibility assessment, credibility discernment, misinformation sharing intention, real information sharing intention, and sharing discernment.\nMethods: Following the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines, we conducted a meta-analysis by searching 4 databases (Web of Science, APA PsycINFO, Proquest, and PubMed) for empirical studies based on inoculation theory and outcome measure-related misinformation published in the English language. Moderator analyses were used to examine the differences in intervention strategy, intervention type, theme, measurement time, team, and intervention design.\nResults: Based on 42 independent studies with 42,530 subjects, we found that psychological inoculation effectively reduces misinformation credibility assessment (d=-0.36, 95% CI -0.50 to -0.23; P<.001) and improves real information credibility assessment (d=0.20, 95% CI 0.06-0.33; P=.005) and real information sharing intention (d=0.09, 95% CI 0.03-0.16; P=.003). However, psychological inoculation does not significantly influence misinformation sharing intention (d=-0.35, 95% CI -0.79 to 0.09; P=.12). Additionally, we find that psychological inoculation effectively enhances credibility discernment (d=0.20, 95% CI 0.13-0.28; P<.001) and sharing discernment (d=0.18, 95% CI 0.12-0.24; P<.001). Regarding health misinformation, psychological inoculation effectively decreases misinformation credibility assessment and misinformation sharing intention. The results of the moderator analyses showed that content-based, passive inoculation was more effective in increasing credibility and sharing intention. The theme of climate change demonstrates a stronger effect on real information credibility. Comparing intervention types showed that pre-post interventions are more effective for misinformation credibility assessment, while post-only interventions are better for credibility discernment.\nConclusions: This study indicated that psychological inoculation enhanced individuals' ability to discern real information from misinformation and share real information. Incorporating psychological inoculation to cultivate an informed public is crucial for societal resilience against misinformation threats in an age of information proliferation. As a scalable and cost-effective intervention strategy, institutions can apply psychological inoculation to mitigate potential misinformation crises.\n",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Systematic review and meta-analysis of RCT interventions. Inclusion of 42 independent studies. Includes moderator analyses (design, type of inoculation etc).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Varied by study",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Varied by study",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["Meta-analysis variable 1","Meta-analysis variable 2","Meta-analysis variable 3","Meta-analysis variable 4","Meta-analysis variable 5","Meta-analysis variable 6"],"Description":["Misinformation Credibility Assessment","Real Information Credibility Assessment","Credibility Discernment","Misinformation Sharing Intention","Real Information Sharing Intention","Sharing Discernment"],"N":["31 studies out of 42","26 studies out of 42","12 studies out of 42","12 studies out of 42","11 studies out of 42","8 studies out of 42"],"Effect size":["Cohen's d: -0.36 (95%-CI: -0.5--0.23)","Cohen's d: 0.2 (95%-CI: 0.06-0.33)","Cohen's d: 0.2 (95%-CI: 0.13-0.28)","Cohen's d: -0.35 (95%-CI: -0.79-0.09)","Cohen's d: 0.09 (95%-CI: 0.03-0.16)","Cohen's d: 0.18 (95%-CI: 0.12-0.24)"],"Comments":["Psychological inoculation effectively reduced misinformation credibility. Inoculation effects for misinformation credibility assessment remain significant over time, but decrease after two weeks: Immediately d = –0.37 (–0.54 to –0.19); One week  d = –0.43 (–0.62 to –0.23); Two weeks or more d= –0.19 (–0.34 to –0.04).","Psychological inoculation effectively improved real information credibility.","Psychological inoculation effectively improved credibility discernment.","Psychological inoculation did not effectively reduce misinformation sharing intention.","Psychological inoculation effectively improved real information sharing intention.","Psychological inoculation effectively improved sharing discernment."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"character","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"fe4bd3de1bc58ec7415e7c12c8a9a1cb","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Wong, C. M. L., & Wu, Y. (2023). Limits to inoculating against the risk of fake news: a replication study in Singapore during COVID-19. Journal of Risk Research, 1-16."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1080/13669877.2023.2249909"},"children":["https://doi.org/10.1080/13669877.2023.2249909"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The COVID-19 pandemic laid bare the problem of fake news as one of the defining challenges of our time. The sudden proliferation of fake news and its direct impact on public health and safety led to increasing attention to pre-bunking interventions as a possible tool against the risks of fake news. These studies claimed that it is possible to use pre-emptive interventions such as games to induce cognitive resistance against the deception techniques deployed by fake new producers. We wanted to test if this method could be as effective in a non-Western context, and in an on-going catastrophic risk event. This paper presents the results of a replication study of Roozenbeek and van der Linden’s gaming experiment with certain modifications tailored to the case of Singapore in 2020 in the midst of the COVID-19 pandemic. We could not replicate the results of the original study. However, we found factors that could have accounted for the different results, including high levels of trust in English mainstream media and the government, and positive attitudes towards censorship. We also found that participants were most resistant against conspiratorial deception techniques but also more vulnerable to impersonation techniques. We reflect on what the results of our study say about the limitations of psychology-focused interventions and the need for a wider suite of interventions targeting different levels of analysis, including sociological factors and the risk context.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A pre-post test experimental design with random assignment to experimental and control groups. Participants in the treatment group were given a fake news test before and after playing the game, and a third test a few days later to see if the effects were sustained. The control group were given the pre- and post-game fake news tests consecutively without playing the game. Participants played alone or in groups of 2-4. Focused Group Discussions (FGDs) were conducted after each gaming session. \n",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"In each fake news test, participants were exposed to 10 stimuli and asked to respond whether the stimuli were ‘Mostly True’ or ‘Mostly False’. The items employed the five deception techniques were used in the game in order to assess changes in participants’ susceptibility to each deception technique.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Media reliance for news; trust in the government, media and COVID-19 vaccines;socio-demographic information",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"N":[122],"Effect size":["Cohen's f: 0.09"],"Comments":["The treatment condition (i.e. playing the fake news game) did not have a significant effect on people’s ability to identify fake news compared to the control group (F(1, 112) = 0.09, p = 0.76)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"4d0f2e26bcf58b2e36a893e602a006d9","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Pereira, F. B., Bueno, N. S., Nunes, F., & Pavão, N. (2023). Inoculation Reduces Misinformation: Experimental Evidence from Multidimensional Interventions in Brazil. Journal of Experimental Political Science, 1-12."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1017/XPS.2023.11"},"children":["https://doi.org/10.1017/XPS.2023.11"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.7910/DVN/ZHWIWG"},"children":["https://doi.org/10.7910/DVN/ZHWIWG"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation is widely seen as a threat to democracy that should be promptly addressed by scholars, journalists, and policymakers. However, some of the debated solutions are either controversial (internet platform regulation) or may be difficult and costly to implement in many settings (fact-checking corrections). This study investigates the effectiveness of preemptive interventions, a type of solution that has received considerably less attention in this debate. Studies show that interventions through awareness and media literacy campaigns can inoculate citizens against misinformation, but these interventions are restricted to a few contexts and settings. Our paper uses two field experiments, one of which was conducted in partnership with Brazil’s main newspaper, to investigate the effectiveness of multidimensional interventions against misinformation in São Paulo. The findings show that preemptive interventions can indeed reduce rumor acceptance and provide insights into the strategies to combat misinformation in democracies.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Study 1: Respondents completed a questionnaire, then were randomly assigned to either a treatment group, receiving an intervention aimed at reducing rumor acceptance, or a control group. The intervention included a free 3-month subscription to Folha de São Paulo, providing access to professional news coverage and fact-checking tools, and an email a week later with a message about fake news, a link to a fact-checking article, and an 8-step guide for spotting fake news. The control group received no intervention. Study 2 mirrored Study 1, but without the newspaper subscription.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"The researchers presented participants with four false rumors and repeated one rumor in different waves of the survey.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Newspaper subscription, political knowledge, trust in the media, political interest, and support for the President.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["A two-wave online survey combined with an experimental intervention conducted during the 2020 mayoral elections in São Paulo, Brazil.","A two-wave online survey combined with a similar experimental intervention conducted during the early months of 2022 in São Paulo."],"N":[1000,1037],"Effect size":["The intent-to-treat estimates from OLS models: -0.12 points in the additive scale of rumor acceptance and -0.24 points in acceptance of the repeated rumor","(no standard effect size available/yet extracted)"],"Comments":["The intervention reduced on average .12 points in the additive scale of rumor acceptance and .24 points in acceptance of the repeated rumor.","The interventions did not significantly affect subjects’ acceptance of real news, but increased between 0.03 and 0.04 points the rejection of rumors. The treatment has a significant effect (both ITT and CACE) on the differences in acceptance between real and false stories."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"43908f8699c98f70b72154e727f4a0cf","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Armand, A., Fracchia, M., & Vicente, P. C. (2021). Let’s call! using the phone to increase acceptance of covid-19 vaccines (No. wp2113). Universidade Nova de Lisboa, Nova School of Business and Economics, NOVAFRICA."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://novafrica.org/wp-content/uploads/2021/10/2113_VF.pdf"},"children":["https://novafrica.org/wp-content/uploads/2021/10/2113_VF.pdf"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"N/A",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Vaccinating against COVID-19 is the main hope to end the current pandemic. We develop and test experimentally three phone-based cumulative interventions to increase COVID-19 vaccine acceptance in Mozambique. First, the provision of a simple positive message informing about these vaccines. Second, the activation of social memory on the country’s success in eradicating wild polio. Finally, the inoculation against fake news by developing among participants a critical view towards misleading information. We find that the combination of the three interventions increases COVID-19 vaccine acceptance and trust in institutions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Researchers randomly assigned respondents from the baseline survey to different treatments based on individual-level randomization, considering their region of residence (Maputo and Cabo Delgado), age group (under 40, 40-50, 50-60, over 60), and gender. The first treatment (labeled \"endorsement\") provides basic COVID-19 vaccine information and a positive endorsement. The second treatment (labeled \"social memory\") leverages the country's success in eradicating wild polio through vaccination to build trust in institutions. The third treatment (labeled \"inoculation\") involves structured interactions to raise awareness, foster critical thinking, and reduce the sharing of fake news. They measured treatment effects on COVID-19 vaccine acceptance and trust in institutions through a follow-up phone survey. The interventions were implemented cumulatively: a control group received no intervention (C), another group received information+endorsement (T1), an additional group received both information+endorsement and social memory interventions (T2), and a final group received information+endorsement, social memory, and inoculation interventions (T3).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Not applicable",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, household size, The baseline survey questionnaire included detailed questions about the respondents’ economic status, behaviors over the past seven days, attitudes towards a future vaccine, and perceptions about the government’s response to the pandemic. The endline survey questionnaire kept past questions and added a new section on trust in institutions, and several questions to account for the potential presence of social desirability bias.",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Longevity was not measured but there were 3 and 4 months in between the baseline and endline.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["T1: Endorsement","T2: Endorsement + Social memory","T3: Endorsement + Social memory + Inoculation"],"Description":["This study investigates the effect of interventions T1 (endorsement) on the acceptance of the COVID-19 vaccine and trust in institutions.","This study investigates the effect of interventions T2 (Endorsement + Social memory) on the acceptance of the COVID-19 vaccine and trust in institutions.","This study investigates the effect of interventions T2 (Endorsement + Social memory + Inoculation) on the acceptance of the COVID-19 vaccine and trust in institutions."],"N":[1419,1419,1419],"Effect size":["OLS regression estimate: acceptance:0.08; trust: 0,11","OLS regression estimate: acceptance:0.14; trust: 0,11","OLS regression estimate: acceptance:0.27; trust: 0,19"]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"1b1172aaaadaccfae339bb0d009c5b6c","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Garg, N. & Yadav, M., (2022). Learning to Resist Misinformation: A Field Experiment in India. (Preprint, not peer-reviewed)"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://assets.namangarg.net/papers/jmp_latest.pdf"},"children":["https://assets.namangarg.net/papers/jmp_latest.pdf"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.1257/rct.7923"},"children":["https://doi.org/10.1257/rct.7923"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Can we inoculate people against misinformation and mitigate its impact on people’s beliefs, attitudes, and behavior? We conduct a large field experiment in India with an intervention providing weekly digests containing a compilation of fact-checks of viral misinformation. In these digests, we also incorporate narrative explainers to give details and context of issues that are politically salient and consistent target of false stories. Specifically, we address misperceptions about Muslims increasingly fuelled by online misinformation. We find that familiarity with fact-checks increases people’s ability to correctly identify misinformation by eleven percentage points. However, belief in true news also decreases by four percentage points. We estimate a structural model to disentangle the two mechanisms of impact—truth discernment, which is the ability to correctly distinguish between false and true news; and skepticism, which changes the overall credulity for both false and true news. The impact is driven by an increase in both truth discernment and skepticism. Whereas skepticism increases immediately, it takes several weeks to become better at discerning truth. Finally, our intervention reduces misperceptions about Muslims, as well as leads to changes in policy attitudes and behavior. Treated individuals are less likely to support discriminatory policies and are more likely to pay for efforts to counter the harassment of interfaith couples.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Weekly digests with summaries of fact-checks of viral misinformation by certified fact-checkers delivered by a custom mobile app for nine weeks to treated individuals. Participants completed one baseline, two follow-up and one endline survey each three weeks apart. In the follow-up and endline surveys 8 statements were shown and respondents were asked to assess their accuracy.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"8 headlines were presented that varied across two dimensions: accuracy (mainstream/true versus false) and political valence (right versus left). All the true headlines were published by mainstream news sources within one month of the respective survey. All the false headlines circulated on social media within one month of the respective survey and were fact-checked and labeled as false by at least one third-party fact-checking website.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, CRT,  factual knowledge and policy attitudes about out-group, accuracy judgments",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"N":[1301],"Effect size":["Beta coefficient (b): 0.107 (false statements); -0.038 (true statements)"],"Comments":["Intervention increased the ability to identify misinformation by 11 percentage points, but also reduced beliefs in true statements by 4 percentage points."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"4656b0beac21b452f54fca2b2898a06c","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Bowles, J., Croke, K., Larreguy, H., Liu, S., & Marshall, J. (2023). Sustaining Exposure to Fact-checks: Misinformation Discernment, Media Consumption, and its Political Implications."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"http://dx.doi.org/10.2139/ssrn.4582703"},"children":["http://dx.doi.org/10.2139/ssrn.4582703"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Exposure to misinformation can affect citizens’ beliefs, political preferences, and compliance with government policies. However, little is known about how to reduce susceptibility to misinformation in a sustained manner outside controlled environments, particularly in the Global South. We evaluate an intervention in South Africa that encouraged individuals to consume biweekly fact-checks—as text messages or podcasts—via WhatsApp for six months. The intervention induced substantial consumption and internalization of fact-checks, while increasing participants’ ability to discern political and health misinformation upon exposure—especially when consumption was financially incentivized. Fact-checks that could be quickly consumed via short text messages or via podcasts with empathetic content were most impactful; short messages further increased government approval and compliance with COVID-19 policies. Conversely, we find limited effects on news consumption choices. Our results demonstrate the benefits of inducing sustained exposure to fact-checks, but highlight the difficulty of shifting broader media consumption patterns.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Twice a month for six months, treated participants were sent three fact-checks via WhatsApp messages. These fact-checks dissected largely-false stories that were trending on social media in South Africa. To encourage the consumption of the fact-checks, treated participants received monthly quizzes with financial incentives to correctly answer questions about the fact-checks or placebo quizzes containing questions about unrelated content. Control participants received placebo quizzes. Participants were randomly assigned to either a control group that received no fact-checks or one of four treatment conditions.\nIn one treatment condition, the fact checks were delivered via text, and in the other three, via podcasts. Participants who received fact-checks via podcasts received either a short version or a normal-length version.   Among the latter, some participants' podcasts also included empathetic content. All treated participants received the same three fact-checks via WhatsApp once every two weeks for six months.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"To measure the discernment of truth and false information, they used a set of two true news stories and two false news that were not subjected to the fact-checks distributed as part of the intervention. To measure the belief in conspiracy theories, they used a similar approach.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Participants' demographic characteristics, discernment of truth and false information and conspiracy theories; verification knowledge, trust in different media sources; consumption of different media sources, verification behavior, and information sharing behavior; and attitudes and self-reported behaviors relating to COVID-19 knowledge and preventative behavior and politics.\n",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study examined whether preemptive exposure to fact-checks increased respondents' ability to discern between true and false content."],"N":[4541],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Consumption of fact-checks increased by 0.65 standard deviations across pooled podcast treatment conditions. Podcast-treated participants became 36 percentage points more likely to report listening to the podcast relative to the control group. Treated respondents who received fact-check quiz incentives increased the number of questions relating to fact-check content that they answered correctly by 0.41 standard deviations. Treated respondents with incentives to consume fact-checks became 0.2 standard deviations more likely to subscribe to Africa Check's content. Any treatment with fact-check quiz incentives increased respondents' discernment between true and false information by 0.06 standard deviations, skepticism of conspiracy theories by 0.1 standard deviations, and information verification knowledge by 0.1 standard deviations. The treatments incentivizing participants to consume fact-checks reduced trust in social media platforms by 0.09 standard deviations. There is no statistically significant effects on information consumption and verification behavior. Any treatment with fact-check quiz incentives reduced sharing of information received via social media by around 0.1 standard deviations. Attitudes and behaviors relating to COVID-19 and government fact-checks delivered by text messages increased an index of COVID-19 knowledge and preventative behavior by 0.14 standard deviations and an index of incumbent government performance by 0.07 standard deviations."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"5cc5b874fe8635a906f1c5f806ad7618","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Inoculation"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Athey, S. C., Matias , C., Koutout, K., Kristine & Li, Z. (2023). Emotion- versus Reasoning-Based Drivers of Misinformation Sharing: A Field Experiment Using Text Message Courses in Kenya. Stanford University Graduate School of Business Research Paper No. 4489759. SSRN (Preprint, not peer-reviewed)"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"http://dx.doi.org/10.2139/ssrn.4489759"},"children":["http://dx.doi.org/10.2139/ssrn.4489759"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://www.socialscienceregistry.org/trials/9721"},"children":["https://www.socialscienceregistry.org/trials/9721"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Two leading hypotheses for why individuals unintentionally share misleading information online are that 1) they are unable to recognize that a post contains misinformation, and 2) they make impulsive, emotional sharing decisions without thinking about whether a post contains misinformation. The strategies to counter each of these drivers of misinformation sharing differ by the techniques that they are designed to address. We categorize techniques according to whether they use misleading reasoning to make recognizing misinformation more difficult (reasoning-based) or manipulate emotions to encourage impulsive sharing decisions (emotions-based). To learn whether interventions designed to counter reasoning- or emotion-based techniques are more effective or whether the approaches are complementary, we evaluate three distinct versions of a low-cost and scalable five-day text message educational course. We assess the impact of the courses in a field experiment with approximately 9,000 participants in Kenya. We measure outcomes using a pre-post survey design that elicits intentions to share and find that all treatment courses work, decreasing misinformation sharing 28% on average relative to no text message course. The treatment designed to counter emotion-based techniques, the “Emotions” course, is more effective than teaching about reasoning-based techniques either alone in the “Reasoning” course or in combination with emotion-based techniques in the “Combo” course. Moreover, the Emotions course performs best on misinformation posts that use emotional manipulation, and does no worse than the Reasoning or Combo courses on misinformation posts that use reasoning- based techniques. In a follow-up experiment approximately two months later, 88% of the treatment effect of the three courses on misinformation sharing persists.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Three distinct versions of a low-cost, five-day text message course are evaluated. The course teaches users to counter misinformation deploying (i) reasoning-based techniques, (ii) emotion-based techniques, or (iii) both. Participants are randomized in one of three courses or one of two baseline (control) conditions. Course messages are delivered via MMS or WhatsApp. The “No-course” baseline receives no course between the pre- and post-survey; the “Facts” baseline is exposed to daily facts about misinformation but no technique-based education. The courses are evaluated with two online pre and post intervention surveys.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Ten posts containing true and false information shown per survey. One post used as an attention check and the remaining nine were drawn from a pool of 60 posts created by using 15 health related facts. For each fact 4 types of posts were created: baseline (non misinformation), \"Emotions\" based misinformation, \"Reasoning\" based misinformation and \"Combo\" of emotions and reasoning based misinformation. In both the pre- and post-survey, each participant saw three non-misinformation posts, two Emotions posts, two Reasoning posts, two Combo posts, and one attention check post, in random order, for a total of ten posts.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, reasons for sharing.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"N":[7688],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Relative to the No-course baseline sharing rate of 63.9 p.p., the treatment courses decrease the Sharing Rate by 18.1 p.p. (SE = 1.16 p.p.) or approximately 28%. Relative to the Facts baseline, the treatment course decreases the Sharing Rate by 10.4 p.p. (SE = 1.19 p.p.) or approximately 18% relative to the 56.3 p.p. sharing rate for the Facts baseline. The difference between the No-course and Facts baselines of 7.7 p.p (SE = 1.44 p.p.) shows that making misinformation salient to participants on a daily basis does have some beneficial impact on sharing behavior, but the treatment courses are providing educational content beyond this salience effect."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"0473a29b7fe967f9dad5e9b990ba16ab","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Wineburg, S., & McGrew, S. (2019). Lateral reading and the nature of expertise: Reading less and learning more when evaluating digital information. Teachers College Record, 121(11), 1–40.\n"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://eric.ed.gov/?id=EJ1262001"},"children":["https://eric.ed.gov/?id=EJ1262001"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The Internet has democratized access to information but in so doing has opened the floodgates to misinformation, fake news, and rank propaganda masquerading as dispassionate analysis. Despite mounting attention to the problem of online misinformation and growing agreement that digital literacy efforts are important, prior research offers few concrete ideas about what skilled evaluations look like. Purpose/Objective/Research Question/Focus of Study: Our purpose in this study was to seek out those who are skilled in online evaluations in order to understand how their strategies and approaches to evaluating digital content might inform educational efforts. We sampled 45 experienced users of the Internet: 10 Ph.D. historians, 10 professional fact checkers, and 25 Stanford University undergraduates. Analysis focused on the strategies participants used to evaluate online information and arrive at judgments of credibility. Research Design: In this expert/novice study, participants thought aloud as they evaluated live websites and searched for information on social and political issues such as bullying, minimum wage, and teacher tenure. We analyze and present findings from three of the tasks participants completed. Findings/Results: Historians and students often fell victim to easily manipulated features of websites, such as official-looking logos and domain names. They read vertically, staying within a website to evaluate its reliability. In contrast, fact checkers read laterally, leaving a site after a quick scan and opening up new browser tabs in order to judge the credibility of the original site. Compared to the other groups, fact checkers arrived at more warranted conclusions in a fraction of the time. Conclusions/Recommendations: We draw on insights gleaned from the fact checkers' practices to examine current curricular approaches to teaching web credibility as well as to suggest alternatives",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"An expert/novice study in which Stanford University undergraduates (n = 25), PhD historians (n = 10), and professional fact-checkers (n = 10) thought aloud as they evaluated websites and searched for information on social and political issues.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Live Internet sources addressing social and political issues",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This exploratory expert-novice study aimed to better understand the nature of expertise in the evaluation of online information."],"N":[45],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["For the first task (evaluating articles about bullying on two websites), Fact checkers had a perfect mean score of 2 (SD = 0); historians, 0.7 (SD = 0.95); and students, .16 (SD = 0.37). For the second task (an article at the minimum wage.com), fact-checkers’ conclusions averaged 3.3 (SD = .82) out of 4, versus historians’ average of 1.3 (SD = 1.4) and students’ .52 (SD = 1.16). For the third task (offline article on the court case Vergara v. California, task: researching the funding source), the fact checkers’ conclusions merited a 3.6 (SD = 0.70), versus historians’ 2.4 (SD = 1.3) and students’ 2.3 (SD = 1.5)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"206e1875b077e30eddad4e46471e276a","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["McGrew, S., Smith, M., Breakstone, J., Ortega, T., Wineburg, S. (2019). Improving university students’ web savvy: An intervention study. British Journal of Educational Psychology, 89(3), 485–500."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1111/bjep.12279"},"children":["https://doi.org/10.1111/bjep.12279"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Young people increasingly turn to the Internet for information about social and political issues. However, they struggle to evaluate the trustworthiness of the information they encounter online. This pilot study investigated whether a focused curricular intervention could improve university students’ ability to make sound judgements of credibility. Participants (n = 67) were students in four sections of a ‘critical thinking and writing’ course at a university on the West Coast of the United States. Course sections were randomly assigned to treatment (n = 29) and control conditions (n = 38). We conducted a pre-and-posttest, treatment/control experiment using a 2 × 2 × 2 design (treatment condition × order × time) with repeated measures on the last factor. Students in the treatment group received two 75-min lessons on evaluating the credibility of online content. An assessment of online reasoning was administered to students 6 weeks prior to the intervention and again 5 weeks after. Students in the treatment group were significantly more likely than students in the control group to have shown gains from pretest to posttest. Results suggest that teaching students a small number of flexible heuristics that can be applied across digital contexts can improve their evaluation of online sources.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A pre-and-posttest, treatment/control experiment using a 2 × 2 × 2 design (treatment condition × order × time) with repeated measures on the last factor. University students in the treatment group received two 75-minute lessons on evaluating the credibility of online content. An assessment of online reasoning was administered to students six weeks prior to the intervention and again five weeks after.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Pretest and posttest included 4 constructed response items asking to evaluate the credibility of online sources with a live Internet connection",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"evaluating articles, evaluating evidence",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This pilot study investigated whether a focused curricular intervention could improve university students’ ability to make sound judgements of credibility."],"N":[67],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Overall, students in the treatment group were over twice as likely (2.15 times) to score higher at posttest than at pretest, while students in the control condition were equally likely (1.00 times) to score higher at posttest than at pretest."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"80e3fd7a8149c2b0085b33ad33163f85","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["McGrew, S. (2020). Learning to evaluate: An intervention in civic online reasoning. Computers & Education, 145, Article 103711."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1016/j.compedu.2019.103711"},"children":["https://doi.org/10.1016/j.compedu.2019.103711"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Students turn to the Internet for information but often struggle to evaluate the trustworthiness of what they find. Teachers should help students develop effective evaluation strategies in order to ensure that students have access to reliable information on which to base decisions. This study reports on the results of an attempt to teach students to reason about online information. Students were taught strategies for evaluating digital content that were based on the practices of professional fact checkers. Eight lessons were devoted to teaching students strategies to effectively evaluate digital content. Pre- and posttests, each composed of four brief, constructed-response items, were administered to 68 11th-grade students who participated in the study. Students' scores improved significantly from pre-to posttest on three of the four tasks: students demonstrated an improved ability to investigate the source of a website, critique evidence, and locate reliable sources during an open Internet search. These results are promising and suggest that explicit instruction on fact-checking strategies may help students develop more effective online evaluation strategies.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A pre-and-posttest classroom intervention with 68 high school students that included 8 lessons on how to evaluate online information about political and social issues, taught approximately once per week over 2 months. All students received the same treatment and took the same outcome measures at pretest and posttest. The pretest and posttest forms included 4 constructed-response tasks. Pre-and-post forms were designed to be parallel.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Pretest and posttest included 4 brief, constructed-response items that were selected to assess a range civic online reasoning skills, including lateral reading.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Students’ ability to evaluate social and political information online",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This study investigated whether a focused curricular intervention could improve high school students’ online reasoning skills."],"N":[68],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["1. Ad Identification. Change from pre to post was not significant (Z = 1.44; p = .15). 2. Lateral Reading. Pre-to-post change was significant (Z =  3.59; p < .001). 3. Analyzing Evidence. Pre-to-post change was significant (Z =  6.23; p < .001). 4. Claim Research. Pre-to-post change was significant (Z = 3.77; p < .001)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"b9efa62029e60d1bfa47c2f122248291","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Donovan, A. M., & Rapp, D. N. (2020). Look it up: Online search reduces the problematic effects of exposures to inaccuracies. Memory & Cognition, 48(7), 1128–1145."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.3758/s13421-020-01047-z"},"children":["https://doi.org/10.3758/s13421-020-01047-z"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"People often reproduce information they read, which is beneficial when that information is accurate. Unfortunately, people are also often exposed to inaccurate information, with subsequent reproductions allowing for problematic decisions and behaviors. One empirically validated consequence of exposures to inaccuracies is that after reading falsehoods, participants are more likely to make errors answering related questions than if they previously read accurate statements, particularly for unfamiliar information. Interventions designed to attenuate these reproductions are often ineffective, at least as studied in tasks that restrict participants to generating answers based on text content and relevant prior knowledge. In the real world, however, people have access to outside resources to evaluate information. In three experiments, we tested whether affording the option to search for relevant online information following exposure to inaccurate statements would reduce reproductions of those inaccuracies on a post-reading task. Participants given the opportunity to search for information were less likely to reproduce inaccurate information and more likely to produce correct responses, in comparison to the performance of participants who were not allowed to search. We also tested whether warnings about potentially inaccurate information would encourage searches and inform responses. While warnings increased searching, additional reductions in inaccurate reproductions were not observed. Given the contingencies of many lab tasks, reproductions of inaccurate information might be overestimated. Resources available in the real world can offer useful supports for reducing the influence of and uncertainty associated with inaccurate exposures, consistent with contemporary accounts of memory and comprehension.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants read stories containing accurate, inaccurate, and neutral (i.e., unspecified) statements. Afterwards they completed a questionnaire with critical items relating to the statements. Studies 1 and 2 used a 3 (statement type: inaccurate, accurate, neutral) × 2 (item difficulty: easy, hard) × 2 (condition: search, no search) design.  Study 3 used a 3 (statement type: inaccurate, accurate, neutral) × 2 (item difficulty: easy, hard) × 2 (condition: warning, no warning) design.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"4 short fictional narratives of common, real-world events and topics adapted from Marsh (2004). Each of the stories contained 8 general knowledge statements for a total of 32 critical statements (accurate, inaccurate, or neutral; 8–12 statements of each type).",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Screen recordings (experiment 2), search behaviour self-report",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Note that Study 3 tested effectiveness of warnings (about the possible inclusion of inaccuracies in the stories as a means of motivating participants to consult online resources) and not of search and hence is not included here.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study 1 experimentally tested whether opportunities for online search would reduce inaccurate reproductions of information.","Study 2 experimentally tested whether opportunities for online search  would reduce inaccurate reproductions of information. The study was conducted in the lab."],"N":[231,96],"Effect size":["Partial-Eta squared (η2p): use of inaccurate statements: 0.05; use of accurate statements: 0.13","Partial-Eta squared (η2p): use of inaccurate statements: 0.14; use of accurate statements: 0.46"],"Comments":["Participants in the search condition (M = 5.17%, SD = 10.97) were less likely to reproduce inaccurate information than were participants in the no-search condition (M = 7.40%, SD = 15.10) [F (1, 214) = 11.01, MS = 0.83, p = .001, ηp2 = .05]. Participants in the search condition produced more correct responses (M = 72.37%, SD = 30.74) than did participants in the no-search condition (M = 59.30%, SD = 35.74), [F (1, 214) = 31.82, MS = 22.25, p < .001, ηp2 = .13].","Participants in the search condition (M = 4.30%, SD = 9.61) used inaccurate information to answer questions less often than did participants in the no-search condition (M = 7.52%, SD = 14.73), [F (1,95) = 14.94, MS = 0.72, p < .001, ηp2 =.14]."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"58f6cda16e70366b1b4fc3c63c2c735f","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Brodsky, J. E., Brooks, P. J., Scimeca, D., Todorova, R., Galati, P., Batson, M., Grosso, R., Matthews, M., Miller, V., & Caulfield, M. (2021). Improving college students’ fact checking strategies through lateral reading instruction in a general education civics course. Cognitive Research: Principles and Implications, 6(1), Article 23."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1186/s41235-021-00291-4"},"children":["https://doi.org/10.1186/s41235-021-00291-4"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/9rbkd/"},"children":["https://osf.io/9rbkd/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"College students lack fact-checking skills, which may lead them to accept information at face value. We report findings from an institution participating in the Digital Polarization Initiative (DPI), a national effort to teach students lateral reading strategies used by expert fact-checkers to verify online information. Lateral reading requires users to leave the information (website) to find out whether someone has already fact-checked the claim, identify the original source, or learn more about the individuals or organizations making the claim. Instructor-matched sections of a general education civics course implemented the DPI curriculum (N = 136 students) or provided business-as-usual civics instruction (N = 94 students). At posttest, students in DPI sections were more likely to use lateral reading to fact-check and correctly evaluate the trustworthiness of information than controls. Aligning with the DPI’s emphasis on using Wikipedia to investigate sources, students in DPI sections reported greater use of Wikipedia at posttest than controls, but did not differ significantly in their trust of Wikipedia. In DPI sections, students who failed to read laterally at posttest reported higher trust of Wikipedia at pretest than students who read at least one problem laterally. Responsiveness to the curriculum was also linked to numbers of online assignments attempted, but unrelated to pretest media literacy knowledge, use of lateral reading, or self-reported use of lateral reading. Further research is needed to determine whether improvements in lateral reading are maintained over time and to explore other factors that might distinguish students whose skills improved after instruction from non-responders.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A field experiment that tested the efficacy of teaching university students the Digital Polarization Initiative's 4 fact-checking moves with a pre-and-post, treatment/control curricular intervention. Students in the treatment condition (n = 136) were taught the 4 moves in 3 sessions of their introductory civics class and were provided opportunities to practice these moves with internet sources in 3 more classes 2 weeks later. Students in the control classes (n = 94) received standard instruction.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Two sets of lateral reading problems (A and B). Problems were adapted from the Stanford History Education Group’s Civic Online Reasoning curriculum and from Mike Caulfield's Four Moves blog. Students completed one of the lateral reading problem sets (A or B) as a pretest and the other problem set as a posttest. Set order was counterbalanced across instructors at pretest and posttest.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Other pre/post outcome measures (Likert-type rating scales):\n- Self-report on use of Wikipedia\n- Trust of Wikipedia\n\nA general media literacy questionnaire adapted from Ashley et al. (2013) that included 18 Likert-type “agree/disagree” items.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This field experiment tested the efficacy of teaching university students Digital Polarization Initiative's (DPI) four fact-checking strategies."],"N":[230],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["At posttest, students in DPI sections had an average score of M = 2.22 (SD = 0.92) across the four problems and received a score of 4 on an average of 1.07 problems (SD = 1.07). In contrast, students in control sections had an average score of M = 1.15 (SD = 0.30) and received a score of 4 on an average of 0.03 problems (SD = 0.23). For the self-reported lateral reading at posttest, there was a significant main effect of condition, F(1, 228) = 4.13, p = .043, ηp2 = 0.02, with students in the DPI sections reporting higher use of lateral reading (M = 3.45, SD = 0.84) than students in the control sections (M = 3.25, SD = 0.88). The interaction of time and condition was not significant, F(1, 228) = 1.06, p = .304, ηp2 = 0.01."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"5e77971c3b97b4c349cd01a89c351e99","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Breakstone, J., Smith, M., Connors, P., Ortega, T., Kerr, D., & Wineburg, S. (2021). Lateral reading: College students learn to critically evaluate internet sources in an online course. The Harvard Kennedy School Misinformation Review, 2(1)."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.37016/mr-2020-56"},"children":["https://doi.org/10.37016/mr-2020-56"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"N/A",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The COVID-19 pandemic has forced college students to spend more time online. Yet many studies show that college students struggle to discern fact from fiction on the Internet. A small body of research suggests that students in face-to-face settings can improve at judging the credibility of online sources. But what about asynchronous remote instruction? In an asynchronous college nutrition course at a large state university, we embedded modules that taught students how to vet websites using fact checkers’ strategies. Chief among these strategies was lateral reading, the act of leaving an unknown website to consult other sources to evaluate the original site. Students improved significantly from pretest to posttest, engaging in lateral reading more often post intervention. These findings inform efforts to scale this type of intervention in higher education.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"All participants in two sections of an asynchronous online course at a state university in the United States completed four 1-hour instructional models that were embedded in their standard coursework. Students completed a pretest at the start of the course and a posttest at the end. To reduce the risk of test order effects, two parallel forms of the outcome measure were administered in opposite order as pre/post tests for each section.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Two sets (A and B) of live internet sources addressing social and political issues",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Pretest and posttest were parallel forms of a 13-point assessment comprising nine items that asked students to evaluate the credibility of different types of online sources.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["A treatment-only, pre-and-post intervention in which students in an online, university-level nutrition course received instruction in fact-checking strategies for evaluating the credibility of online sources."],"N":[87],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Average scores improved from 3.95 points out of 13 at pretest to 7.08 at posttest, an average gain of 3.13 points. A repeated-measures ANOVA revealed that the gains from pretest to posttest were statistically significant, Meandiff = 3.13; F(1, 85) = 136.03, p < .001."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"11f38c3aff5484d1526b15a15d97ee3d","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Yang, S., Lee, J. W., Kim, H. J., Kang, M., Chong, E., & Kim, E. M. (2021). Can an online educational game contribute to developing information literate citizens? Computers & Education, 161, 104057."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1016/j.compedu.2020.104057"},"children":["https://doi.org/10.1016/j.compedu.2020.104057"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"This study began with concerns about mis-/disinformation in the new media environment as we sought ways to combat unreliable information. We developed an online educational game designed to enhance core media and information literacy competencies and to help young adults become information literate citizens. We examined the game’s effects on skepticism toward online information and information discernment skills, two of the most important MIL competencies. In an online experiment, a total of 210 participants between 20 and 29 years of age were randomly assigned to one of three groups: the game group that played the developed online game, the quiz group that completed a quiz with no game element, and the control group that did not receive any treatment. Before treatment, the participants’ perceived levels of intellectual civic skills were measured with a pre-questionnaire. After treatment, skepticism toward online information and information discernment skills as learning outcomes were measured with survey items on skepticism toward online information and an information discernment skills test, respectively. The results showed that the game effectively enhanced the cognitive aspect of media and information literacy (information discernment skills), but not the attitudinal aspect (skepticism toward online information). In addition, the game was found to be effective regardless of the participant’s perceived level of intellectual civic skills, while quiz with no game element was found to be effective only when the participant’s perceived level of intellectual civic skills was high. These findings demonstrate the importance of digital game-based intervention as an educational resource. We extend our discussion to the advantages and limitations of games in fostering learners’ abilities to evaluate online information.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A pre-posttest experimental design with random assignment to experimental and control groups. Participants were assigned to one of three groups: a game group that played the online game (The game provides examples of false and misleading online information and feedback to players' response to these messages), a quiz group that completed a quiz with no game element, and a control group that did not receive any treatment. Before treatment, the participants’ perceived levels of intellectual civic skills were measured. After treatment, skepticism toward online information and information discernment skills were measured.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"News articles and information composed in Korean",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Information discernment skills were measured by asking participants to evaluate four online news articles and information items. Participants evaluated each piece of information and chose the most appropriate reason for their response from of the four possible answer choices. Skepticism toward online information was measured using 10 self-report items on a five-point scale. Intellectual civic skills were measured as a moderating variable using 4 self-report items on a five-point scale",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["The experimental conditions did not have a significant effect on skepticism toward online information. Meanwhile, the results demonstrated that participants’ intellectual civic skills positively predicted their levels of skepticism toward online information, F(1, 204) = 9.12, p = 0.003. With regard to information discernment skills, there were significant differences among the experimental conditions, F(2, 204) = 4.62, p = 0.011. Pairwise comparisons revealed that participants in the game group showed statistically significant higher achievement compared to those in the control group (p = 0.003). On the other hand, the quiz group did not show difference compared to the control group (p = 0.267), and showed marginally significant difference to the game group (p = 0.064).The game was equally effective for the participants with various levels of perceived intellectual civic skills."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"ce3b6ce93a4d66f67f31239d49213d06","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Kobayashi, T., Taka, F., & Suzuki, T. (2021). Can “Googling” correct misbelief? Cognitive and affective consequences of online search. PLoS ONE, 16(9), e0256575."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0256575"},"children":["https://doi.org/10.1371/journal.pone.0256575"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GWISTQ"},"children":["https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GWISTQ"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"With increasing concern over online misinformation in perspective, this study experimentally examined the cognitive as well as the affective consequences of online search. Results of the two experiments using widely shared, prejudiced misinformation about an ethnic minority in Japan indicated that (a) online search reduces on average the likelihood of believing the misinformation, (b) the magnitude of the effect is larger among those who are predisposed to believe the misinformation, (c) cognitive correction is observed whether searchers are motivated to achieve a directional goal or an accuracy goal, and (d) online search deteriorates affective feeling toward the target groups of the misinformation. Theoretical implications are discussed in relation to the robustness of confirmation bias in online search and the “belief echo” in which exposure to negative misinformation continues to shape attitudes even after the misinformation has been effectively discredited.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Study 1: between subjects: 2 (statement type: discriminatory misinformation, unrelated control statement). Participants searched online to evaluate the statement as true or false. Search was bounded between a minimum of 5 and maximum of 10 minutes using any search engine.\nStudy 2: between subjects: 2 (statement type) x 3 (search goal: agreeableness, accuracy, no goal)",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"2 statements, one discriminatory about an ethnic minority, another about troubles in online transactions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"party identification, predisposition to believe the discriminatory misinformation, demographics",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Investigated the average treatment effect of online search on the likelihood of believing misinformation."],"N":[1032],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Online search reduces the likelihood of believing misinformation on average. 37.94% of the control group believed the misinformation to be true, falling to 25.62% in the treatment group."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"6af46f127ff1e0248133a4d474390371","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Wineburg, S., Breakstone, J., McGrew, S., Smith, M. D., & Ortega, T. (2022). Lateral reading on the open Internet: A district-wide field study in high school government classes. Journal of Educational Psychology, 114(5), 893–909."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1037/edu0000740"},"children":["https://doi.org/10.1037/edu0000740"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.1037/edu0000740.supp"},"children":["https://doi.org/10.1037/edu0000740.supp"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"In a study conducted across an urban school district, we tested a classroom-based intervention in which students were taught online evaluation strategies drawn from research with professional fact checkers. Students practiced the heuristic of lateral reading: leaving an unfamiliar website to search the open Web before investing attention in the site at hand. Professional development was provided to high school teachers who then implemented six 50-minute lessons in a district-mandated government course. Using a matched control design, students in treatment classrooms (n = 271) were compared to peers (n = 228) in regular classrooms. A multilevel linear mixed model showed that students in experimental classrooms grew significantly in their ability to judge the credibility of digital content. These findings inform efforts to prepare young people to make wise decisions about the information that darts across their screens",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In a field experiment conducted across all high schools in an urban district, researchers tested a classroom-based intervention in which students were taught lateral reading. Professional development was provided to teachers who then implemented six 50-minute lessons in a district-mandated government course. Using a matched-control design, students in treatment classrooms (n = 271) were compared to peers (n = 228) in regular classrooms. Schools (3 control, 3 treatment) were matched based on race/ethnicity and the percentage of students enrolled in the free/reduced lunch program. Matched schools were assigned to opposite conditions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Assessments including seven constructed response items designed to assess participants' judgements of credibility of online sources. The pretest and posttest used parallel forms.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Covariates included in the multilevel model: home language, ethnicity, race, gender, hours spend online per day, and frequency of checking the trustworthiness of online information.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The present study investigated whether high school students would improve as evaluators of online content on the open Web after completing six 50-minute lessons based on Civic Online Reasoning curriculum taught by their teachers."],"N":[499],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Students in the treatment condition (n = 271) were more likely to show improvement from pretest to posttest than control students (n = 228), Robust beta coefficient for condition x time(pre-post) = 1.66, SE = .44, t = 3.77, 95% CI (.44, 3.77)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"5ea579d9ea2f7f0900c40a5f6bf9afd2","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Panizza, F., Ronzani, P., Martini, C., Mattavelli, S., Morisseau, T., & Motterlini, M. (2022). Lateral reading and monetary incentives to spot disinformation about science. Scientific Reports, 12(1), Article 5678."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1038/s41598-022-09168-y"},"children":["https://doi.org/10.1038/s41598-022-09168-y"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/5zx9g/"},"children":["https://osf.io/5zx9g/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Disinformation about science can impose enormous economic and public health burdens. A recently proposed strategy to help online users recognise false content is to follow the techniques of professional fact checkers, such as looking for information on other websites (lateral reading) and looking beyond the first results suggested by search engines (click restraint). In two preregistered online experiments (N = 5387), we simulated a social media environment and tested two interventions, one in the form of a pop-up meant to advise participants to follow such techniques, the other based on monetary incentives. We measured participants’ ability to identify whether information was scientifically valid or invalid. Analysis of participants’ search style reveals that both monetary incentives and pop-up increased the use of fact-checking strategies. Monetary incentives were overall effective in increasing accuracy, whereas the pop-up worked when the source of information was unknown. Pop-up and incentives, when used together, produced a cumulative effect on accuracy. We suggest that monetary incentives enhance content relevance, and could be combined with fact-checking techniques to counteract disinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Study 1: Participants were randomly assigned to 1 of 3 experimental conditions: control, incentive, and pop-up. Study 2 had a between-subjects design with 2 factors, pop-up (present, absent) and monetary incentive (present, absent).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 1: Each participant observed one interactive, science-themed Facebook post from a set of 9 different Facebook posts varying in various properties, such as the scientific topic, the source reputation, and its level of factual reporting.\nStudy 2: 1 out of 6 interactive, science-themed Facebook posts from sources unknown to participants.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Search behaviour (tracked and self-report), content plausibility, trust in scientists, science literacy, conspiracy ideation",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"In Experiment 1, results were aggregated with posts from known sources (lateral reading is a heuristic to determine trustworthiness of unfamiliar sources).  Lateral reading is as effective as the strongest intervention (i.e. paying participants to be accurate), but that very few participants actually follow this strategy. The pop-up increases lateral reading and click restraint: +11% [95% CI: +7%, +17%]. Participants who use lateral reading and click restraint increase accuracy by +0.4 (on a scale from 1 to 6), \nbut: even when the pop-up is present, only 16% of participants (1/6 of the sample) reports using lateral reading, and only 11% reports adopting click restraint.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Experiment 1 tested separately the efficacy of pop-up (incl. lateral reading) and monetary incentives, and compared their effects to a control condition with no interventions.","Experiment 2 replicated the format of the first one, with two main modifications: 1) a pre-screening survey to identify lesser-known sources of information and only used those sources as the basis for the Facebook posts the participants were asked to evaluate; 2) added an experimental condition that included both incentive and pop-up interventions, to test the interaction between the two."],"N":[517,3003],"Effect size":["Common Language Effect Size: 0.54 (95%-CI: 0.49-0.58)","Common Language Effect Size: 0.52 (95%-CI: 0.5-0.54)"],"Comments":["In the first study (whole sample N = 2384), results showed a significant effect of incentive (β=0.293 [0.092, 0.494], z=3.225, p=0.003) and a lack of significance for the pop-up (β=−0.009 [−0.207,0.188], z=−0.103, p=.918). Technique adoption: both incentive and pop-up increased technique adoption ( incentive: β=1.042 [0.527, 1.556], z=4.728, p<0.001; pop-up: β=1.556 [1.065, 2.046], z=7.405, p<0.001), but that the increase was markedly higher with the presence of the pop-up than with monetary incentives (β=0.514 [0.157, 0.871], z=3.362, p<0.001). Participants were more likely to use lateral reading when the source was unknown (stimuli included known sources, such as BBC).","In the second study, results revealed a significant effect of pop-up on accuracy scores (β=0.137 [−0.018,0.292], z=2.115, p=0.034; Mixed-effects regression with errors clustered by post: p=0.052), but not on correct guessing (β=0.076 [−0.112,0.265], z=0.966, p=0.334). The combination of the two interventions significantly increased both accuracy indices compared to control (accuracy score: β=0.487 [0.268, 0.705], z=5.315, p<0.001; correct guessing: β=0.389 [0.123, 0.654], z=3.496, p<0.001), and that the contribution of incentive was greater than the contribution of pop-up (accuracy score: β=0.213 [−0.007,0.432], z=2.307, p=0.028; correct guessing: β=0.2362 [−0.032,0.504], z=2.103, p=0.047). According to the ordinal logistic regression model, the combination of the two interventions led to a 10.4% [5.4%,14.2%] increase in correct guessing, and a 6.9% [2.8%,12.4%] increase in  correct responses compared to control."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"5de32f69a607440e4bfb8f9d9db69cdb","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["McGrew, S., & Breakstone, J. (2023). Civic Online Reasoning across the curriculum: Developing and testing the efficacy of digital literacy lessons. AERA Open, 9."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/23328584231176451"},"children":["https://doi.org/10.1177/23328584231176451"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"N/A",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Given the current threat posed by toxic digital content, preparing students to evaluate online sources cannot be relegated to a single subject area—this instruction should happen across the curriculum. This article focuses on materials designed to teach students to evaluate online information across subject areas. ninth-grade biology and world geography teachers taught a series of curriculum-embedded lessons based on the following design principles: (a) Focus on a core question and strategy; (b) engage students in evaluating real online content; (c) feature cognitive apprenticeship and formative assessment; and (d) support teacher learning. We examine whether these lessons helped students become more skilled evaluators of online content. Pretest/posttest data (N =574) showed statistically significant growth in students’ ability to evaluate the credibility of online content. We analyze the role played by the curriculum design principles in this interdisciplinary intervention and explore implications for future initiatives.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A pre-and-post, treatment-only intervention. Civic Online Reasoning lessons were integrated into ninth grade biology and geography courses at a large suburban high school. ANOVA was used to measure the significance of the within-participant changes in scores from pretest to posttest. \nKey principles transmitted in the lessons: concentrating on a central question and strategy, involving students in evaluating actual online content, incorporating cognitive apprenticeship and formative assessment.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Assessment featuring six questions that asked students to evaluate the credibility of real internet sources",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Students completed a pretest at the beginning of the school year and a posttest at the end of the school year. There were two versions of the assessment, and Qualtrics randomly assigned students one of the two forms at pretest. At posttest, students completed the other form. The assessment included six items: three constructed-response questions and three multiple-choice questions. Each item asked students to evaluate real online sources. Although the curriculum featured examples connected to biology and geography, the pretest and the posttest included sources about social and political issues (e.g., climate change, gun control, student debt, foreign policy). This design provided evidence of whether students could evaluate the trustworthiness of Internet sources on a range of",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["A pre-and-post, treatment-only intervention in which Civic Online Reasoning lessons were integrated into ninth grade biology and geography courses at a large suburban high school in the United States. ANOVA was used to measure the significance of the within-participant changes in scores from pretest to posttest."],"N":[574],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["ANOVA revealed he difference in scores from pre to post was statistically significant (Mpre = 2.24, Mpost = 3.75; F (1,572) = 299.91, p < .001). Results also indicated significant main effects of time (ATStime = 301.88, p < .001) and class type (ATS class = 149.44, p < .001), which suggested that students’ scores were significantly different from pretest to posttest and that there were significant differences in scores across class type (general vs. honors). The results also indicate a significant interaction between class type and time (ATS class x time = 13.14, p < .001), which supports a rejection of the null hypothesis that students’ scores exhibited the same patterns from pretest to posttest across class type (general versus honors)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"61290128d11e3fb031c29cb280ee64d4","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Fendt, M., Nistor, N., Scheibenzuber, C., & Artmann, B. (2023). Sourcing against misinformation: Effects of a scalable lateral reading training based on cognitive apprenticeship. Computers in Human Behavior, 146, 107820."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1016/j.chb.2023.107820"},"children":["https://doi.org/10.1016/j.chb.2023.107820"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/hr5js/?view_only=138a6e4183014f338f900d714776a94f"},"children":["https://osf.io/hr5js/?view_only=138a6e4183014f338f900d714776a94f"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The pervasive problem of misinformation requires fostering Internet users' information literacy. Existing interventions raise the question of how the skills enabling the discernment of truthful vs. fake news can be efficiently trained, and which training approaches may be scalable. We conducted a lateral reading training based on cognitive apprenticeship and assessed its effects on N = 312 participants using an online news credibility test. The quantitative, 3 × 2 experimental design included the factors training type (no training vs. cognitive apprenticeship with written instructions vs. cognitive apprenticeship with human trainers) and trainer education (undergraduate freshmen vs. graduate students of education). Overall, we found significant effects of the training type and marginal effects of the trainer education, but no significant differences between cognitive apprenticeship with written instructions and cognitive apprenticeship with human trainers. The training effects pertained mainly to participants' fake news identification ability, whereas their truthful news evaluation was changed little. In sum, participants’ discernment of truthful vs. fake news was improved. Consequently, lateral reading training based on written instructions appears to be an effective and scalable media education intervention.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A lab-based study that employed a pre-post 3 × 2 factorial design. One factor was the treatment type, with three levels: critically dealing with online news (no training–control group); lateral reading training based on partial cognitive apprenticeship (written instructions groups); and lateral reading training based on fully developed cognitive apprenticeship (cognitive apprenticeship group). The other factor was trainer education, with two levels: some participants were managed and trained by freshmen; others by graduate students of educational sciences. The quantitative analysis included gender, age, education level, and political attitudes as covariates.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Three \"real\" and three \"fake\" articles from German online news sources that covered the Covid-19 pandemic.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Participants were shown six articles (three truthful, three fake) at pretest and posttest. Articles were different at pre and post. Participants rated each article on a Likert scale from 1 = absolutely not credible to 7 = absolutely credible.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["A lab-based experiment with at 3X2 factorial design that tested the efficacy of cognitive apprenticeship interventions on participants' abilities to identify 'truthful' and 'fake' news."],"N":[312],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Training effects on truthful news credibility: The within-subject effect was significant, F(1, 305) = 3.92, p = .049, partial η2 = 0.01 (small effect). The interaction effect datapoint*training type was marginally significant, F(2, 305) = 2.97, p = .053, partial η2 = 0.02 (small effect). There was also a significant interaction effect datapoint*political opinion, F(1, 305) = 7.06, p = .01, partial η2 = 0.02 (small effect). The between-subject effects of training type and trainer education were not significant. There was a significant covariate effect of political opinion, F(1, 305) = 17.53, p < .001, partial η2 = 0.05 (small to medium effect). Training effects on fake news credibility: The within-subject effect was not significant F(1, 305) = 2.62, p = .11. However, there were significant interaction effects datapoint*training type, F(2, 305) = 14.17, p < .001, partial η2 = 0.09 (medium effect) and datapoint*trainer education, F(2, 305) = 5.44, p = .02, partial η2 = 0.02 (small effect). The between-subject effect of training type was significant, F(2, 305) = 18.16, p < .001, partial η2 = 0.11 (medium to large effect). Multiple Bonferroni adjusted comparisons showed significant differences between the written instructions and the control group, p < .001, MDiff = − 0.45, 95%-CI = [0.18, 0.73], as well as between the cognitive apprenticeship and the control group, p < .001, MDiff = 0.64, 95%-CI = [0.38, 0.90]. The difference between the written instructions and the cognitive apprenticeship group was not significant (p > .05). The trainer education effect was not significant, F(1, 305) = 2.91, p = .09, partial η2 = 0.01. The covariate effects were significant for online news-related self confidence with F(1, 305) = 7.64, p = .01, partial η2 = 0.02 (small effect)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"c67caa8c287d932a26b9f459c49ad7d1","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Barzilai. S., Mor-Hagani S., Abed F., Tal-Savir D., Goldik N., Talmon I, Davidow O. (2023). Misinformation Is Contagious: Middle school students learn how to evaluate and share information responsibly through a digital game. Computers & Education, 202, 104832."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1016/j.compedu.2023.104832"},"children":["https://doi.org/10.1016/j.compedu.2023.104832"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Digital games have emerged as promising tools for countering the spread of misinformation online. Previous studies have mostly used games to inoculate players against misleading communication techniques. There has also been a lack of research on misinformation games in middle school. Hence, the aims of this investigation were to examine to what extent a game can support middle school students’ competence to evaluate online information and their dispositions to share information responsibly. For this purpose, we developed a game, Misinformation Is Contagious, that models reliable evaluation strategies and the social implications of sharing (in) accurate information. In two studies with 7th and 8th grade students (N = 84 and N = 131), we found that playing the misinformation game resulted in better accuracy discernment, sharing discernment, and metastrategic knowledge about corroboration, compared to playing a control language game. In Study 1, the effects on discernment scores were mainly due to higher ratings of accurate messages; whereas in Study 2, the effects were mainly due to lower ratings of inaccurate messages. In both studies, accuracy discernment mediated the effect of playing the misinformation game on sharing discernment. In Study 2, the misinformation game also had a direct effect on sharing discernment, suggesting it may have impacted players’ dispositions to value accuracy while sharing. However, the game did not affect students’ self-reported stances regarding sharing misinformation. These results provide initial evidence that a game designed to support evaluation strategies can help students resist misinformation and identify reliable information. The findings also suggest that games can potentially promote responsible information sharing.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In two controlled experiments, 7th and 8th grade students (N = 84 and N = 131) were randomly assigned to playing an online game called \"Misinformation Is Contagious\" that models evaluation strategies and the social implications of sharing (in)accurate information or to playing a control language game. Players must decide whether to share incoming messages, using provided tools that mimic evaluation strategies to check messages before sharing. After playing the games, participants responded to sharing intention, accuracy judgment, knowledge about misinformation evaluation strategies, and misinformation sharing stances measures.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Participants rated 12-14 short inaccurate and accurate messages about COVID-19 published on news websites or social media.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Participants rated the likelihood that they would share each message and the accuracy of each message using 7-point scales. Knowledge about misinformation evaluation strategies was measured using an open-ended prompt with coding of recurring responses. Self-reported stances regarding sharing misinformation were measured using 2-3 self-report items. Background measures included topic interest, information consumption, and sharing habits.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1&2, Variable: Accuracy discernement","1&2, Variable: Sharing discernment","1&2, Variable: Stances regarding sharing misinformation","1&2, Variable: Metastrategic knowledge about evaluation strategies"],"Description":["Study comparing accuracy discernment between players of a misinformation game and a control group.","Study comparing sharing discernment between players of a misinformation game and a control group.","Study comparing stances towards sharing COVID-19 misinformation between the experimental and control groups.","Study comparing the number of misinformation evaluation strategies described by players of a misinformation game and a control group."],"N":[215,215,215,215],"Effect size":["Cohen's d: 0.49, 0.41","Cohen's d: 0.46, 0.59","Cohen's d: 0.02, 0.18","Cohen's d: 0.35, 0.41"],"Comments":["Accuracy discernment was higher among players of the misinformation game.","Sharing discernment was higher among players of the misinformation game.","There were no significant differences in self-reported stances toward sharing COVID-19 misinformation between the experimental and control groups.","In study 1, the difference failed to achieve significance. In Study 2, players of the misinformation game described a signficantly higher number of strategies than control group participants."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"4e1e640633419b50b47cf954632fd661","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Apuke, O. D., & Gever, C. V. (2023). A quasi experiment on how the field of librarianship can help in combating fake news. The Journal of Academic Librarianship, 49(1), 102616"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1016/j.acalib.2022.102616"},"children":["https://doi.org/10.1016/j.acalib.2022.102616"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"This study experimented how fake news could be combated through social media literacy skills training. A quasi-experiment in one public university's library was conducted and 470 participants were randomly split into equal parts to form a control and experiment group. The respondents in the experiment group were exposed to 8 weeks of training to improve their social media literacy skills to fight fake news. We realised that social media literacy skills training is effective in increasing social media knowledge, users' recognition of fake news, tendency to verify information and reduce the inclination to share false news. Implications for research and practice were discussed.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"A pre-posttest experimental design with random assignment to experimental and control groups. Students in the experimental group participated in social media literacy skills training in which they were taught to in recognize misinformation and verify information on social media, dedicating two weeks (20 hours) to teach the significance of validating information on social media using methods like fact-checkers and additional sources. Students in the control group did not receive instruction.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Participants rated 20 news items which were fact-checked by www.fullfact.org. The items were presented in a Facebook post format and included 10 false and 10 accurate news items.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Participants evaluated the objectivity, professionalism (expertise), argument strength, and trustworthiness of each news item on a 5-point scale. False news recognition was based on the mean of the four items. Sharing intentions were measured using a single statement per news item. Knowledge about social media and verification were measured using a single self-report statement each.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","1","Table 5","Table 6"],"Description":["Pre- and post-intervention of participants' knowledge of social media.","Pre- and post-intervention of participants' ability to recognize false news.","Pre- and post-intervention of participants' fake news verification capability.","Pre- and post-intervention of participants likelihood of fake news sharing."],"N":[235,235,235,235],"Effect size":["t-Value: 8.84","t-Value: 7.23","t-Value: 6.43","t-Value: 8.43"],"Comments":["Participants in the treatment group reported a significant improvement in their knowledge of social media post-intervention.","The experimental group significantly outperformed the control group in identifying fake news post-intervention.","The treatment group's scores on their ability to verify information significantly improved post-intervention.","The experimental group scored better in their ability to distinguish real news from fake news post-intervention."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"7b39971360ad4c09d4ed22abae7c0059","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Lateral reading and verification strategies"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Resnick, P., Alfayez, A., Im, J., & Gilbert, E. (2021). Informed crowds can effectively identify misinformation. arXiv.  (Preprint, not peer-reviewed)"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.48550/arXiv.2108.07898"},"children":["https://doi.org/10.48550/arXiv.2108.07898"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Can crowd workers be trusted to judge whether news-like articles circulating on the Internet are wildly misleading, or does partisanship and inexperience get in the way? We assembled pools of both liberal and conservative crowd raters and tested three ways of asking them to make judgments about 374 articles. In a no research condition, they were just asked to view the article and then render a judgment. In an individual research condition, they were also asked to search for corroborating evidence and provide a link to the best evidence they found. In a collective research condition, they were not asked to search, but instead to look at links collected from workers in the individual research condition. The individual research condition reduced the partisanship of judgments. Moreover, the judgments of a panel of sixteen or more crowd workers were better than that of a panel of three expert journalists, as measured by alignment with a held out journalist's ratings. Without research, the crowd judgments were better than those of a single journalist, but not as good as the average of two journalists.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"3 conditions (no research control, individual research, collective research) × 3 respondent's ideology (liberal, conservative, moderate). None of the raters received explicit training. Both treatment conditions required raters to seek out or consider external evidence.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"368 articles (headline and lede), of which 207 that were flagged by Facebook algorithms as requiring fact-checking and 165 from the study by Godel et al, 2021, which consisted of most popular articles in several political and non-political categories.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"misinformation judgements, subjective opinion on enforcement action, prediction about other raters’ subjective opinions",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study investigated whether requirement to search for corroborating evidence either in a collective or individual research conditions would improve misinformation detection in lay people ratings (journalists' ratings were taken as a benchmark)."],"N":[1301],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Lay raters in the two research conditions correlate with a journalist better than do raters in the no research condition. The individual research condition has greater power than the collective research condition for large groups of lay raters. In the individual research condition, 15 lay raters were equivalent to 3 journalists; even 54 raters were not sufficient to achieve the same power as three journalists in the collective search condition and in the no research condition."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"0d870640151581616e3adf22bd7a41de","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Media-literacy tips"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Guess, A. M., Lerner, M., Lyons, B., Montgomery, J. M., Nyhan, B., Reifler, J., & Sircar, N. (2020). A digital media literacy intervention increases discernment between mainstream and false news in the United States and India. Proceedings of the National Academy of Sciences, 117(27), 15536–15545."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1073/pnas.1920498117"},"children":["https://doi.org/10.1073/pnas.1920498117"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.7910/DVN/Q5QINN"},"children":["https://doi.org/10.7910/DVN/Q5QINN"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Widespread belief in misinformation circulating online is a critical challenge for modern societies. While research to date has focused on psychological and political antecedents to this phenomenon, few studies have explored the role of digital media literacy shortfalls. Using data from preregistered survey experiments conducted around recent elections in the United States and India, we assess the effectiveness of an intervention modeled closely on the world’s largest media literacy campaign, which provided “tips” on how to spot false news to people in 14 countries. Our results indicate that exposure to this intervention reduced the perceived accuracy of both mainstream and false news headlines, but effects on the latter were significantly larger. As a result, the intervention improved discernment between mainstream and false news headlines among both a nationally representative sample in the United States (by 26.5%) and a highly educated online sample in India (by 17.5%). This increase in discernment remained measurable several weeks later in the United States (but not in India). However, we find no effects among a representative sample of respondents in a largely rural area of northern India, where rates of social media use are far lower.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Longitudinal RCTs, self-reported survey measures, Web tracking data",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Mock social media posts based on real stories",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Online untrustworthy news consumption",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Note that effect sizes are substantially larger for respondents who were successfully treated with the media literacy intervention. The reported effect sizes are for the Intention to treat.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1: U.S. online","2: India online","3: India face-to-face"],"Description":["Study 1 experimentally tested whether exposure to the media literacy intervention would cause a decrease in the perceived accuracy of false news articles.","Study 2 experimentally tested whether exposure to the media literacy intervention would cause a decrease in the perceived accuracy of false news articles.","Study 3 experimentally tested whether face-to-face exposure to the media literacy intervention would cause a decrease in the perceived accuracy of false news articles."],"N":[4907,3273,3744],"Effect size":["Cohen’s d: 0.2","Cohen’s d: 0.11","Cohen’s d: n.s."],"Comments":["The media literacy treatment significantly reduced beliefs in false news articles. Wave 1 of the US study: a decrease of nearly 0.2 points on a 4-point scale (intent to treat [ITT]: β=−0.196, SE=0.020; P<0.005). Wave 2 of the US study: While the effect is still present weeks later, its magnitude attenuates by more than half relative to wave 1 (ITT, β=−0.080 [SE=0.019], P<0.005; ATT, β=−0.121 [SE=0.028], P<0.005).","The media literacy treatment significantly reduced beliefs in false news articles (ITT: β=−0.126, SE = 0.026; P<0.005) in the first wave of a two-wave survey. The study found no statistically reliable evidence that the treatment affected headline accuracy ratings among wave 2 respondents in either India sample.","The study found no evidence that the media literacy treatment systematically affected beliefs in false news stories or discrimination between false and mainstream news among India face-to-face respondents."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"cd012d61c804dbb8b1b7180a7b41464f","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Media-literacy tips"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Badrinathan, S. (2021). Educative interventions to combat misinformation: Evidence from a field experiment in India. American Political Science Review, 115(4), 1325–1341."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1017/S0003055421000459"},"children":["https://doi.org/10.1017/S0003055421000459"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.7910/DVN/ITKNX5"},"children":["https://doi.org/10.7910/DVN/ITKNX5"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation makes democratic governance harder, especially in developing countries. Despite its real-world import, little is known about how to combat misinformation outside of the United States, particularly in places with low education, accelerating Internet access, and encrypted information sharing. This study uses a field experiment in India to test the efficacy of a pedagogical intervention on respondents’ ability to identify misinformation during the 2019 elections (N = 1,224). Treated respondents received hour-long in-person media literacy training in which enumerators discussed inoculation strategies, corrections, and the importance of verifying misinformation, all in a coherent learning module. Receiving this hour-long media literacy intervention did not significantly increase respondents’ ability to identify misinformation on average. However, treated respondents who support the ruling party became significantly less able to identify pro-attitudinal stories. These findings point to the resilience of misinformation in India and the presence of motivated reasoning in a traditionally nonideological party system.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants were randomized into 1 of 3 groups: 2 treatment and 1 placebo control. Participants in both treatment conditions received an hour-long in-person media literacy training session covering inoculation strategies, corrections, and the importance of verifying misinformation. They also received a demonstration of how to fact-check news stories and a flyer with tips on how to spot misinformation. One treatment group received corrections to 4false pro-BJP (Bharatiya Janata Party, a right-wing political party) stories; the other received corrections to 4 false anti-BJP stories. Apart from differences in the stories that were fact-checked, the tips on the flyer were the same for both treatment groups. Each treatment condition had an equal proportion of BJP and non-BJP partisans. Control group participants were shown a placebo demonstration about plastic pollution and received a flyer containing tips to reduce plastic usage. Trained enumerators administered the intervention in a household visit. Approximately 2 weeks after the intervention, participants were revisited to conduct an endline survey and measure the outcomes of interest.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"A series of 14 news stories, which varied in content, salience, and critically, partisan slant. Half of the stories were pro-BJP and the other half \nanti-BJP.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"BJP support, political knowledge, digital literacy, self-reported WhatsApp use, WhatsApp trust",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This study experimentally tested whether an hour-long media literacy intervention would increase ability to identify misinformation among respondents in Bihar, India."],"N":[1224],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["The study found no evidence that the intervention increased respondents' ability to identify misinformation, on average. While there was no average treatment effect, the interaction effect of the treatment on BJP partisans produces a negative effect on the ability to identify misinformation. For pro-BJP stories, the treatment effect for non-BJP supporters was 0.277, indicating that those who did not support the BJP and received the treatment identified an additional 0.277 stories. However, the treatment effect for BJP supporters was -0.135, indicating that those who supported the BJP and received the treatment identified 0.135 fewer stories."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"86fb3f7416fe961d567f9bdd9406bbfa","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Media-literacy tips"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Ali, A., & Qazi, I. A. (2023). Countering misinformation on social media through educational interventions: Evidence from a randomized experiment in Pakistan. Journal of Development Economics, 169,103108."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1016/j.jdeveco.2023.103108"},"children":["https://doi.org/10.1016/j.jdeveco.2023.103108"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://www.socialscienceregistry.org/trials/4003"},"children":["https://www.socialscienceregistry.org/trials/4003"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Fake news is a growing problem in developing countries with potentially far-reaching consequences. This paper evaluates the effectiveness of two educational interventions to counter misinformation among low digital literacy populations in urban Pakistan using a randomized experiment. We find no evidence that video-based general educational messages about misinformation have any statistically significant impact on the ability to correctly identify news items as true or fake. However, when such messages are augmented with personalized feedback based on individuals’ past engagement with fake news, we find an improvement of 3.3 percentage points or 4.5 percent in correctly identifying news (true or fake) relative to the control group. Our results suggest that educational interventions can enable information discernment but their effectiveness depends on how well their features and delivery are customized for the population of interest.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"In Treatment 1, participants were shown an informational video in the national language that educated them about common features of misinformation. In Treatment 2, participants were shown the video and then given personalized feedback about their own responses to fake news stories shown at baseline. The feedback highlighted the features of each fake news item that indicated the news was fake. Endline surveys were conducted after 1 week and then 4 to 6 weeks after the treatment was delivered. To assess the longevity and external validity of the treatments, researchers conducted a follow-up phone survey 15 months after the interventions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"6 true and 11 fake news headlines presented as screenshots across two endlines",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Age, gender, digital literacy",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study aimed to evaluate the effectiveness of two educational interventions to counter misinformation among low-digital literacy populations. Treatment one = educational video. Treatment 2 = video + personalized feedback."],"N":[750],"Effect size":["Average treatment effect: 3.3"],"Comments":["Treatment 1 does not have any significant impact. Participants who received treatment 2 were 5.6 percentage points (7.9%) more likely to correctly identify fake news relative to the control group. There is no significant effect on correctly identifying true news. As a result, the overall effect of treatment 2 on all news is 3.3 percentage points (4.5%), which points to improved discernment of news."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"c0c04a0f17a06e5f63494988c3dd682a","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Media-literacy tips"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Qian, S., Shen, C., & Zhang, J. (2023). Fighting cheapfakes: using a digital media literacy intervention to motivate reverse search of out-of-context visual misinformation. Journal of Computer-Mediated Communication, 28(1), zmac024."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1093/jcmc/zmac024"},"children":["https://doi.org/10.1093/jcmc/zmac024"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"As a significant source of misinformation, out-of-context visual misinformation refers to visuals presented in an unrelated context. This study explores whether a digital media literacy intervention that features reverse image search tools has significant effects on participants’ message credibility judgment, discernment of visual misinformation, and intention of using reverse image search tools. Data were collected from a pre-registered, web-based, between-subjects experiment (N = 905). Results revealed a significant difference on intention of using reverse search tools among three experimental conditions: active intervention that involved both knowledge and behavior, passive intervention that involved knowledge only, and a control condition. Specifically, active intervention significantly increased intention of using reverse search tools, compared to the passive intervention and the control. Neither active nor passive intervention had an effect on credibility judgment or misinformation discernment. We discuss the implications for future digital media intervention designs and journalism practice that aim to combat visual misinformation.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants were initially surveyed on digital media literacy, visual literacy, and trust in media. Following this, they were randomly assigned to one of three groups:Active Intervention Group: Participants were shown a digital media literacy education infographic and instructed to practice reverse image search using a provided image.Passive Intervention Group: Participants viewed the same infographic as the active group but without instructions on practicing reverse image search.Control Group: Participants read a travel-related infographic.After viewing the infographics, all participants were asked to evaluate four visual news posts taken from social media sites, two of which were accurately attributed and two were misattributed",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"The test stimuli used in the experiment were eight visual posts created based on four unedited images. Each visual post contained one of these images accompanied by a text caption. These captions were derived from fact-checking articles on Snopes.com and either accurately described the events captured in the image or described a completely unrelated scenario, making them out-of-context. The captions covered diverse sociopolitical issues. The visual posts were designed using a generic social media post format without platform logos to avoid biased processing. The source of the post was presented using gender-neutral names and object-centered images for profile pictures. The number of likes and comments were held constant across posts.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, Digital Media Literacy, Visual Literacy, and Trust in Media, time participants spent evaluating the visual posts",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","1","1"],"Description":["This study investigates the effects of active and passive interventions on the perceived credibility of visual posts and the intention to use the reverse image search in the future.","This study investigates whether digital media literacy would moderate the effects of intervention on discernment.","This study uses time spent on evaluating visual posts as a proxy measure to check if participants performed reverse image search."],"N":[880,834,894],"Effect size":["F-statistic and Beta coefficient: F (2, 877) = 32.52 for intention to use reverse image search, B = 0.75 for active intervention, B = 0.53 for passive intervention","F-statistic: F (2,831) = 0.994","F-statistic: F (2, 891) = 12.08"],"Comments":["Active and passive interventions significantly increased the intention to use the reverse image search in the future compared to the control group. Active intervention was more effective than passive intervention.","Digital media literacy did not moderate the relationship between intervention and discernment.","Participants spent significantly longer time in the active intervention group when evaluating the four visual posts than in the passive and the control group."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"d057f024607f22c9fc2eb5e887e68f75","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Source-credibility labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Kim, A., Moravec, P. L., & Dennis, A. R. (2019). Combating Fake News on Social Media with Source Ratings: The Effects of User and Expert Reputation Ratings, Journal of Management Information Systems, 36:3, 931-968."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1080/07421222.2019.1628921"},"children":["https://doi.org/10.1080/07421222.2019.1628921"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"As a remedy against fake news on social media, we examine the effectiveness of three different mechanisms for source ratings that can be applied to articles when they are initially published: expert rating (where expert reviewers fact-check articles, which are aggregated to provide a source rating), user article rating (where users rate articles, which are aggregated to provide a source rating), and user source rating (where users rate the sources themselves). We conducted two experiments and found that source ratings influenced social media users’ beliefs in the articles and that the rating mechanisms behind the ratings mattered. Low ratings, which would mark the usual culprits in spreading fake news, had stronger effects than did high ratings. When the ratings were low, users paid more attention to the rating mechanism, and, overall, expert ratings and user article ratings had stronger effects than did user source ratings. We also noticed a second order effect, where ratings on some sources led users to be more skeptical of sources without ratings, even with instructions to the contrary. A user’s belief in an article, in turn, influenced the extent to which users would engage with the article (e.g., read, like, comment and share). Lastly, we found confirmation bias to be prominent; users were more likely to believe — and spread — articles that aligned with their beliefs. Overall, our results show that source rating is a viable measure against fake news and propose how the rating mechanism should be\ndesigned.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"The study used a repeated measures design with a control group and three treatment groups. Participants were presented with a series of news headlines and asked to rate their credibility. The control group received two headlines with no reputation ratings, while the treatment groups received six headlines with either expert ratings, user article ratings, or user source ratings. The order of presentation was randomized to control for any headline-specific effects. The study also included a measure of confirmation bias and controlled for any order effects.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"News headlines, which were manipulated to be either left-leaning or right-leaning on the political spectrum. The headlines were presented with either user ratings or expert ratings.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Confirmation bias, demographics, Facebook use",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Investigates the effect of rating mechanisms on the believability of articles.","Further investigates the effect of rating mechanisms and confirmation bias on the believability of articles."],"N":[590,299],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["Ratings affected believability, with larger effects for low ratings than high ratings, and expert > user-article-based > user-source. Some smaller effects on reading/liking/sharing intentions..","Only strongest effects survive (a low expert or user-article-based rating reducing believability)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"c5f59102df7b34b2a658eefa3a538b46","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Source-credibility labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Aslett, K., Guess, A. M., Bonneau, R.,  Nagler, J., & Tucker, J. A. (2022). News credibility labels have limited average effects on news diet quality and fail to reduce misperceptions. Science advances, 8(18), eabl3844."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1126/sciadv.abl3844"},"children":["https://doi.org/10.1126/sciadv.abl3844"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://github.com/SMAPPNYU/Labels_Have_Limited_Effects"},"children":["https://github.com/SMAPPNYU/Labels_Have_Limited_Effects"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"As the primary arena for viral misinformation shifts toward transnational threats, the search continues for scalable countermeasures  compatible with principles of transparency and free expression. We conducted a randomized field experiment evaluating the impact of source credibility labels embedded in users’ social feeds and search results pages. By combining representative surveys (n = 3337) and digital trace data (n = 968) from a subset of respondents, we provide a rare ecologically valid test of such an intervention on both attitudes and behavior. On average across the sample, we are unable to detect changes in real-world  consumption of news from low-quality sources after 3 weeks. We can also  rule out small effects on perceived accuracy of popular misinformation spread about the Black Lives Matter movement and coronavirus disease 2019. However, we present suggestive evidence of a substantively meaningful increase in news diet quality among the heaviest consumers of  misinformation. We discuss the implications of our findings for scholars and practitioners.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"To measure the effect of these source labels, we fielded a two-wave online panel survey from 28 May to 30 June 2020 (wave 1: May 28 to June 9, n = 3862; wave 2: June 19 to 30, n = 3337) that included a randomized incentive to install the NewsGuard web extension at the beginning of the first wave. n addition to studying survey-based outcomes, we analyze linked digital trace data to measure the quality of news consumption of a subset of our  participants. We create five distinct measures of news diet quality (details can be found in Materials and Methods) over three time periods:  (i) the period before a respondent was assigned treatment in the wave 1  survey, (ii) the 3- to 4-week period from treatment assignment (May 28 to June 9) to June 30, and (iii) the nearly 2-week period from July 1 to 13.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Not applicable as the study analyzed respondents information diets (visits to URLs)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Perceived accuracy of true and false statements, trust in media, affective polarization",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"The majority of respondents in a survey of 968 people did not visit an unreliable news site during the 2-3 week pretreatment period. Less than 12% of the sample's news consumption was made up of at least 5% visits to unreliable news sites, according to NewsGuard. Furthermore, only 1.5% of respondents' news consumption had an average NewsGuard reliability score below 60, which is the threshold for reliability.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study focused on the impact of source reliability labels on the news consumption of online users."],"N":["968 for behavioral news consumption data collection"],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["The study found no significant effect of the source reliability labels on the quality of news consumption of online users. Even after the intervention, the news consumption patterns did not change significantly. However, a slight effect was observed among those who consume the most low-quality news. Relative to the average pretreatment value,  authors estimate a 5.4% increase in the treatment period and a 8.6% increase beginning July 1 in the average reliability score of news consumed."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"character","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"5370ccc459753f3faf12a60abb0e4846","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Source-credibility labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Shahid, F., Mare, S., & Vashistha, A., (2022). Examining Source Effects on Perceptions of Fake News in Rural India. Proc. ACM Hum.-Comput. Interact. 6, CSCW1, Article 89(April)."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1145/3512936"},"children":["https://doi.org/10.1145/3512936"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"This paper presents a between-subjects design experiment with 478 people in India to investigate how rural and urban social media users perceive credible and fake posts, and how different types of sources impact their perceptions of information credibility and sharing behaviors. Our findings reveal that: (1) rural social media users were less adept in differentiating between credible and fake posts than their urban counterparts, and (2) source effects on trust and sharing intent manifested differently for urban and rural users. For example, fake posts from family members garnered greater trust among urban users but were trusted the least by rural users. In case of sharing Facebook posts, urban users were more willing to share fake posts from family, whereas, rural users were more inclined to share fake posts from journalists. Drawing on these findings, we propose design interventions to counteract fake news in low-resource environments of the Global South.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"The study used an online survey on Qualtrics to collect data from 478 participants (159 rural, 319 urban) in India. Participants were randomly assigned to one of the seven source conditions: No source (baseline), Strangers, Friends, Family, Celebrity, Journalist, and News Media. In each experimental arm, participants were asked to review a set of Facebook posts shared by other users in the past six months (e.g., from friends, celebrities, or news media) based on the assigned source condition.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"COVID-related true and verified fake Facebook posts with one of seven source conditions. 6 true and 3 fake posts were used in main experiment.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Effect size":["(no standard effect size available/yet extracted)"]},"columns":[{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"f8494a1118808b25bb3010df5b188634","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Source-credibility labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Celadin, T., Capraro, V. ., Pennycook, G., & Rand, D. G. (2023). Displaying News Source Trustworthiness Ratings Reduces Sharing Intentions for False News Posts. Journal of Online Trust and Safety, 1(5)."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.54501/jots.v1i5.100"},"children":["https://doi.org/10.54501/jots.v1i5.100"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Professional fact-checking of individual news headlines is an effective way to fight misinformation, but it is not easily scalable, because it cannot keep pace with the massive speed at which news content gets posted on social media. Here we provide evidence for the effectiveness of ratings of news sources, instead of individual news articles. In a large pre-registered experiment with quota-sampled Americans, we find that participants are less likely to share false headlines (and are more discerning of true versus false headlines) when 1-to-5 star source trustworthiness ratings were applied to news headlines. This is true when the ratings are generated both by fact-checkers and by laypeople (although the effect is stronger using fact-checker ratings). We also observe a positive spillover effect: sharing discernment also increases for headlines whose source was not rated, likely because the presence of ratings on some headlines prompts users to reflect on source quality more generally. This study suggests that displaying information regarding the trustworthiness of news sources provides a scalable approach for reducing the spread of low-quality information.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants were shown 24 news headlines in Facebook format, 12 pro-democrat (6 true and 6 false) and 12 pro-republican (6 true and 6 false), in random order, from real-world sources (with or without x/5 stars source-credibility rating); then rated sharing intent.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"24 news headlines in Facebook format, 12 pro-democrat (6 true and 6 false) and 12 pro-republican (6 true and 6 false), from real-world sources",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Sharing intention",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Examination of the impact of fact-checker ratings on sharing of false headlines.","Examination of the impact of layperson ratings on sharing of false headlines."],"N":[1627,1627],"Effect size":["Beta coefficient: -0.08 (95%-CI: -0.12--0.05)","Beta coefficient: -0.04 (95%-CI: -0.08--0.01)"],"Comments":["Sharing of false headlines was significantly reduced by the fact-checker ratings.","Sharing of false headlines was significantly reduced by the layperson ratings."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"8098d40b15c54b2b2bd94848a31cafe0","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Social norms"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Cookson, D., Jolley, D., Dempsey, R. C., & Povey, R. (2021). A social norms approach intervention to address misperceptions of anti-vaccine conspiracy beliefs amongst UK parents. PLOS ONE, 16(11), Article e0258985."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1371/journal.pone.0258985"},"children":["https://doi.org/10.1371/journal.pone.0258985"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/rhb5p/"},"children":["https://osf.io/rhb5p/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Anti-vaccine conspiracy beliefs among parents can reduce vaccination intentions. Parents’ beliefs in anti-vaccine conspiracy theories are also related to their perceptions of other parents’ conspiracy beliefs. Further, research has shown that parents hold misperceptions of anti-vaccine conspiracy belief norms: UK parents overestimate the anti-vaccine conspiracy beliefs of other parents. The present study tested the effectiveness of a Social Norms Approach intervention, which corrects misperceptions using normative feedback, to reduce UK parents’ anti-vaccine conspiracy beliefs and increase vaccination intentions. At baseline, 202 UK parents of young children reported their personal belief in anti-vaccine conspiracy theories, future intentions to vaccinate, and their perceptions of other UK parents’ beliefs and intentions. Participants were then randomly assigned to a normative feedback condition (n = 89) or an assessment-only control condition (n = 113). The normative feedback compared participants’ personal anti-vaccine conspiracy beliefs and perceptions of other UK parents’ beliefs with actual normative belief levels. Parents receiving the normative feedback showed significantly reduced personal belief in anti-vaccine conspiracy beliefs at immediate post-test. As hypothesised, changes in normative perceptions of anti-vaccine conspiracy beliefs mediated the effect of the intervention. The intervention, did not directly increase vaccination intentions, however mediation analysis showed that the normative feedback increased perceptions of other parents’ vaccination intentions, which in turn increased personal vaccination intentions. No significant effects remained after a six-week follow-up. The current research demonstrates the potential utility of Social Norms Approach interventions for correcting misperceptions and reducing anti-vaccine conspiracy beliefs among UK parents. Further research could explore utilising a top-up intervention to maintain the efficacy.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"The study employed a 2 × 3 (intervention condition by time) mixed experimental design. Parents of young children reported on anti-vaccination conspiracy beliefs, then were randomly assigned to a normative feedback condition (revealing the actual norm amongst parents) or a control condition. Beliefs and intentions were assessed at 3 time points: baseline, immediately after the intervention (for the control condition, this was immediately after a 60-second delay), and a 6-week follow-up. Intervention: 1 page of normative feedback (a graph of personal belief against the norm amongst parents in the UK) along with an accompanying text.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"A scenario, in which participants are asked to imagine that they were the parent of an infant named Sophie, aged 8 months, and that their doctor had provided them with information regarding the (fictitious) disease ‘dysomeria’, which may lead to serious consequences with symptoms such as fever and vomiting.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Belief, intention to vaccinate, (change in) perceived norms",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study tested the effectiveness of a Social Norms intervention, which corrects misperceptions using normative feedback, to reduce UK parents’ anti-vaccine conspiracy beliefs and increase vaccination intentions."],"N":[202],"Effect size":["Partial eta squared (η2p): 0.03"],"Comments":["There was a significant interaction between time and condition on belief in anti-vaccine conspiracy theories, indicating the effectiveness of the intervention, F(1.56, 253.27) = 4.73, p = .016, ηp2 = .03. There was no difference in belief in anti-vaccine conspiracy theories from baseline to the six-week follow up."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"be23d223cb2310303bfcd69437014183","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Social norms"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Andı, S. & Akesson, J. (2021). Nudging away false news: Evidence from a social norms experiment. Digital Journalism, 9(1), 106–125."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1080/21670811.2020.1847674"},"children":["https://doi.org/10.1080/21670811.2020.1847674"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Many are concerned with the proliferation of false information on social media. This article explores whether “social norm-based nudges” can help address this issue by changing the sharing behaviour of social media users. In order to do so, we conduct an online survey experiment (n = 1,003), where participants are randomly exposed to a social norm-based message while choosing to read and/or share a false news article. The message warns participants that there is an abundance of “false information” online and tells them that most responsible people think twice before sharing articles with their network. Our analysis finds that the nudge reduced the proportion of people willing to share the article by 5.1 percentage points, with a 46.7% increase in the proportion of respondents stating that they do not want to share the article because it is false or inaccurate.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants read 1 of 3 fake news articles (2 right-leaning, 1 left-leaning) and were randomized to either a social norm or a control condition. The social norm condition combined descriptive and injunctive norms into a general warning that \"there is a lot of misinformation out there and most people think twice about sharing fake news with their friends and family.\"",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Three articles from false news sites that were active at the time of the experiment. Two of the articles were taken from conservative-leaning websites and one article was taken from a left-leaning website.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Willingness to share; motivation for sharing; understanding of the article's claims",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["The study experimentally tested whether  a social norm-based message would impact participants' willingness to share a false news article."],"N":[1004],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["5.17% reduction in willingness to share (SE = 0.02) or 27.5% reduction compared to control sharing (18.8%)"]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"edafdc7368afadbbfa3a0fa21c8dd9e8","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Social norms"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Ecker, U. K. H., Sanderson, J. A., McIlhiney, P., Rowsell, J. J., Quekett, H. L., Brown, G. D. A., & Lewandowsky, S. (2022). Combining refutations and social norms increases belief change. Quarterly Journal of Experimental Psychology."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1177/17470218221111750"},"children":["https://doi.org/10.1177/17470218221111750"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/ekxzy/"},"children":["https://osf.io/ekxzy/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformed beliefs are difficult to change. Refutations that target false claims typically reduce false beliefs, but tend to be only partially effective. In this study, a social norming approach was explored to test whether provision of peer norms could provide an alternative or complementary approach to refutation. Three experiments investigated whether a descriptive norm—by itself or in combination with a refutation—could reduce the endorsement of worldview-congruent claims. Experiment 1 found that using a single-point estimate to communicate a norm affected belief but had less impact than a refutation. Experiment 2 used a verbally presented distribution of four values to communicate a norm, which was largely ineffective. Experiment 3 used a graphically presented social norm with 25 values, which was found to be as effective at reducing claim belief as a refutation, with the combination of both interventions being most impactful. These results provide a proof of concept that normative information can aid in the debunking of false or equivocal claims, and suggests that theories of misinformation processing should take social factors into account.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants were shown predictive claims (X will have effect Y), followed by refutations of the claims, fictional peer norms at odds with the claims (i.e., norms that indicated low endorsement of/skepticism about the claims), or both. Experiment 1 used a 2 × 2 between-participants design with factors confidentiality (private, public) and post-refutation norm (absent, present). Experiment 2 comprised 2 parallel sub-experiments, 2A and 2B, each with 3 within-subjects conditions. Experiment 2A included refutation, narrow-norm, and wide-norm conditions. Experiment 2B combined norms and refutations, and thus compared a refutation condition (which was identical to Experiment 2A) with refutation-plus-narrow-norm and refutation-plus-wide-norm conditions. Thus, each participant received 3 claims that were then challenged by a refutation and/or norm. Experiment 3 was identical to Experiment 3 but the norms were presented in graphical format as distributions of individual ratings; these could be narrow or wide, representing a sharp or weak consensus.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Fictional articles about real-world topics. All experiments used claims that were worldview-congruent for most participants, as determined by a pilot rating.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Claim belief; predictive estimates relating to claims; misinformation reliance in inferential reasoning; ancillary measures: Need for authenticity; social assertiveness",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2","3"],"Description":["Experiment 1 tested whether social norms can reduce belief in a contested claim, and whether such an effect is dependent on the public nature of belief expressions. Experiment 1 also used a point norm (x out of 100 peers endorsed the claim).","Experiment 2 again examined whether social-norm information can reduce the endorsement of questionable worldview-congruent claims, either in isolation or in combination with a refutation. Moreover, Experiment 2 used a distribution rather than a point norm.","Experiment 3 presented both claim-endorsement and predicted-estimates norms. Like in the Exp. 2, both norms used a distribution-based approach but with a larger number of data points; to facilitate this, a graphical presentation format was employed."],"N":[143,144,154],"Effect size":["Cohen's d (private condition, pre-refutation norm): 0.31","Cohen's d (narrow norm condition): 0.18","Cohen's d (narrow norm condition): 0.77"],"Comments":["mean belief-change scores (Belief Rating 2 – Belief Rating 1) differed significantly from zero in both the private condition, M = −0.29 (SD = 0.93), t(71) = −2.67, d = 0.31, p = .009, and the public condition, M = 0.37 (SD = 0.83), t(70) = −3.71, d = 0.44, p < .001. This demonstrated a small belief-reducing effect of the initial, pre-refutation norm.","Change scores were significantly different from zero in the refutation condition of Experiment 2A, M = −1.53, SD = 1.76, d = 0.87, and all conditions of Experiment 2B (refutation: M = −1.75, SD = 1.98, d = 0.89; refutation-plus-narrow-norm: M = −1.61, SD = 2.14, d = 0.75; refutation-plus-wide-norm: M = −1.15, SD = 1.77, d = 0.65), all t(72) ⩽ 5.54, all p < .001. There was no significant belief change in the norm-only conditions of Experiment 2A (narrow: M = −0.21, SD = 1.15, d = 0.18; wide: M = −0.10, SD = 1.15, d = 0.08), t(72) ⩾ 1.54, p ⩾ .129. This established that claim belief was reduced significantly by a refutation (either with or without an additional norm) but not a stand-alone norm.","Belief change was significantly different from zero in all conditions of Experiment 3A (refutation: M = −8.93, SD = 9.71, d = 0.92; narrow-norm: M = −8.11, SD = 10.54, d = 0.77; wide-norm: M = −6.55, SD = 10.15, d = 0.65), as well as all conditions of Experiment 3B (refutation: M = −10.00, SD = 18.45, d = 0.54; refutation-plus-narrow-norm: M = −18.66, SD = 17.29, d = 1.08; refutation-plus-wide-norm: M = −15.04, SD = 14.36, d = 1.05), all t(75/77) ⩽ 4.79, all p < .001, establishing that claim belief was reduced significantly by a refutation or either type of norm."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"3d660b85fd5e7a2c2df0ab1e5b733626","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Multiple interventions: Social norms and Source-credibility labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Prike, T., Butler, L., & Ecker, U. K. H. (2023). Source-credibility information and social norms improve truth discernment and reduce engagement with misinformation online. PsyArXiv. (Preprint, not peer-reviewed)"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.31234/osf.io/dhx6f"},"children":["https://doi.org/10.31234/osf.io/dhx6f"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://doi.org/10.31234/osf.io/dhx6f"},"children":["https://doi.org/10.31234/osf.io/dhx6f"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Misinformation on social media is a pervasive challenge. In this study (N = 415) a social-media simulation was used to test two potential interventions for countering misinformation: a credibility badge and a social norm. The credibility badge was implemented by associating accounts, including participants’, with a credibility score. Participants’ credibility score was dynamically updated depending on their engagement with true and false posts. To implement the social-norm intervention, participants were provided with both a descriptive norm (i.e., most people do not share misinformation) and an injunctive norm (i.e., sharing \nmisinformation is the wrong thing to do). Both interventions were effective. The social-norm intervention led to reduced belief in false claims and improved discrimination between true and false claims. It also had some positive impact on social-media engagement, although some effects were not robust to alternative analysis specifications. The presence of credibility badges led to greater belief in true claims, lower belief in false claims, and improved discrimination. The credibility-badge intervention also had robust positive impacts on social-media engagement, leading to increased flagging and decreased liking and sharing of false \nposts. Cumulatively, the results suggest that both interventions have potential to combat misinformation and improve the social-media information landscape.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants took part in a social-media simulation, with instructions to “Engage as you would on social media and try to maximize your follower count!”. They received 40 false and 40 true claims (with images and a fictional source; claims identified as true or false by valid refutational/affirmative comments); they decided for each to like/share/flag/skip; claims were then presented again in plain verbal format (no images/sources) and belief in all claims was measured.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"80 claims of varying, largely unclear veracity, 40 of which were objectively true (e.g., “The unicorn is the national animal of Scotland”) and 40 of which were objectively false (e.g., “Most people only use between 10 and 50% of their brains”)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Engagement (composite score of flagging (-1), skipping (0), liking (+1), sharing (+2)) and Claim Belief, plus basic demographics",{"name":"div","attribs":{"className":"detail-label"},"children":["Comment"]},"Note that source credibility badge intervention is a little different to a typical source credibility labels in that it does not only include source credibility information on post sources, but also a credibility score for the participant, which changes dynamically depending on participant choices, thus highlighting potential reputational implications of misinformation sharing.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1. Credibility badges, engagement","2. Social norms, engagement","3. Credibility badges, beliefs","4. Social norms, beliefs"],"Description":["The study investigates the impact of credibility badges on engagement with social media posts containing false claims.","The study examines the effect of social norms on engagement with social media posts containing false claims.","The study explores the impact of credibility badges on beliefs in false claims.","The study investigates the effect of social norms on beliefs in false claims."],"N":[411,411,411,411],"Effect size":["Eta squared (ηp2): 0.09 (95%-CI: 0.04-0.15)","Eta squared (ηp2): 0.01 (95%-CI: 0-0.04)","Eta squared (ηp2): 0.03 (95%-CI: 0.01-0.07)","Eta squared (ηp2): 0.02 (95%-CI: 0-0.05)"],"Comments":["There was a significant main effect of credibility badge, with less positive engagements when credibility badges were included.","There was a significant main effect of social norm, with less positive engagements when a social norm was included.","The presence of a credibility badge significantly reduced belief in false claims.","The presence of a social norm intervention significantly reduced belief in false claims."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"2fbb0461a8b7cc03371e6d3391efa224","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Ecker, U. K. H., Lewandowsky, S., & Tang, D. T. W. (2010). Explicit warnings reduce but do not eliminate the continued influence of misinformation. Memory & Cognition, 38(8), 1087–1100."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.3758/MC.38.8.1087"},"children":["https://doi.org/10.3758/MC.38.8.1087"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Information that initially is presumed to be correct, but that is later retracted or corrected, often continues to influence memory and reasoning. This occurs even if the retraction itself is well remembered. The present study investigated whether the continued influence of misinformation can be reduced by explicitly warning people at the outset that they may be misled. A specific warning— giving detailed information about the continued influence effect (CIE)—succeeded in reducing the continued reliance on outdated information but did not eliminate it. A more general warning—reminding people that facts are not always properly checked before information is disseminated—was even less effective. In an additional experiment, a specific warning was combined with the provision of a plausible alternative explanation for the retracted information. This combined manipulation further reduced the CIE but still failed to eliminate it altogether.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Studied whether the continued influence of misinformation can be reduced by explicitly warning people at the outset that they may be misled. Study 1: RCT with 4 retraction conditions and a no-retraction control condition in a between-subjects design. Study 2: A between- subjects design with 3 conditions: (1) specific warning + alternative explanation, (2) no-retraction + specific warning, and (3) alternative-throughout.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Study 1: A folder with 14 statements with a fictitious account of a minibus accident. Condition 1: retraction only, Condition 2: retraction and an alternative explanation, Condition 3: specific warning, Condition 4: a general warning\n\nStudy 2: Materials identical to Study 1, with two exceptions: Specific warning was presented in all conditions, and, in the alternative-throughout condition, the passengers were initially reported to be young instead of old, and no further correction was presented",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Attention and manipulation checks",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study 1 experimentally tested whether the continued influence of misinformation can be reduced by explicitly warning people at the outset that they may be misled.","Study 2 experimentally tested whether alerting people to the effects of misinformation could eliminate the CIE in cases in which there is alternative information available."],"N":[125,92],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["A general warning did not reduce the level of continued influence effect (CIE) found with a mere retraction. Both a specific warning and the provision of an alternative explanation strongly reduced the CIE The alternative and the specific warning reduced references to misinformation  by 51%–65% (42%–67% in the alternative condition, 53%–63% in the specific warning condition).","The combined effect of the specific warning and the provision of an alternative account reduced reliance on misinformation more than the constituent strategies alone."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"4ba4ded1ac0b4a31acdf62d233d103dc","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Kim, A., Moravec, P. L., & Dennis, A. R. (2019). Combating fake news on social media with source ratings: The effects of user and expert reputation ratings. Journal of Management Information Systems, 36(3), 931–968."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1080/07421222.2019.1628921"},"children":["https://doi.org/10.1080/07421222.2019.1628921"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"As a remedy against fake news on social media, we examine the effectiveness of three different mechanisms for source ratings that can be applied to articles when they are initially published: expert rating (where expert reviewers fact-check articles, which are aggregated to provide a source rating), user article rating (where users rate articles, which are aggregated to provide a source rating), and user source rating (where users rate the sources themselves). We conducted two experiments and found that source ratings influenced social media users’ beliefs in the articles and that the rating mechanisms behind the ratings mattered. Low ratings, which would mark the usual culprits in spreading fake news, had stronger effects than did high ratings. When the ratings were low, users paid more attention to the rating mechanism, and, overall, expert ratings and user article ratings had stronger effects than did user source ratings. We also noticed a second-order effect, where ratings on some sources led users to be more skeptical of sources without ratings, even with instructions to the contrary. A user’s belief in an article, in turn, influenced the extent to which users would engage with the article (e.g., read, like, comment and share). Lastly, we found confirmation bias to be prominent; users were more likely to believe — and spread — articles that aligned with their beliefs. Overall, our results show that source rating is a viable measure against fake news and propose how the rating mechanism should be designed.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Studied the effectiveness of 3 types of source ratings (expert rating, user article rating, and user source rating) on belief in fake news with a repeated measures experiment. Participants received 2 headlines in the control treatment (no reputation ratings) and the remaining 6 headlines in 1 of 3 randomly assigned between-subjects treatments: expert rating, user article rating, or user source rating.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"8 news headlines on controversial political issues. The headlines focused on the issue of abortion, with four designed to appeal to politically left-leaning participants and the other four to right-leaning participants. To minimize any news source specific effect (e.g., trusted sources versus unknown sources), researchers invented eight names that sounded plausible.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Confirmation bias scale",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study tested the effectiveness of three different types of source ratings: expert rating, user article rating and user source rating.","Study tested the effectiveness of three different types of source ratings: expert rating, user article rating and user source rating."],"N":[590,299],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["When ratings were high, both expert rating and user article rating had significant effects, but user source ratings had no effect. For low-rated sources, all three mechanisms had significant effects; expert ratings had stronger impacts than did user source ratings (Chi-Squared = 15.39, 𝑝<0.001), as did user article ratings (Chi-Squared = 4.87, 𝑝<0.05); there was, however, no significant differences between expert rating and user article rating (Chi-Squared = 2.89, 𝑝>0.05).","High ratings had no significant effect across all rating mechanisms, while low ratings from expert rating and user article rating mechanisms had a negative influence on believability (H2 is partially supported); user source ratings had no effect. Comparing the rating mechanisms that had significant effects, there were no differences in the effects of the expert rating and user article rating mechanisms for low ratings (Chi-Squared = 0.64, p >.05)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"f6dd8964d543b4b2d87cb14dfb221839","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Mena, P. (2019). Cleaning Up Social Media: The Effect of Warning Labels on Likelihood of Sharing False News on Facebook. Policy & Internet, 12(2): 165-183. \n"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1002/poi3.214"},"children":["https://doi.org/10.1002/poi3.214"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"The flagging of false news has been one of the ways suggested to discourage people from sharing deceiving news stories on social media, and recent years have seen a growing number of initiatives focused on providing credibility labels for news online. Using an experimental design, this study assesses the effectiveness of such labels designed to reduce the sharing of false news. Specifically, it examines the impact of warning labels on Facebook users’ intentions to share false news stories, drawing on a sample of 501 participants from across the political spectrum. The study also explores users’ perceptions of the likelihood that other people would share false news even after seeing a warning label. We find that the flagging of false news may indeed have an effect on reducing false news sharing intentions by diminishing the credibility of misleading information. Furthermore, we find that users may be prone to believing that others are more likely to share false news than themselves, confirming the third-person effect. This study shows that flagging of false news on social media platforms like Facebook may indeed help the current efforts to combat sharing of deceiving information on social media.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Participants were presented with a simulated Facebook post that contained false news content. This post either included or did not include a warning label stating, “Disputed by Snopes.com and PolitiFact.” The study employed a 2x2 between-subjects experimental design, based on the presence or absence of the warning label and the presence or absence of a supporting false news article for the Facebook post.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Facebook post with or without warning label",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, political leaning",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["This study experimentally tests the effectiveness of warning labels designed to reduce the sharing of false news."],"N":[501],"Effect size":["Cohen’s d: 0.36"],"Comments":["The flagging of false news had a small to moderate effect on decreasing the likelihood of sharing the fabricated post on Facebook."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"a06fc01eeb3e08d0ccc908912a14bdb7","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Clayton, K., Blair, S., Busam, J. A., Forstner, S., Glance, J., Green, G., Kawata, A., Kovvuri, A., Martin, J., Morgan, E., Sandhu, M., Sang, R., Scholz-Bright, R., Welch, A. T., Wolff, A. G., Zhou, A., & Nyhan, B. (2020). Real solutions for fake news? Measuring the effectiveness of general warnings and fact-check tags in reducing belief in false stories on social media. Political Behavior, 42(4), 1073–1095."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1007/s11109-019-09533-0"},"children":["https://doi.org/10.1007/s11109-019-09533-0"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YDC4XD"},"children":["https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YDC4XD"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Social media has increasingly enabled “fake news” to circulate widely, most notably during the 2016 U.S. presidential campaign. These intentionally false or misleading stories threaten the democratic goal of a well-informed electorate. This study evaluates the effectiveness of strategies that could be used by Facebook and other social media to counter false stories. Results from a pre-registered experiment indicate that false headlines are perceived as less accurate when people receive a general warning about misleading information on social media or when specific headlines are accompanied by a “Disputed” or “Rated false” tag. Though the magnitudes of these effects are relatively modest, they generally do not vary by whether headlines were congenial to respondents’ political views. In addition, we find that adding a “Rated false” tag to an article headline lowers its perceived accuracy more than adding a “Disputed” tag (Facebook’s original approach) relative to a control condition. Finally, though exposure to the “Disputed” or “Rated false” tags did not affect the perceived accuracy of unlabeled false or true headlines, exposure to a general warning decreased belief in the accuracy of true headlines, suggesting the need for further research into how to most effectively counter false news without distorting belief in true information.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Evaluated the effectiveness of labels on false news headlines to counter false stories on a 2 (general warning: yes, no) × 3 (label: none, \"disputed,\" \"rated false\") between-subjects design, including 6 treatment groups and 1 control group.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"6 false news headlines (3 Pro-Trump and 3 anti-Trump) from Snopes and Buzzfeed. 3 true political headlines from mainstream media sources. News sources (and authors)  were omitted to minimize potentially confounding variables and isolate the effects of warnings and tags on belief in false news headlines.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, use of social media, political preferences, voting behavior, and trust in fact-checking and the media; approval of Trump",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1: General warning","1: Disputed tag","1: Rated False tag"],"Description":["Participants received a general warning about misleading articles","Specific headlines are accompanied by a Disputed tag.","Specific headlines are accompanied by a Rated False tag."],"N":[1250,429,397],"Effect size":["Cohen’s d: 0.08","Cohen’s d: 0.26","Cohen’s d: 0.38"],"Comments":["Average belief in false headlines was slightly lower for participants who saw a general warning before seeing headlines than for participants who saw headlines with no warning (− 0.08; p < .05). However, the substantive magnitude of this reduction in perceived belief accuracy is small (Cohen’s d = 0.08).","Average perceived accuracy for participants who saw a headline with a Disputed tag was 0.24 points lower on our four-point scale than for participants who saw no tag (p < 0.01; Cohen’s d = 0.26).","Average perceived accuracy for participants who saw a headline with a Rated False tag was 0.34 points lower on our four-point scale than for participants who saw no tag (p < 0.01; Cohen’s d = 0.38)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"163f095f0fb5a8f951ec91bf92c52ea3","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Pennycook, G., Bear, A., Collins, E. T., & Rand, D. G. (2020). The implied truth effect: Attaching warnings to a subset of fake news headlines increases perceived accuracy of headlines without warnings. Management Science, 66(11), 4944–4957."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1287/mnsc.2019.3478"},"children":["https://doi.org/10.1287/mnsc.2019.3478"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/b5m3n/"},"children":["https://osf.io/b5m3n/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"What can be done to combat political misinformation? One prominent intervention involves attaching warnings to headlines of news stories that have been disputed by third-party fact-checkers. Here we demonstrate a hitherto unappreciated potential consequence of such a warning: an implied truth effect, whereby false headlines that fail to get tagged are considered validated and thus are seen as more accurate. With a formal model, we demonstrate that Bayesian belief updating can lead to such an implied truth effect. In Study 1 (n = 5,271 MTurkers), we find that although warnings do lead to a modest reduction in perceived accuracy of false headlines relative to a control condition (particularly for politically concordant headlines), we also observed the hypothesized implied truth effect: the presence of warnings caused untagged headlines to be seen as more accurate than in the control. In Study 2 (n = 1,568 MTurkers), we find the same effects in the context of decisions about which headlines to consider sharing on social media. We also find that attaching verifications to some true headlines—which removes the ambiguity about whether untagged headlines have not been checked or have been verified—eliminates, and in fact slightly reverses, the implied truth effect. Together these results contest theories of motivated reasoning while identifying a potential challenge for the policy of using warning tags to fight misinformation—a challenge that is particularly concerning given that it is much easier to produce misinformation than it is to debunk it.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Study 1: In the control condition, 12 false and 12 true news headlines were displayed without any warnings. In the treatment condition,participants were shown 6 randomly selected false news headlines displayed with warnings (\"Disputed by 3rd Party Fact-Checkers\"), and the remainder of the items (6 false, 12 true) displayed without any warnings. Study 2: In the control condition, all headlines were presented in their original form. In the first treatment condition, three quarters of false headlines were stamped with “FALSE.” In the second treatment condition, three quarters of false headlines were stamped “FALSE” and three quarters of true headlines were stamped “TRUE.”",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"A series of false and true headlines. All headlines were presented in standard “Facebook format ”with picture, headline, lede sentence, and source. The false news headlines were selected from Snopes.com (verified as having been fabricated and entirely untrue).True news headlines were a selection of contemporary stories from mainstream news outlets that did not contain factual errors or fabrication.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","2"],"Description":["Study 1 tests the predictions regarding the existence of a warning effect and an implied truth effect when warnings are attached to a subset of false headlines.","Study 2 tests whether the effects found in Study 1 generalize to sharing intentions."],"N":[5271,1568],"Effect size":["(no standard effect size available/yet extracted)","(no standard effect size available/yet extracted)"],"Comments":["The warning decreases belief in items that are tagged (the warning effect) but increases belief in items that are untagged (the implied truth effect). Both the warning effect and the implied truth effect were quite small. Warning effect: False headlines in the warning treatment that were presented with warnings were perceived as less accurate (M=0.187) than false headlines in the control (M=0.220). The warning effect was roughly twice as large for politically concordant headlines (warning, M= 0.210; control, M= 0.253) as for politically discordant headlines (warning, M= 0.187; control, M=0.164)","Participants were less likely to consider sharing false headlines tagged with a warning (16.1%) compared with false headlines in the control (29.8%; p<0.001). The warning effect was significantly larger for concordant false headlines (warned: 16.7%; control: 33.7%) than for discordant false headlines (warned: 14.7%; control: 26.0%)."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"2d4b46a1a2d35c50481fe45b424fcd04","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Ecker, U. K. H., O’Reilly, Z., Reid, J. S., & Chang, E. P. (2020). The effectiveness of short-format refutational fact-checks. The British Journal of Psychology, 111, 36-54.\n"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1111/bjop.12383"},"children":["https://doi.org/10.1111/bjop.12383"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/c45tf/?view_only=ca57d7349b07453cabe94feb6d72b638"},"children":["https://osf.io/c45tf/?view_only=ca57d7349b07453cabe94feb6d72b638"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Fact-checking has become an important feature of the modern media landscape. However, it is unclear what the most effective format of fact-checks is. Some have argued that simple retractions that repeat a false claim and tag it as false may backfire because they boost the claim's familiarity. More detailed refutations may provide a more promising approach, but may not be feasible under the severe space constraints associated with social-media communication. In two experiments, we tested whether (1) simple ‘false-tag’ retractions can indeed be ineffective or harmful; and (2) short-format (140-character) refutations are more effective than simple retractions. Regarding (1), simple retractions reduced belief in false claims, and we found no evidence for a familiarity-driven backfire effect. Regarding (2), short-format refutations were found to be more effective than simple retractions after a 1-week delay but not a one-day delay. At both delays, however, they were associated with reduced misinformation-congruent reasoning.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Studied the most effective format of fact-checking using two experiments that tested whether simple 'false-tag' retractions are ineffective or harmful and if short-format refutations are more effective than simple retractions. \n\nRetraction condition: Affirmations and plain retractions for both true and false claims.\nRefutation condition:Detailed refutations for both true and false claims.\nRetraction-Only condition: Affirmations and retractions for claims without initial exposure to claims.\nRefutation-Only condition: Affirmations and refutations for claims without initial exposure to claims.\nNo-Exposure Condition: Inference questions.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"12 claims in a randomized order (six true and six false)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Belief ratings; and two inference questions: 1) an estimation of the true value relating to each claim; 2) a rating of agreement/disagreement regarding a statement related to the respective claim, on a 0-10 scale.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","1","12","1; 2"],"Description":["The experiments used true and false claims across various experimental conditions; claim veracity was manipulated within subjects, and experimental condition was a between-subjects factor.","The experiments used true and false claims across various experimental conditions; claim veracity was manipulated within subjects, and experimental condition was a between-subjects factor.","The experiments used true and false claims across various experimental conditions; claim veracity was manipulated within subjects, and experimental condition was a between-subjects factor.","The experiments used true and false claims across various experimental conditions; claim veracity was manipulated within subjects, and experimental condition was a between-subjects factor."],"N":[531,369,900,900],"Effect size":["Partial eta squared (η2p): 0.01","Partial eta squared (η2p): 0.03","Partial eta squared (η2p): Study 1: 0.12; Study 2: 0.03","Partial eta squared (η2p): Study 1: 0.03; Study 2: 0.1"],"Comments":["Refutations were not more effective than plain retractions at reducing claim belief after a 1-day retention interval.","Refutations are more effective than plain false-tag retractions at reducing claim belief after a 1-week retention interval.","Detailed refutations lead to lower inferential-reasoning scores compared to plain retractions.","Refutations might reduce belief in true claims."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"b77affe11eaf7e33b4a532463ebe9b09","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Grady, R. H., Ditto, P. H., & Loftus, E. F. (2021). Nevertheless, partisanship persisted: Fake news warnings help briefly, but bias returns with time. Cognitive Research: Principles and Implications, 6(1), Article 52."]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1186/s41235-021-00315-z"},"children":["https://doi.org/10.1186/s41235-021-00315-z"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/gtuha/"},"children":["https://osf.io/gtuha/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Politically oriented “fake news”—false stories or headlines created to support or attack a political position or person—is increasingly being shared and believed on social media. Many online platforms have taken steps to address this by adding a warning label to articles identified as false, but past research has shown mixed evidence for the effectiveness of such labels, and many prior studies have looked only at either short-term impacts or non-political information. This study tested three versions of fake news labels with 541 online participants in a two-wave study. A warning that came before a false headline was initially very effective in both discouraging belief in false headlines generally and eliminating a partisan congruency effect (the tendency to believe politically congenial information more readily than politically uncongenial information). In the follow-up survey two weeks later, however, we found both high levels of belief in the articles and the re-emergence of a partisan congruency effect in all warning conditions, even though participants had known just two weeks ago the items were false. The new pre-warning before the headline showed some small improvements over other types, but did not stop people from believing the article once seen again without a warning. This finding suggests that warnings do have an important immediate impact and may work well in the short term, though the durability of that protection is limited.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"Studied the effect of warning labels in 3 conditions—before, during, and after headline and rating—in a 2-wave online study (2-week delay).",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Series of headlines (3 false and 9 true) with images above them and questions below them, with four new headlines added at T2, drawn from recent news (one true Democrat-friendly, one true Republican-friendly, and one true politically neutral news item, one politically neutral fake news)",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Political Interest, Partisan preferences, Feelings towards political groups, Social media use, Political news consumption",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1"],"Description":["Study 2 experimentally tested whether subtly making the concept of accuracy salient increased the quality of COVID-19 information that people were willing to share online."],"N":[418],"Effect size":["(no standard effect size available/yet extracted)"],"Comments":["Participants in the Warning-Before condition believed the false news items less than those the Warning-After condition for both the congruent (b = 1.283, SE = 0.149, p < 0.001) and incongruent (b = 0.734, SE = .120, p < 0.001) items, while the interaction shows that the former was especially pronounced."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"dc7cb04d4d3d0ff09df36d9628fc87d5","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Lee, J., Kim, J. W., & Yun Lee, H. (2023). Unlocking Conspiracy Belief Systems: How Fact-Checking Label on Twitter Counters Conspiratorial MMR Vaccine Misinformation. Health \nCommunication, 38(9), 1780-1792. \n\n"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1080/10410236.2022.2031452"},"children":["https://doi.org/10.1080/10410236.2022.2031452"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"This study tested whether a simple fact-checking label on Twitter effectively reduces vaccine conspiracy beliefs, misinformation engagement intentions, and vaccination intentions. A web-based experiment (N = 206) of adults living in the United States through Amazon Mechanical Turk (MTurk) was conducted for the measles–mumps–rubella (MMR) vaccine in March 2020. The results showed that the fact-checking label attached to the conspiratorial misinformation post significantly reduced MMR vaccine conspiracy beliefs compared to the no fact-checking (misinformation-only) condition but did not directly affect MMR misinformation engagement intentions and MMR vaccination intentions. In addition, we found that the fact-checking label effectively decreased vaccine conspiracy beliefs and misinformation engagement intentions for those whose prior favorable attitudes toward MMR vaccination were relatively low. Based on our findings, we suggest that public health professionals and health communicators use the fact-checking label as a promising tool for countering conspiracy theories about vaccination. However, they should further seek alternative ways to limit the public’s engagement in misinformation-related activities on social media and promote health protective behavioral intentions, given the limited effects of fact-checking labels.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"2 (fact-checking: present vs. absent) x 2 (prior attitudes: positive vs. negative) between-subjects design. Participants were randomly assigned to one of four experimental conditions: (1) fact-checking label present and positive prior attitude toward the vaccine, (2) fact-checking label present and negative prior attitude toward the vaccine, (3) fact-checking label absent and positive prior attitude toward the vaccine, and (4) fact-checking label absent and negative prior attitude toward the vaccine.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"The test stimuli used in this study were three different tweets: (1) a conspiratorial misinformation tweet about the MMR vaccine, (2) a weather news tweet for the no fact-checking condition, and (3) a tweet for the fact-checking label condition. The conspiratorial misinformation tweet was designed to contain false information about the MMR vaccine, while the other two tweets were neutral and did not contain any information about vaccines. These tweets were used to manipulate the fact-checking condition in the study.",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, political ideology, trust in government, trust in science, and prior attitudes toward vaccines",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","1","1"],"Description":["Examined the effectiveness of fact-checking labels in reducing vaccine conspiracy beliefs","Examined the effectiveness of fact-checking labels on misinformation engagement intentions","Examined the effectiveness of fact-checking labels on vaccination intentions"],"N":[206,206,206],"Effect size":["Cohen's d: -0.28 (95%-CI: -0.53--0.03)","Cohen's d: -0.21 (95%-CI: -0.46-0.04)","Cohen's d: -0.05 (95%-CI: -0.3-0.2)"],"Comments":["Fact-checking had a small negative effect on vaccine conspiracy beliefs","There was no statistically significant effect of fact-checking on misinformation engagement intentions.","There was no statistically significant effect of fact-checking on vaccination intentions"]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"d3692430b3afd4c9b1c7ef7e3ef79ec6","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Koch, T. K., Frischlich, L., & Lermer, E. (2023). Effects of fact‐checking warning labels and social endorsement cues on climate change fake news credibility and engagement on social media. Journal of Applied Social Psychology, 53(6): 495-507. \n"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1111/jasp.12959"},"children":["https://doi.org/10.1111/jasp.12959"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},{"name":"a","attribs":{"href":"https://osf.io/ugt2v/"},"children":["https://osf.io/ugt2v/"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Online fake news can have noxious consequences. Social media platforms are experimenting with different interventions to curb fake news' spread, often employing them simultaneously. However, research investigating the interaction of these interventions is limited. Here, we use the heuristic-systematic model of information processing (HSM) as a theoretical framework to jointly test two interventions against fake news that are implemented at scale by social media platforms: (1) adding warning labels from fact checkers to initiate systematic processing and (2) removing social endorsement cues (e.g., engagement counts) to reduce the influence of this heuristic cue. Moreover, we accounted for dispositions previously found to affect a person's response to fake news through motivated reasoning or cognitive style. An online experiment in Germany (N = 571) confirmed that warning labels reduced the perceived credibility of a fake news post exaggerating the consequences of climate change. Warning labels also lowered the (self-reported) likelihood to amplify fake news. Removing social endorsement cues did not have an effect. In line with research on motivated reasoning, left-leaning individuals perceived the climate fake news to be more credible and reported a higher likelihood to amplify it. Supporting research on cognitive style, participants with lower educational levels and a less analytic thinking style also reported a higher likelihood of amplification. Elaboration likelihood was associated only with age, involvement, and political leaning, but not affected by warning labels. Our findings contribute to the mounting evidence for the effectiveness of warning labels while questioning their relevance for systematic processing.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"The study examined the impact of warning labels on perceived credibility and belief in a fake news post. This was gauged by manipulating social endorsements, which were assessed by asking participants how likely they were to engage in certain behaviors, such as liking and sharing the post, discussing the post with others, or seeking more information on the topic. The online experiment employed a 2x2 between-subjects design, contrasting the presence vs. absence of warning labels with the presence vs. absence of social endorsement cues.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Fake news posts with varying topics (i.e., sports, politics, and environment). All posts consisted of a picture, a headline, and a brief caption resembling regular news posts on Facebook",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Education, analytical thinking, interest in environmental topics, political leaning",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["1","1","1"],"Description":["This study experimentally tested the effect of warning labels on a fake news post to measure perceived credibility .","This study experimentally tested the effect of warning labels on a fake news post to measure amplification likelihood","This study experimentally tested the effect of warning labels on a fake news post to measure elaboration likelihood."],"N":[571,571,571],"Effect size":["R-squared (R²): 0.04","R-squared (R²): 0.05","R-squared (R²): 0.05"],"Comments":["Warning labels reduced the perceived credibility of fake news posts. Social endorsement cues had no such effect on credibility perceptions.","Participants who saw the warning label reported a lower likelihood that they would amplify the fake news post. Social endorsement cues had no significant effect.","Neither warning labels nor social endorsement or their interaction predicted participants' elaboration likelihood."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"eba8b2a70a658ca8bd64f46b6bb0abce","static":false,"nested":true},"children":[]}]},{"name":"div","attribs":{"className":"package-detail"},"children":[{"name":"div","attribs":{"className":"detail-header"},"children":["Warning and fact-checking labels"]},{"name":"div","attribs":{"className":"detail-description"},"children":["Moon, W.-K., Chung, M., & Jones-Jang, S. Mo. (2022). How Can We Fight Partisan Biases in the COVID-19 Pandemic? AI Source Labels on Fact-checking Messages Reduce Motivated Reasoning.\nMass Communication and Society, 26(4): 646-670.\n"]},{"name":"div","attribs":{"className":"detail-label"},"children":["DOI"]},{"name":"a","attribs":{"href":"https://doi.org/10.1080/15205436.2022.2097926"},"children":["https://doi.org/10.1080/15205436.2022.2097926"]},{"name":"div","attribs":{"className":"detail-label"},"children":["Open data"]},"NA",{"name":"div","attribs":{"className":"detail-label"},"children":["Abstract"]},"Upon a surge of misinformation surrounding COVID-19, fact-checking has received much attention as a tool to fight the rampant misinformation. However, such correction efforts have faced challenges from partisans’ biased information processing. For example, partisans trust or distrust a fact-checking message based on whether the message benefits or harms their supporting party. To minimize such politically biased processing of corrective health information, this experimental study examined how different source labels of fact-checkers (human experts vs. AI vs. user consensus) affect partisans’ perceived credibility of fact-checking messages about COVID-19. Our findings showed that AI and user consensus (vs. human experts) source labels on fact-checking messages significantly reduced partisan-based motivated reasoning in evaluating fact-checking message credibility.",{"name":"div","attribs":{"className":"detail-label"},"children":["Method"]},"The study conducted an online experiment using a 2x3 factorial design based on social identity (ingroup vs. outgroup) and fact-checking sources (human experts, AI, or crowdsourced consensus). Participants were exposed to one of two fabricated COVID-19 news stories linked to either their supported political party (ingroup) or the opposing party (outgroup). These stories were presented as Facebook posts. After assessing the initial story's credibility, participants were then shown a debunking message from one of the three fact-checking sources. The content of the debunking message remained consistent across sources. The study's primary objective was to measure participants' perceived credibility of these fact-checking messages, assessing aspects like accuracy, trustworthiness, and objectivity.",{"name":"div","attribs":{"className":"detail-label"},"children":["Test stimuli"]},"Fabricated COVID-19 news stories linked to either their supported political party (ingroup) or the opposing party (outgroup); the stories were presented as Facebook posts",{"name":"div","attribs":{"className":"detail-label"},"children":["Additional measures"]},"Demographics, political leaning; six 5-point bipolar items were adopted: inaccurate-accurate, untrustworthy-trustworthy, unqualified-qualified, imbalanced-balanced, not objective-objective, and bias-unbiased. Measured credibility of fake news stories and fact-checking messages using the 5-point bipolar items.",{"name":"div","attribs":{"className":"detail-label"},"children":["Results"]},{"name":"Reactable","attribs":{"data":{"Study":["H1","H2","H3"],"Description":["Comparison of participants' perceived credibility of a false news story about COVID-19 based on in-group and out-group news condition.","Perception of the credibility of fact-checking messages based on the in-group and out-group condition.","Investigation of the effectiveness of alternative fact-checking sources in decreasing motivated reasoning."],"N":[353,353,349],"Effect size":["η2 (partial eta): 0.01","η2: 0.01","η2: 0.02"],"Comments":["Those who read an article from their supporting party reported greater perceived credibility than those who read from their opponent party.","Participants in the in-group condition were less likely to perceive fact-checking messages as credible than those in the out-group condition.","The pattern of motivated reasoning varied by fact-checking sources, with significant differences observed when the fact-checker was a human expert."]},"columns":[{"id":"Study","name":"Study","type":"character","headerClassName":"header","minWidth":100},{"id":"Description","name":"Description","type":"character","headerClassName":"header","minWidth":400},{"id":"N","name":"N","type":"numeric","headerClassName":"header","minWidth":80},{"id":"Effect size","name":"Effect size","type":"character","headerClassName":"header","minWidth":200},{"id":"Comments","name":"Results","type":"character","headerClassName":"header","minWidth":400}],"pagination":false,"className":"archived-table","inline":true,"theme":{"cellPadding":"8px 12px"},"dataKey":"2d941c59376639e7a68b64d1d7dacb6a","static":false,"nested":true},"children":[]}]}]},{"id":"Intervention","name":"Intervention","type":"character","minWidth":85,"rowHeader":true,"style":{"fontWeight":600}},{"id":"References","name":"References","type":"character","minWidth":100},{"id":"Experimental setting","name":"Experimental setting","type":"character","minWidth":90,"aggregate":"frequency"},{"id":"Design","name":"Design","type":"character","minWidth":80,"aggregate":"frequency"},{"id":"Treatment","name":"Treatment","type":"character","minWidth":120},{"id":"Paradigm","name":"Paradigm","type":"character","minWidth":80},{"id":"Outcome variable","name":"Outcome variable","type":"character","minWidth":90},{"id":"Sample size","name":"Sample size","type":"numeric","minWidth":70,"format":{"cell":{"digits":0},"aggregated":{"digits":0}},"align":"left"},{"id":"Sample Country","name":"Sample Country","type":"character","minWidth":80,"aggregate":"frequency"},{"id":"Sample demographics","name":"Demographics","type":"character","minWidth":100,"aggregate":"frequency"},{"id":"Recruitment","name":"Recruitment","type":"character","minWidth":80},{"id":"Main findings","name":"Main findings","type":"character","minWidth":160},{"id":"Longevity","name":"Longevity","type":"character","minWidth":80}],"filterable":true,"searchable":true,"defaultPageSize":12,"showPageSizeOptions":true,"pageSizeOptions":[6,12,18],"highlight":true,"bordered":true,"striped":true,"showSortable":true,"width":"1800px","theme":{"borderColor":"#dfe2e5","stripedColor":"#f6f8fa","highlightColor":"#f0f5f9","style":{"fontFamily":"-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, Arial, sans-serif"},"searchInputStyle":{"width":"100%"}},"elementId":"evidence-table","dataKey":"7911547176049ad888c65596461bb083"},"children":[]},"class":"reactR_markup"},"evals":[],"jsHooks":[]}</script>
<p><a
href="https://ai_society.mpib.dev/intervention_toolbox/privacy.html">Privacy
policy</a> - <a
href="https://ai_society.mpib.dev/intervention_toolbox/terms.html">Imprint/Provider
Identification</a> <!-- ### Note on effect sizes: --></p>
<!-- *Cohen's d* -->
<!-- Interpretation suggested by Cohen: -->
<!--     .2: Small effect size -->
<!--     .5: Medium effect size -->
<!--     .8: or higher: Large effect size -->
<!-- *Partial eta squared (η2p)* "describes a proportion of variability in a sample associated with an independent variable; it is calculated as the ratio between the sum of squares for a particular factor in an ANOVA and that sum of squares combined with the sum of squares for its specific error term." -->
<!-- The following rules of thumb are commonly used to interpret values for Partial eta squared: -->
<!--     .01: Small effect size -->
<!--     .06: Medium effect size -->
<!--     .14 or higher: Large effect size -->
<!-- Note that these are general suggestions for interpretations, and that "the values should be considered in the context of research in an area." -->
<!-- https://www.bps.org.uk/psychologist/methods-why-are-effect-sizes-still-neglected -->
<style type="text/css">
p {
 font-size: 20px;
 width: 1200px;
  margin: 10px 15px 15px 0px;
 }

li {
  font-size: 20px;
  }


h1 {
  font-size: 32px;
  width: 1200px;
}



 button { color: white;
 background-color: rgba(14, 114, 186, 0.7);
 margin: 10px 15px 15px 0px;
 border: none;
 padding: 7px 7px;
 <!-- text-align: center; -->
 <!-- text-decoration: none; -->
 <!-- <!-- display: inline-block; --> -->
 <!-- font-size: 16px -->
  }
  
  
 .button2 { color: black;
 background-color: rgba(14, 114, 186, 0.1);
 margin: 10px 30px 0px 5px;
 border: none;
 padding: 4px 4px;
 font-size: 12px;
  }


body {
  # background-color: rgba(176, 219, 234, 0.5);
   <!-- margin-top: 10px; -->
  <!-- margin-left: 5px; -->
 padding-left: 0px;
   width: 50%;
}

.btn-workflowr {
  display: none
}


.btn-workflowr-sessioninfo {
  visibility: hidden
}

.btn-default {
  visibility: hidden
}

.package-detail {
  padding: 24px;
  box-shadow: inset 0 1px 3px #dbdbdb;
  background: hsl(213, 20%, 99%);
}

.detail-label {
  margin: 20px 0 4px;  
  font-size: 14px;
  color: rgba(0, 0, 0, 0.6);
}

.detail-header {
  margin-bottom: 16px;
  font-size: 20px;
  font-weight: 600;
}

.detail-title {
  margin-left: 18px;
  font-size: 15px;
  font-weight: 600;
  color: rgba(0, 0, 0, 0.8);
}

.detail-description {
  font-size: 14px;
}

.archived-table {
  border: 1px solid hsl(213, 33%, 93%);
  border-radius: 4px;
  box-shadow: 0 2px 7px 0 rgba(0, 0, 0, 0.05);
  font-size: 14px;
}

</style>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span>
Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre><code>R version 4.3.1 (2023-06-16 ucrt)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 19043)

Matrix products: default


locale:
[1] LC_COLLATE=English_United States.utf8 
[2] LC_CTYPE=English_United States.utf8   
[3] LC_MONETARY=English_United States.utf8
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.utf8    

time zone: Europe/Berlin
tzcode source: internal

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] sparkline_2.0   jsonlite_1.8.7  htmltools_0.5.6 glue_1.6.2     
 [5] reactable_0.4.4 readxl_1.4.3    here_1.0.1      lubridate_1.9.2
 [9] forcats_1.0.0   stringr_1.5.0   dplyr_1.1.3     purrr_1.0.2    
[13] readr_2.1.4     tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.3  
[17] tidyverse_2.0.0 pacman_0.5.1    workflowr_1.7.1

loaded via a namespace (and not attached):
 [1] gtable_0.3.4      xfun_0.40         bslib_0.5.1       htmlwidgets_1.6.2
 [5] processx_3.8.2    callr_3.7.3       tzdb_0.4.0        vctrs_0.6.3      
 [9] tools_4.3.1       crosstalk_1.2.0   ps_1.7.5          generics_0.1.3   
[13] fansi_1.0.4       pkgconfig_2.0.3   lifecycle_1.0.3   compiler_4.3.1   
[17] git2r_0.32.0      munsell_0.5.0     getPass_0.2-2     httpuv_1.6.11    
[21] sass_0.4.7        yaml_2.3.7        later_1.3.1       pillar_1.9.0     
[25] jquerylib_0.1.4   whisker_0.4.1     ellipsis_0.3.2    cachem_1.0.8     
[29] tidyselect_1.2.0  digest_0.6.33     stringi_1.7.12    rprojroot_2.0.3  
[33] fastmap_1.1.1     grid_4.3.1        colorspace_2.1-0  cli_3.6.1        
[37] magrittr_2.0.3    utf8_1.2.3        withr_2.5.0       reactR_0.4.4     
[41] scales_1.2.1      promises_1.2.1    timechange_0.2.0  rmarkdown_2.24   
[45] httr_1.4.7        cellranger_1.1.0  hms_1.1.3         evaluate_0.21    
[49] knitr_1.43        rlang_1.1.1       Rcpp_1.0.11       rstudioapi_0.15.0
[53] R6_2.5.1          fs_1.6.3         </code></pre>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
https://docs.mathjax.org/en/latest/web/configuration.html. This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>





</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
